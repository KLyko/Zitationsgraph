Introduction
Despite being a natural, seemingly effortless, every-
day human activity verbal communication is a
highly complex process, drawing on a wide range
of cognitive abilities. Short-term ‘working’
memory, knowledge of phonological structure,
grammatical convention, and word meaning are
among the fundamental requirements (Garrard,
2008; Price, 2010). Understanding the language
system in sufficient detail not only to enumerate
its functional components, but also to explain how
these map on to physical (i.e. neuronal) activity in
the brain, has been a long-standing aim of cognitive
neuroscience.
Correspondence:
Peter Garrard, Stroke and
Dementia Research Centre,
St George’s, University of
London, Cranmer Terrace,
London SW17 0RE, UK
E-mail:
pgarrard@sgul.ac.uk
Literary and Linguistic Computing, Vol. 26, No. 4, 2011.  The Author 2011. Published by Oxford University Press on
behalf of ALLC. All rights reserved. For Permissions, please email: journals.permissions@oup.com
389
doi:10.1093/llc/fqr018 Advance Access published on 24 May 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Three types of methodological approach are cur-
rently used to test hypotheses arising from theoret-
ical models: recordings of cognitive event-related
potentials (ERPs), which reveal the distribution of
changes in electrical potential (corresponding to
simultaneous regional activity) over the brain’s sur-
face during specific cognitive tasks; functional neu-
roimaging, which is used to delineate brain regions
where blood oxygen consumption (and hence
neural activity) changes in response to a
well-defined cognitive task; and finally, the study
of the patterns of cognitive difficulty shown by in-
dividuals who have suffered some form of brain
injury, showing how cognitive abilities (such as lin-
guistic communication) fractionate into more basic
subcomponents—an approach referred to as
neuropsychology.
Although much of our understanding of the or-
ganization of the linguistic system has come from
the study of patients with aphasic deficits due to
focal brain damage (e.g. following a stroke),
interesting neurolinguistic data have also been ob-
tained from patients whose brain damage is second-
ary to progressive neurodegenerative dementia
such as Alzheimer’s disease (AD) and fronto-
temporal dementia (FTD) (a rarer, more focal,
cause of brain degeneration). The cognitive profiles
of individuals with these conditions suggest that
knowledge of phonology, grammatical role, and
meanings of individual words may all be independ-
ently disrupted. The experimental paradigms that
are employed to demonstrate these problems are
generally based on single words, whether as stimuli,
responses (e.g. in picture naming tests), or both
(Kertesz, 1986; Neary et al., 2005). Such tasks, how-
ever, despite being both robust and straightforward
to administer and score, are somewhat lacking in
everyday validity. More recently it has been recog-
nized that samples of connected discourse provide
rich sources of insight into linguistic cognition in
both the intact and damaged brain (Chi, 1997;
Ash et al., 2006; 2009; Meteyard and Patterson,
2009; Garrard and Forsyth, 2010).
There are a number of different approaches to
the analysis of samples of connected language:
quantification and classification of errors; compari-
sons of lexical variables such as frequency and
concreteness; measures of syntactic complexity,
narrative coherence, and semantic richness; latent
semantic analysis (LSA); and principal components
analysis of word type occurrences across groups of
speakers. All of these methods require, as a first step,
the production of a written version, in a standard
orthography, of the text to be analysed. Although
this might seem like a trivial procedure the reality is
that transcription is fraught with problems relating
to accuracy, consistency, and standardization. For
English alone there are numerous schemes for tran-
scribing episodes of individual and conversational
connected speech (Atkinson and Heritage, 1984;
Boden and Zimmermann, 1991; Schiffrin, 1994;
Leech et al., 1995; MacWhinney, 1995; Carter,
2004; Jefferson, 2004; MICASE, 2007; Forsyth
et al., 2008). Yet, even when a single set of conven-
tions is adopted, it is common to see considerable
variation between individual transcribers.
Why should this be so? One important factor is
nicely summarized in the title of an article by Ochs
(1979) ‘Transcription as theory’: in other words,
there is simply no single objective method of tran-
scribing speech, a point also emphasized by
Cucchiarini (1996):
. . . a transcription, whatever the type, is
always the result of an analysis or classification
of speech material. Far from being the reality
itself, transcription is an abstraction from it.
(p. 132).
In accepting that transcription is an ‘act of inter-
pretation’ (Bucholtz, 2000, p. 1463), we acknow-
ledge that multiple different interpretations are
to be expected. An empirical study by Chiari
(2007) has highlighted the extent to which a tran-
scription is a representation of what is intended
rather than what is actually said. This arises as
a natural consequence of our normal mode of lis-
tening, in which we seek to understand what the
speaker means rather than to record the form in
which such meaning is expressed. As Chiari (2007)
argues:
. . . even with the best possible audio quality,
when trying to concentrate attention on the
reconstruction of linguistic form, we tend to
shift and rely on our understanding strategies,
P. Garrard et al.
390 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
that lead us to re-create text in a plausible
way. (p. 10).
Despite widespread recognition that the process
of transcription is problematic, transcribing is not a
prime focus of concern for most researchers, and it
is not surprising that the transcription process itself
is seldom described in great depth or detail. The
following extracts from the Methodology sections
of a selection of papers that use transcripts as data
amply illustrate this point:
The interviews were tape recorded and tran-
scribed for analysis. (Martinson et al., 1993).
The recordings of each participant were tran-
scribed verbatim. (Hux et al., 2008)
Following transcription, each memory was
segmented into informational bits or details.
(McKinnon et al., 2008)
It appears that practice among researchers varies
even on important questions such as: (i) the number
of listeners (one versus more than one) who pro-
duce independent transcriptions of a recording; (ii)
how multiple transcripts are used—e.g. for cross-
checking or for comparing content; (iii) how such
cross-checks or comparisons are carried out; and
(iv) how any differences or divergences that are
identified between transcripts are resolved. The
principal aim of the present paper is to bring
these important issues into focus. We anticipate
that our observations and methods will be of par-
ticular interest to all the researchers who use
spoken language as research data, and hope that
the guidelines we propose for mitigating the prob-
lems associated with transcribing, and the compu-
tational techniques we describe for measuring and
maximizing adherence to those guidelines, will help
to enhance the consistency and accuracy of their
data.
In pursuit of this overall objective, we propose a
set of techniques that can assist in the process of
arriving at a transcription that is adequate for the
purposes in hand by quantifying the discrepancy
between different transcripts of the same recording
on an objective basis. We exemplify and test these
techniques by applying them to a series of spoken
interactions collected as part of an extensive longi-
tudinal study of dementia and cognitive ageing: the
Oxford Project to Investigate Memory and Ageing
(OPTIMA) http://www.medsci.ox.ac.uk/optima.
2 Methods
2.1 Overall procedures
The techniques we propose are embedded within a
set of overall procedures aimed at transforming
digitized sound recordings into reliable written re-
cords of linguistic events. For the sake of clarity,
these procedures are now summarized:
1 Capture discourse from study participants with
the best affordable audio equipment. [We do
not consider the additional complications intro-
duced by video recording, for which see Carter
and Adolphs (2008); Gorgos (2009).]
2 Establish and put in writing (or adapt from pre-
viously published guidelines) a preliminary set of
transcription conventions.
3 Arrange for two transcribers independently to
transcribe a sample of spoken interactions in the
form of digital text files.
4 Align pairs of transcript files using a modified
version of the Needleman–Wunsch algorithm
(Needleman and Wunsch, 1970).
5 Compute similarity scores using the F-measure
(Weiss et al., 2005) for benchmarking purposes.
6 Produce individual mismatch tables for amend-
ment of transcripts.
7 Produce an aggregate discrepancy table to suggest
improvements in transcription practices and/or
amendment(s) of transcription conventions. The
conventions developed by us using Steps 2–7 are
reproduced in Appendix A1.
8 Transcribe the full data set using the practices and
conventions agreed in Step 7.
According to this procedure, analysis of the lin-
guistic attributes of the transcripts and their rela-
tionships with other information (about the
speakers and the context of their interactions)
only begins after completion of the eight steps out-
lined above, the purpose of which is to ensure, as far
as possible, that the data derived from analysing the
transcripts will bear the evidential weight that is
likely to be placed upon it.
Techniques for transcribers
Literary and Linguistic Computing, Vol. 26, No. 4, 2011 391
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
2.2 Alignment and comparison of
transcripts
At the heart of our method is an automated tech-
nique for assessing the agreement between a pair
of transcripts. Quantifying the similarity between
two symbol sequences, however, is not a simple
matter: some authors use phrases such as ‘reliability
at 80% or greater’ (Williams et al., 2003) without
specifying how such figures are computed, despite
the fact that a single mismatch at any point can
throw both sequences out of alignment, resulting
in close to 0% agreement between the subsequent
symbols.
When authors quote a percentage agreement or
an index such as kappa (Cohen, 1960), this should
imply that the two symbol streams are aligned,
whether by human inspection or using some form
of automated procedure, such as the ‘compar-
e-and-merge-documents’ facility in Microsoft
Word, which can be used to obtain an optimal
alignment of each document pair, prior to calculat-
ing a similarity score.
There are limits to the usefulness of this ap-
proach, however, because as soon as the number
of mismatches rises above a certain level, the
software’s automatic highlighting becomes confus-
ing. This difficulty is illustrated in Fig. 1, which
shows an extract from a parliamentary interchange
involving the Minister of State for the Cabinet
Office on 1 July 2009. In this extract, word is com-
paring the official parliamentary report (Hansard,
Vol. 495 Col. 283, 1 July 2009), with a transcription
from audio recording following the conventions
described in Appendix A1.
2.2.1 Text alignment
The problems associated with Word’s built-in align-
ment software were overcome using a
special-purpose text alignment programme, written
in PYTHON 2.6. The programme implements a
variant of the Needleman–Wunsch algorithm
(1970), adapted to work on tokens rather than in-
dividual characters. The Needleman–Wunsch algo-
rithm is commonly used to align DNA or amino
acid sequences, though Sankoff and Kruskal
(1983) describe several other applications of the
family of dynamic-programming algorithms to
which it belongs. A dynamic programme works
in two phases: first it breaks a larger problem
into a series of subproblems, saving the optimal
solution found to each in a matrix (forward
Fig. 1 Comparison of a verbatim transcript of a broadcast debate from the House of Commons with the official record
of the debate published in Hansard, using the ‘compare and merge documents’ facility in Microsoft Word
P. Garrard et al.
392 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
phase); it then selects the solution that provided the
optimal overall cost (backward phase). A technical
description of the algorithm on which present
programme is based can be found in Cannarozzi
(2005).
Most algorithms of this sort use characters as the
atomic units of comparison, but ours works with
tokens as its base units, which entails a
pre-processing stage in which texts are broken
into sequences of tokens (mostly words, but also
sometimes punctuation symbols). The programme
also gives the option of folding all letters into lower
case, an option that is used in all the examples
quoted below.
Once the texts have been turned into token se-
quences, the alignment algorithm equalizes the
lengths of the two sequences (if they are of unequal
length) by inserting null strings into the shorter se-
quence. It then finds the maximum value for a
payoff/penalty function (which assigns a cost or
reward to the various kinds of match and mismatch
that can occur). The types of match and mismatch
that are recognized, the defining features of each,
and the payoff/penalty scores assigned to them,
are shown in Table 1. It will be noted that the
programme recognizes eight distinct relation-
ships and that it assigns scores at five levels,
from minus one to plus four, which are used as
heuristics to guide it towards an optimal alignment
solution.
To illustrate, the result of applying the algorithm
to the text strings:
The ancient Cretan palace ruins of Knossos
constitute a historical treasure trove (Text 1)
and
In the Minoan ruins of Knossos is an historic
treasure trove (Text 2)
is shown in Table 2.
The output consists of the two texts aligned ver-
tically with one another, together with the
Table 1 Types of match and mismatch recognized between pairs of aligned transcripts, together with defining criteria
and illustrative examples
Token comparison result (code) Definition Score
Long match (LM) An alignment of identical tokens of four or more characters 4
Short match (SM) An alignment of identical tokens of less than four characters 3
Long partial match (LP) An alignment between two tokens, one of which is a four-or-more- character-long
substring of the other
2
Short partial match (SP) An alignment between two tokens, one of which is a less-than-four-character-long
substring of the other
0
Short mismatch (SX) An alignment between two non-identical tokens, the shorter of which is less than four
characters long
0
Short gapa (SG) If a null token is aligned with a token consisting of four or more characters 0
Long gap (LG) If a null token is aligned with a token consisting of less than four characters 1
Long mismatch (LX) An alignment between two non-identical tokens, the shorter of which is four or more
characters long
1
aA ‘gap’ occurs when a null token is introduced into either sequence to achieve an optimal alignment score.
Table 2 Alignment between two sample texts showing the
classification of the token match or mismatch at each
point
Match or
mismatch type
Token
number
Text 1 Text 2
SG 0 [00] in
SM 1 the the
LG 2 ancient [00]
LG 3 cretan [00]
LX 4 palace minoan
LM 5 ruins ruins
SM 6 of of
LM 7 knossos knossos
SX 8 constitute is
SP 9 a an
LP 10 historical historic
LM 11 treasure treasure
LM 12 trove trove
SM 13 . .
Techniques for transcribers
Literary and Linguistic Computing, Vol. 26, No. 4, 2011 393
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
classification of the match / mismatch between each
token pair. All eight recognized match types des-
cribed in Table 1 are represented. The string ‘[00]’
is a place-holder used to indicate points at which the
programme introduced a gap during the alignment
process.
2.2.2 Quanitfying similarity in aligned texts
Once the two sequences have been aligned, the dis-
crepancies (deletions, insertions, and substitutions)
can be counted and used to obtain a variety of
similarity scores. We have experimented with a
number of agreement indices, of which the F1 meas-
ure (McCowan et al., 2005), a metric based on no-
tions of precision and recall as used in information
retrieval (IR), seems most appropriate as a general
index of agreement between symbol sequences.
The index ultimately derives from the work of
van Rijsbergen (1979) and is defined as
F1 ¼ ð2  p  rÞðp þ rÞ :
F1 is thus the harmonic mean of p (precision) and r
(recall), with precision and recall weighted equally,
and yields a value in the range from 0 to 1, where 0
indicates complete difference and 1 means complete
identity. Considered in the context of IR (for ex-
ample, a search for documents in a repository
such as the World Wide Web), precision is defined
as the proportion of the retrieved entries that are
relevant to the needs of the user, while recall is
defined as the proportion of all the relevant entries
that are retrieved. In most databases, the number of
items in the archive will greatly exceed the number
of relevant items. In the context of a comparison
between two independent transcripts, however,
this asymmetry disappears, since neither sequence
can be regarded as a ‘gold standard’, so p and r
become, in effect, interchangeable. Thus:
p ¼ h
n
and
r ¼ h
m
where h is the number of matching tokens (hits),
and n and m are the lengths (number of tokens) in
the first and second token sequences. In the
small-scale example given in Table 2, n is 13, m is
12, and h (the number of matches) is 7. Using the
above formulae:
p ¼ 7
13
r ¼ 7
12
F1 ¼ 2 7
13
 7
12
 
=
7
13
þ 7
12
 
¼ 98
175
¼ 0:56:
It should be noted that this measure is based on
all-or-nothing matching: either a pair of tokens is
the same or it is different. As noted above, the vari-
ous levels of match shown in Table 1 are used to
guide the alignment process; once aligned, the F1
measure is computed using a dichotomous distinc-
tion between matching and non-matching tokens.
2.3 Transcription data
Data used in the study came from a large set of re-
corded interviews with a group of volunteers in a
long-term study of the effects of ageing on cognitive
function and the incidence of AD—the OPTIMA
project (see above). Participants undergo clinical
and cognitive evaluations at roughly 6-monthly
intervals, over a number of years. At entry, some of
the participants have been diagnosed with AD or
other types of dementia, while other study entrants
enjoy normal cognitive function. Among the latter,
serial evaluation will disclose either stability or later
cognitive decline. Among the tasks participants are
asked to perform is the cookie theft picture descrip-
tion test [a component of the Boston Diagnostic
Aphasia Examination (Goodglass and Kaplan,
1983)]. In this test, participants are shown a line
drawing of a kitchen scene in which various events
are occurring: a daydreaming housewife is allowing a
sink to overflow, while a boy falls off a stool as, egged
on by his sister, he reaches into a jar labelled ‘cookies’
behind her back. Participants are shown the picture
and simply asked to ‘describe what is going on’.
From the database of participants who had been
interviewed at least five times we selected five
P. Garrard et al.
394 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
subjects. One was male (M1) and four female
(F1F4). The ages of the participants at the time
of the first and fifth dialogue selected for the present
analysis are given in Table 3. For each participant,
the portions of their first five interviews dealing
with the cookie theft description were transcribed
independently by two different transcribers, giving
twenty-five pairs of transcripts with which to inves-
tigate transcriber agreement.1
Although the cookie theft task is designed to
evoke a monologue by the participant, in practice
the interviewer often intervenes with comments,
prompts or ‘back-channel’ signals (Yngve, 1970),
producing a dialogue. As our interest extends to
dialogue, we took the decision to take as the text
to be processed all speech between the first and last
recorded utterance by the volunteer, including the
interviewer’s utterances within that stretch of talk.
A sample of this data—after processing by the align-
ment programme—is provided in Appendix A2.
This is the shortest of the twenty-five transcripts
referred to above.
Since both transcribers were attempting to record
the same speech events, full agreement is obviously
desirable, but there is no established baseline that
would specify how much discrepancy can be toler-
ated. To provide an empirical benchmark, we there-
fore collected a set of text pairs, of which each
member was, in some sense, a replication of the
other. These, along with the cookie theft descrip-
tions, are detailed in Table 4.
The selection of texts is entirely exploratory:
there are no strong theoretical reasons to predict
what range of scores might be expected in each cat-
egory. What can be said in advance of testing, how-
ever, is that the three categories of what might be
termed semantic equivalence (Biblical versions,
Poetry translation, and Sports press) would be ex-
pected to show to a high degree the phenomenon
mentioned in the Introduction section—to wit, the
tendency to replicate meaning rather than form.
This tendency should be less evident in
Parliamentary speeches, and less so still in the cate-
gories (Letters and Cookie theft transcriptions)
where faithful reproduction of the form as well as
the content of the discourse was a high priority. As
Table 4 Text pairs used for comparison, including the data of primary interest to this study
Category Description
Biblical versions Four extracts from the Bible (Genesis 1, Joshua 8, Psalm 101 and Romans 1) in the English Standard
Version (ESV) and the New International Version (NIV).
Parliamentary speeches Three short extracts from unscripted parliamentary exchanges in the House of Commons: one
transcribed from audio and the other reproduced from the official record (Hansard).
Letters Five letters handwritten by the novelist and philosopher Iris Murdoch to a friend, independently
transcribed from manuscript by two transcribers.
Cookie theft descriptions Twenty-five independently transcribed cookie theft description dialogues by OPTIMA participants
(see text).
Plagiarism Three student essays and the texts which they were later found to have plagiarized.
Poetry translation Pairs of translations into English of poems—three from Hungarian and one from Chinese.
Sports press Pairs of reports of three sporting events (Rugby League: England versus Australia, 1 November 2009;
Boxing: Haye versus Valuev, 8 November 2009; Football: Brazil versus England, 15 November 2009)
published in two different British Sunday newspapers (The Independent on Sunday and The Sunday
Times)
OCR output Three articles from The Times online archive (one from 1912 and two from 1917), one version
transcribed manually from the page images and the other using optical character recognition (OCR)
software.
Table 3 Ages of participants whose descriptions were
used in the study
Participant Age at
first transcribed
episode
Age at fifth
transcribed
episode
M1 44 48
F1 53 63
F2 59 65
F3 64 69
F4 79 84
Techniques for transcribers
Literary and Linguistic Computing, Vol. 26, No. 4, 2011 395
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
for the examples of Plagiarism and OCR output,
these can vary depending on numerous factors,
such as the skill of the plagiarist in disguising his
or her source, the font and quality of the texts sub-
ject to OCR, and the sophistication of the OCR
procedure. This exercise thus provides a set of base-
lines against which to judge the performance of the
similarity scoring algorithm as applied to English
texts.
3 Results
3.1 Similarity scoring
The F1 scores of the twenty-five pairs of OPTIMA
transcripts ranged from 0.6842 to 0.9787 with a
mean of 0.87, a median of 0.87 and a standard de-
viation of 0.06. As it is rare for transcription agree-
ment to be quoted in this manner, the same
measures were computed for the other text group-
ings listed in Table 3, above, in order to provide a
context for interpreting these similarity scores.
Figure 2 displays a boxplot of the eight sets of re-
sults. Inspection of the figure confirms that the
mean F1 measure for the cookie theft transcripts
was the highest of the eight groups.
As expected, reporters working for different
newspapers who write about the same sporting
event have the lowest level of formal similarity,
less even than free poetic translations, and much
less than plagiarized material. Of the two groups
that compared different translations of the same
text, the biblical translations were noticeably more
concordant than the poems. A plausible post hoc
explanation for this is that translators of sacred
scripture have to pay very close attention to the lit-
eral meaning of the original text and to the wording
of earlier authoritative translations, while translators
of secular poetry are free to rephrase the gist of the
original in their own words, indeed are encouraged
to use the original poem as a launch pad for their
own creative efforts.
One of the most pertinent contrasts is between the
cookie theft data and the House of Commons
Fig. 2 Parallel boxplots (median, upper and lower quartiles, and range outliers) of the similarity measure (F1)
between eight types of text pair, in which each member of a pair is some form of replication of the other. The highest
similarity values are associated with independent transcriptions of verbal (cookie theft descriptions) and written (letters
of Iris Murdoch) material; the lowest with descriptions of the same sporting event by two different sports journalists
P. Garrard et al.
396 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
transcripts. In both, two independent transcribers
have produced written versions of the same dialogue
and the F1 scores are similar, though not as close of
those associated with handwritten letters and optic-
ally recognized news stories, which fall within the
lower to upper quartile range of the OPTIMA tran-
scripts. For all the other groups (including the House
of Commons transcripts), the upper quartile has a
lower score than the lower quartile of the OPTIMA
transcripts. A Wilcoxon signed-rank test confirms
that the OPTIMA and Commons subgroups differ
significantly (W¼ 65, P¼ 0.0409, two-tailed) with
respect to F1 scores. Whether this first-pass level of
agreement is satisfactory depends on the uses to
which the data will be put as well as the attributes
of the transcribed language extracted for analysis, but
it is encouraging that our transcripts, according to
this measure, represent a more nearly verbatim
record than official parliamentary proceedings.
Still, a higher overall degree of similarity than the
Hansard transcripts may not be a level to be content
with, as it is well-known that verbal exchanges in
parliament are subject to systematic ‘rectifications’
by the Hansard editorial staff, including but not lim-
ited to, the deletion of non-lexical fillers (such as ‘er’
and ‘um’), repetitions (such as ‘I I’) and uncom-
pleted phrases, and the replacement of ‘have to’
by ‘must’ and phrasal verbs by single lexical items
(e.g. ‘look at’ by ‘consider’ or ‘examine’) (Mollin,
2007).
The purpose of the text-comparison software
is not merely to assess similarity but also to indi-
cate how it might be enhanced, and the value of a
similarity scoring system such as this is that it can
identify and quantify commonly occurring inconsis-
tencies among transcribers. Studying such inconsis-
tencies allows us to take a further step towards
improving transcription practice.
3.2 Identifying mismatches
To facilitate correction of errors in individual cases,
a programme was written (again in PYTHON 2.6)
that took output from the alignment programme
and produced, for each transcript-pair, a table of
mismatches, to enable reference back to the episodes
in which they occured. A total of 342 mismatches
were found across the full set of twenty-five pairs of
transcriptions. An example of the programme’s
output relating to a single pair of transcriptions is
shown in Table 5. The two transcript sources are
labelled 1 and 2 (though neither is considered to
be the ‘preferred’ version at this stage). The aim of
tabulation is to provide an initial focus for later
attempts at improving consistency.
The programme can also be used to aggregate
discrepancies of the same type and produce a table
of such discrepancies in descending order of fre-
quency. The combined table for the twenty-five
transcripts examined here is displayed in Table 6
(with discrepancies that occur only once omitted
to save space).
The first three columns of the table contain
output from the programme. The last column,
which is labelled ‘origin of mismatch’, contains a
presumed interpretation of the mismatch. This
means that a human reader can consider whether
a common cause is likely to give rise to a number of
mismatches of a certain type. Sorting the table by
Table 5 Sample mismatch table for a single pair of cookie
theft transcripts after optimal alignment
Transcriber 1 Transcriber 2
.. [00]
[?well] Well
n: [00]
mhhm [00]
p: [00]
n: [00]
mhhm Um
p: [00]
er [00]
[00] Going
gonna To
taking [00]
out Taken
of From
.. [00]
and [00]
.. Um
drying Dyring
er [00]
which Um
As in Table 2, ‘[00]’ indicates a point at which optimal alignment
necessitated the introduction of a gap.
Techniques for transcribers
Literary and Linguistic Computing, Vol. 26, No. 4, 2011 397
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
entries in this column allows the common causes of
discrepancy to be shown together (Table 7). It will
be seen that this table now includes a new column
containing potential remedies that could remove
some types of mismatch or mitigate their effects.
By considering the frequencies associated with
each potential remedy, ‘low-hanging fruit’ (i.e.
simple changes with non-trivial effects) can be
identified.
For example, if we were to ignore pauses, the
table shows that we would avoid almost 10% of
the mismatches [twenty-nine cases where
Transcriber 1 has recorded a pause (‘..’) while
Transcriber 2 has not, plus the two cases where
the reverse is true]. It is also of passing interest to
note that when the presence or absence of a token
depends on a subjective judgement, it is inevitable
that transcribers will have different thresholds; it is
at least reassuring that Transcriber 1’s lower thresh-
old for recording pauses appears to be consistent!
Fillers are another common source of discrepancy,
and if unified (i.e. all treated as equivalent),
then only five mismatches would be saved. In con-
trast, if fillers were ignored altogether, a further
twenty-three mismatches would be eliminated.
Importantly, both of these changes could be
achieved by post-processing, without the need for
re-transcription. Further reflections on the implica-
tions of such results are presented in the following
section.
4 Discussion
In this article, we have outlined an approach to
transcription, aimed at mitigating one of the main
problems of transcribing, namely disagreement be-
tween transcribers. While acknowledging that agree-
ment does not guarantee accuracy, we would argue
that disagreement between transcribers is usually a
sign that some aspect of the transcription process re-
quires re-examination. We have therefore outlined
an approach that enables classes of discrepancy to
be identified automatically and systematically, and
presented computational techniques for performing
such automatic identifications. We have imple-
mented these techniques in a suite of software mod-
ules and subjected them to an initial trial by
applying them to real-world data from an important
area of medical research.
Table 6 Table of discrepancy types occurring between all
twenty-five cookie theft transcript pairs, ordered by
number of occurrences (n)
Transcriber
1
Transcriber
2
n Origin of mismatch
.. [00] 29 Pause recorded by T1 only
[00] is 10 Contraction expanded, T2 only
and [00] 7 Insertion by T1
n: [00] 7 Speaker change, T1 only
the [00] 7 Insertion by T1
a the 6 Word mismatch
er [00] 6 Filler, T1 only
p: [00] 6 Speaker change, T1 only
um [00] 6 Filler, T1 only
[00] boy 5 Contraction expanded, T2 only
[00] day 5 Insertion by T2
[00] the 5 Insertion by T2
um hm 5 Filler mismatch
[00] n: 4 Speaker change, T2 only
boy’s is 4 Contraction expanded, T2 only
the a 4 Word mismatch
um and 4 Filler-word mismatch
[00] a 3 Insertion by T2
[00] p: 3 Speaker change, T2 only
falling fallen 3 Word mismatch
i [00] 3 Insertion by T1
? [00] 2 Punctuation, T1 only
[#] [00] 2 Unintelligible to T1 only
[00] . 2 Punctuation, T2 only
[00] .. 2 Pause recorded, T2 only
[00] [¼laugh] 2 Non-verbal vocalization, T2 only
[00] and 2 Insertion by T2
[00] er 2 Filler, T2 only
[00] for 2 Insertion by T2
[00] me 2 Insertion by T2
all [00] 2 Insertion by T1
and um 2 Word-filler mismatch
daydreaming dreaming 2 Word break/hyphenation
huh [00] 2 Filler, T1 only
huh ha 2 Interjection mismatch
in [00] 2 Insertion by T1
kids kid’s 2 Word mismatch
mm [00] 2 Filler, T1 only
on [00] 2 Insertion by T1
stool’s stool 2 Contraction expanded, T2 only
that’s [00] 2 Insertion by T1
well [00] 2 Insertion by T1
window’s window 2 Word mismatch
You [00] 2 Insertion by T1
P. Garrard et al.
398 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Using a novel method of text alignment and
quantification of agreement, the level of similarity
between cookie theft transcribers in the samples
used in this study would appear to be acceptable:
the degree of difference between transcripts is
overall less than that between Hansard and a verba-
tim transcription. This suggests that the transcrip-
tion conventions that we outline in Appendix A1
provide for an acceptable degree of reproducibility.
It is also worth remarking that there were very few
Table 7 Discrepancy table with suggested remedies
Transcriber 1 Transcriber 2 n Origin of mismatch Potential remedy
stool’s stool 2 Contraction expanded, T2 only Expand contracted ‘is’
boy’s is 4 Contraction expanded, T2 only Expand contracted ‘is’
[00] boy 5 Contraction expanded, T2 only Expand contracted ‘is’
[00] is 10 Contraction expanded, T2 only Expand contracted ‘is’
Um hm 5 Filler mismatch Ignore/unify fillers
Um and 4 Filler-word mismatch
Huh [00] 2 Filler, T1 only Ignore/unify fillers
Mm [00] 2 Filler, T1 only Ignore/unify fillers
Er [00] 6 Filler, T1 only Ignore/unify fillers
um [00] 6 Filler, T1 only Ignore/unify fillers
[00] er 2 Filler, T2 only Ignore/unify fillers
all [00] 2 Insertion by T1
in [00] 2 Insertion by T1
on [00] 2 Insertion by T1
that’s [00] 2 Insertion by T1
well [00] 2 Insertion by T1
you [00] 2 Insertion by T1
i [00] 3 Insertion by T1
and [00] 7 Insertion by T1
the [00] 7 Insertion by T1
[00] me 2 Insertion by T2
[00] for 2 Insertion by T2
[00] and 2 Insertion by T2
[00] a 3 Insertion by T2
[00] the 5 Insertion by T2
[00] day 5 Insertion by T2
huh ha 2 Interjection mismatch Ignore/unify interjections
[00] [¼laugh] 2 Non-verbal vocalization, T2 only
.. [00] 29 Pause recorded, T1 only Ignore pauses
[00] .. 2 Pause recorded, T2 only Ignore pauses
? [00] 2 Punctuation, T1 only Remove punctuation
[00] . 2 Punctuation, T2 only Remove punctuation
p: [00] 6 Speaker change, T1 only Analyse only p’s text
n: [00] 7 Speaker change, T1 only Analyse only p’s text
[00] p: 3 Speaker change, T2 only Analyse only p’s text
[00] n: 4 Speaker change, T2 only Analyse only p’s text
[#] [00] 2 Unintelligible, T1 only
daydreaming dreaming 2 word break/hyphenation
and um 2 Word-filler mismatch
kids kid’s 2 Word mismatch
window’s window 2 Word mismatch
falling fallen 3 Word mismatch
the a 4 Word mismatch
a the 6 Word mismatch
Techniques for transcribers
Literary and Linguistic Computing, Vol. 26, No. 4, 2011 399
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
misspellings or typographical errors (n¼ 3), so a
spell-check would not significantly improve our
level of agreement.
Nevertheless, there remain aspects of our tran-
scription practices which are clearly less than ideal,
particularly the recording of paralinguistic features.
In particular: (i) pauses were not recorded consist-
ently; (ii) coding of fillers (such as ‘er’ and ‘um’)
was not consistent between transcribers; (iii) coding
of interjections (such as ‘ha’ and ‘huh’) was also
inconsistent between transcribers; (iv) there was dis-
agreement about what constituted a backchannel
signal (e.g. ‘mhhm’); (v) other non-verbal vocaliza-
tions are not recorded consistently (e.g. ‘[¼laugh]’);
and (vi) some changes of speaker are indicated by
only one transcriber.
It appears, therefore, that the recording and
representation of various types of paralinguistic fea-
ture in transcription is somewhat idiosyncratic,
and thus unreliable, suggesting that they should be
removed in the interests of consistency. This is not
to imply that such data are not useful markers;
indeed Arciuli et al. (2010) showed that, in a labora-
tory setting, the frequency of the filler ‘um’ was
the most important discriminator between state-
ments made when subjects were instructed to
tell the truth and those made when they were in-
structed to lie. If similar importance can be shown
to attach to such items in the context of normal
versus pathological age-associated language
change, then clearly this decision would need to
be revised, and more stringent guidelines for their
representation included in the transcription guide-
lines instead.
We started out with the aim of recording the
dialogue between a patient (p:) and nurse (n:),
that almost always results from the performance of
the cookie theft task. It is worth noting in this con-
nection that in two of the three episodes with
the lowest similarity scores, these low scores resulted
from a final non-verbal utterance of the patient
(‘mm’ and ‘[¼laugh]’, respectively) being
omitted by one or other transcriber. Thus, in both
cases an unmatched utterance by the nurse was
included in one transcript but not the other.
(Transcript 137BS_01/07/96 is reproduced in
Appendix A2.) This side-effect of imperfect
agreement on non-verbal items, leading to a dis-
agreement about where the transcript ends,
would have minimal effect on any analysis that
confined itself only to the utterances of the
patient, which in fact are our main subject of
concern.
As far as the more lexical side of the data is con-
cerned, our transcripts agree quite well, though two
areas require specific attention: (i) contractions and
genitives, such as ‘‘boy’s’’ versus ‘boy is’, ‘gonna’
versus ‘going to’, ‘‘sink’s’’ versus ‘sinks’ and
‘‘you’ve’’ versus ‘you have’; and (ii) hyphenation
and word-separation, such as ‘all right’ versus
‘alright’, ‘overrunning’ versus ‘over-running’ or
‘thank you’ versus ‘thankyou’. There is no a
simple practical way of standardizing these discre-
pancies post hoc, though one could do a guided
search for places where they might be found.
Point (ii) could also be addressed by expanding all
enclitic versions of ‘be’ and ‘have’, such that ‘‘boy’s’’
is rendered as ‘boy is’ or ‘boy has’ (except when
context indicates a possessive). Such a restriction
would, however, move us further towards the
Hansard-style approach of recording what we
think the speakers ought to have said and away
from the ideal of verbatim transcription.
The central aim of these techniques was to pro-
vide information in a consistent fashion which will
assist human reviewers to reflect on their transcrip-
tion practices in a rational manner, ideally before
making an expensive commitment to a large-scale
transcription exercise. Such reviewing may lead to
revisions that improve agreement and thereby in-
crease the reliability of the transcripts. But revising
the transcripts may not be the only outcome.
Other reasonable responses might include adapting
proposed transcription practices, amending the
guidelines for transcribers, or avoiding analyses
based on phenomena that have not been reliably
recorded.
We do not claim to have solved the problems
of transcription, nor do we offer a rigid prescrip-
tion for all kinds of transcribing. We have, how-
ever, made explicit the stages required by a
systematic approach to obtaining greater reliabil-
ity in this richly informative source of language
data.
P. Garrard et al.
400 Literary and Linguistic Computing, Vol. 26, No. 4, 2011
 at U
B Leipzig on January 4, 2012
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Funding
This work was supported by a UK Medical Research
Council research grant (grant number G0801370) to
P.G. and C.deJ.
Acknowledgements
Thanks are due to Dr R. Forsyth for writing the
PYTHON scripts, and to the University of
Southampton Medical School, where much of this
research was carried out.