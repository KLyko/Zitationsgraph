1 Introduction
The First International Conference of the Alliance
of Digital Humanities Organizations (ADHO) was
special in many ways, not least because it was the first
time that the Association for Computers and the
Humanities (ACH) and the Association for Literary
and Linguistic Computing (ALLC) convened their
joint conference under this new name. Held at the
Universite´ Paris-Sorbonne, Digital Humanities
2006 (to give it its shorter title) was organized by
Liliane Gallet-Blanchard, Marie-Madeleine Martinet
and other members of Cultures Anglophones et
Technologies de l’Information (CATI). There were
214 participants from twenty-four countries. One
hundred and twenty papers and sessions and thirty-
one posters were presented.
As with previous ACH/ALLC joint conferences,
Digital Humanities 2006 did not have a pre-defined
theme. Instead, the Programme Committee (headed
by Lisa Lena Opas-Hanninen, University of
Joensuu) maintained the well-established tradition
of encouraging papers, posters and panels on any
aspect of digital humanities, including:
 text analysis, corpora, language processing,
 IT in librarianship and documentation,
 computer-based research in literary, cultural and
historical studies,
 computing applications for the arts, architecture
and music,
 information design and modelling,
 (the cultural impact of) new media, and
 humanities computing in academic curricula.
Whilst the presentations given at Digital
Humanities 2006 confirm that digital humanities
work is being undertaken across the whole spectrum
of humanities research (broadly defined), a number
of areas—namely, (1) literature and linguistics (and,
in particular, computational stylistics), (2) comput-
ing applications for the arts and architecture, (3) the
development of new/evaluation of existing digital
resources (especially literary and/or historical) and
the ongoing importance of maintaining, managing
and preserving such resources—were particularly
well represented. In the following sections, I high-
light a number of paper/poster presentations that
ably represented (1)–(3).
2 The Emergence of an Implicit
Theme at the Paris Conference
An additional, if implicit, theme quickly emerged:
namely, the challenge of learning to speak the
diverse ‘languages’ of the humanities and ICT
disciplines, so that we might better understand
each other’s needs and/or aspirations. Indeed, the
‘call’ for more effective cross-disciplinary commu-
nication was initially voiced by Tapio Sepannen
(University of Oulu) in the opening plenary,
Multimedia Information Retrieval, and became
stronger—and more inter-disciplinary in focus—in
the course of the conference (as the 214 attendees
were treated to 150 papers and posters, all of
an admirable quality.) For example, Patrick
Juola (Duquesne University) suggested that
Correspondence:
Dawn Archer, Department of
Humanities, University of
Central Lancashire, Fylde 409,
Preston PR1 2HE, UK.
E-mail: dearcher@uclan.ac.uk
Literary and Linguistic Computing, Vol. 23, No. 1, 2008.  The Author 2007. Published by Oxford University Press on
behalf of ALLC and ACH. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org
doi:10.1093/llc/fqm037 Advance Access Published on 14 December 2007
103
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
‘miscommunication’ was a potential problem for
researchers within the humanities community as a
whole, when he identified a ‘perceived neglect on
the part of the broader humanities community’ in
respect to Digital Humanities applications. Juola is
convinced that this neglect is indicative of a
mismatch of expectations between the actual needs
of the wider humanities community and what we
(within the Digital Humanities community) per-
ceive them to need from the tools we develop. His
‘answer’, at the conference, was to encourage fellow
attendees to develop Killer Applications in Digital
Humanities, which fill a genuine need of the
humanities community and, by so doing, create an
acceptance of that tool and the supporting methods/
results. Papa, Warwick, Terras and Huntington
(University College London) also highlighted The
(in)visibility of Digital Humanities Resources in
Academic Contexts in their Paris presentation.
However, they suggested that, prior to building
more resources, we should first undertake research
to determine whether (and if so, how) the hundreds
of digital resources/tools that have already been
developed are utilized by those outside of the
humanities computing community. They have
already undertaken a project, entitled LAIRAH,
which seeks to analyse user uptake of a number of
online digital resources. In a similar vein, Bei Yu
and John Unsworth (University of Illinois) high-
lighted (what they described as) a ‘chicken-and-egg’
dilemma: a need for more collaboration in order to
produce tools that are useful to the Humanities
scholar against a need for tools that might promote
collaboration. As will become clear in the next
section, they also went on to suggest that data
mining techniques might provide a useful means
of determining potential areas of collaboration
between the Digital Humanities community and
the wider humanities community.
3 Literary and Linguistic
Computing
Although there was a diverse range of linguistics-
related studies, a number of papers touched
upon computational stylistics, in particular. For
example, David Hoover (New York University), in
his paper Stylometry, Chronology and Styles of
Henry James, described ways in which we might
use (an amended version of) Burrow’s Delta
to determine the evolving style of authors.
However, he also suggested that grouping words
semantically may offer an additional step towards
accurately determining an evolution of style;
in particular, style as it relates to characterization.
Other Burrows-related presentations included
Sterling Stein and Shlomo Argamon’s (Illinois
Insititute of Technology) Mathematical Explanation
of Burrows’ Delta.
The nora project was particularly well repre-
sented at the conference, with a special session
devoted to Text Mining and Literary Interpretation.
As previously intimated, Yu and Unsworth
also debated Potential Data Mining Applications
in Literary Criticism in an additional presentation.
Using Buckland’s (1999) idea that ‘vocabulary
is a central concept in information transition
between domains’ as their starting point, their
approach has been to compare the vocabularies of
(and, in particular, the verbs utilized by) ‘data
miners’ and ‘literary scholars’, via keyword analysis,
from which they hope to identify potential activities
that will be of interest to both communities.
I cannot help but wonder whether a supplementary
product of such work (to be undertaken by
members of the nora team and/or others) might
be an indication of lexical items that (1) mean
different things in the two communities, (2) are
unknown to one of the communities, and/or (3) are
indicative of different (cultural) worldviews/belief
systems. For such work may help bridge any
potential ‘miscommunication’ gap(s).
Louisa Connors (University of Newcastle,
Australia) is also interested in bridging the gap
between different communities: indeed, the primary
aim of her presentation was to encourage partici-
pants to consider Combining Cognitive Stylistics
and Computational Stylistics. As such, she echoed
both the call for more effective cross-disciplinary
communication initially voiced by Sepannen
(see above), and the lack of uptake of computational
techniques within the more traditional disciplines;
in this instance, literary criticism. Connors offered
D. Archer
104 Literary and Linguistic Computing, Vol. 23, No. 1, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
a way forward for both the greater uptake of
computing techniques within the latter and also
‘a more interpretive framework [for] computational
stylistics’ that was extremely thought-provoking:
she suggested that we use cognitive linguistics
(and the idea of authorial agency that she believes
it assumes) as a means of simultaneously debunking
the idea of the death of the author (Barthes, 1977)
and providing a theoretical justification for a focus
on function and form (and, by so doing, justifying
computational approaches to literary analysis that
were interpretively ‘rich’).
4 Computing Applications for
the Arts and Architecture
There were a small number of presentations relating
to computing applications for the arts and archi-
tecture. One of the most notable architectural
examples was Robert Tavernor’s plenary presenta-
tion Visualising 21st Century London: Tavernor
described how London planners are increasingly
utilizing visualization methods as a (better) means
of representing the urban landscape when assess-
ing (the potential impact of) proposed new
developments.
In contrast to Tavernor, Stef Gard, Sam Bucolo
and Theodor Wyeld (University of Queensland
Australasian CRC for Interaction Design) reported
on a virtual project that seeks to integrate key
cultural elements in a synthetic environment repre-
sentative of Australian indigenous groups. Interest-
ingly, cultural differences in perception—relating to
the importance of detail when (re-)presenting the
landscape—have had to be accounted for when
creating this particular virtual environment as
a means of ensuring ‘contextual accuracy’. I found
their approach—that of learning from the Abori-
ginal people themselves and also revising their usual
academic practices of collecting cultural knowl-
edge—a useful reminder of the importance of
‘learning to speak one another’s language’ in a
way that takes account of inter-/cross-cultural
differences.
An excellent paper relating to the use of com-
puting tools for organizing and accessing history of
art information was given by Nuria Rodrı´guez
Ortega (University of Ma´laga) and Alejandro
Bı´a and Juan Malonda (University of Miguel
Herma´ndez). The presenters initially emphasized a
recurring problem within the field of art history
(that of terminological and discursive ambiguity/
vagueness), and then documented two complimen-
tary tools they have developed: The Tesauro
Terminolo´gico-Conceptual (TTC) that provides
art specialists and other users with a taxonomy of
art-related terms and concepts, and a textual
database that provides their context-in-use (thus
further aiding understanding). Future plans include
further developing the textual database so that it
becomes ‘an exhaustive repository of pictorial and
artistic texts that could serve as a basic reference tool
for theoretical and critical studies, and, more
generally, for research dealing with texts, sources
and documents’.
In case participants were of the impression that
this is one particular area of Humanities where there
has been extensive uptake of digital applications,
Martyn Jessop (King’s College London) reminded us
that ‘research opportunities offered by spatial and
spatial–temporal data remain relatively unexplored’,
in spite of being freely available at relatively low cost.
His paper at the conference was largely devoted to
the factors which he believes are inhibiting mean-
ingful collaboration across disciplines. These factors
include weaknesses in existing methodologies,
problems associated with knowing about/finding
the data that already exists, poor transference of
research practices across the disciplines and the
problem of encouraging institutions to support
researchers, who wish to explore such areas, given
the start-up/maintenance costs and a fear that such
research may not be deemed to be prestigious enough
(in research assessment terms). Jessop suggested that
one way forward (in this particular instance) might
be the development of a virtual ‘one-stop shop’,
which provides a clear statement of advantages
expressed in distinct (humanities-related) research
outcomes; a set of sample projects, detailing the
methodology used, with suggestions as to how
that methodology might be transmitted between
disciplines; and links to data sources and ‘useful’
tools.
Digital Humanities 2006
Literary and Linguistic Computing, Vol. 23, No. 1, 2008 105
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
The sessions devoted to the Alliance of Digital
Humanities Organizations1 (ADHO) deserve special
mention at this point, as they showed an awareness
of the importance of disseminating good practice
in the Digitized Humanities community whilst
also promoting the Digitized Humanities approach
to the wider Arts and Humanities communities
(and beyond). Indeed, there are plans to use the
Digital Humanities Quarterly e-journal as a means of
attracting a more general audience. The e-journal is
an especially useful tool in this regard, as contribu-
tors will be able to show ‘how’ their models work
as well as the types of results that are achievable. The
new book series, Topics in Humanities Computing,
may also provide an opportunity for academics
from the traditional and digital humanities to
explore the same topics from their respective
perspectives, with the spin-off potential of high-
lighting the most effective/useful ‘cross-over’ points
for particular disciplines.
5 Issues Relating to the
Development of New/Evaluation
of Existing Digital Resources
and the Ongoing Importance
of Maintaining, Managing
and Preserving Resources
The need for the design and implementation of
a framework for creating and managing digital
resources seems to be a recurrent theme at the
ALLC/ACH conferences, and the Digital Human-
ities 2006 conference was no exception. Several
papers raised issues relating to the use of TEI. By
way of illustration, Fabrio Franceschini and Elena
Pierazzo (University of Pisa) reported a study which
makes use of lexical entries to analyse the language
of young people. Significantly, however, much of
their presentation was devoted to the difficulties of
finding suitable storage and retrieval solutions for
their material (i.e. questionnaires and word lists,
which trace innovative forms to their relevant
lemmas). In particular, they highlighted the
problems they had encountered when using the
TEI Terminological Databases tag set and their
justification for opting for a relational database
structure (which they call BaDaLi). Claire Carline,
Eric Haswell and Martin Holmes (University of
Victoria) also highlighted problems they had
experienced when using TEI and SVG to annotate
verse and prose polemics (relating to seventeenth-
century French engravings). Nonetheless, they were
careful to point out the overall benefits of a scheme
(like TEI) which ‘encourages customization of its
guidelines to accommodate [ . . . ] a wide range of
implementations’. Syd Bauman (Brown University)
and Lou Burnard (Oxford University) also informed
participants of what they can expect from the newly
released TEI P5, in their paper: TEI P5: What’s in
It for Me?
It was gratifying to see that many presentations
on the development of new/evaluation of existing
digital resources highlighted the importance of
theoretical as well as technical considerations: for
example, Tyler Kendall (Duke University) and
Amanda French (North Caroline State University)
described how their aversion to the hypothesis that
transcripts are ‘unbiased representations of the data’
(cf. Edwards, 2001: 321) has led them to develop
a ‘proof of concept’ tool [involving a MySQL
database, application pages written in PHP and
open source phonetic software (Praat)] that links
transcript text to audio data at the utterance-unit
level and, by so doing, provides a more robust (and
linguistically accurate) tie between the audio data
and the analysts’ representations of it. It is their
belief that their NC SLAAP software has ‘potentially
tremendous implications for a wide range of
linguistic approaches’. The next step, presumably,
is to encourage academics within these linguistic
sub-disciplines to try out the tool for themselves,
and then provide the kind of feedback that will help
further develop it (in a way that is useful for these
different sub-fields).
Interestingly, Stan Ruecker sought to encourage
participants to reflect upon the different phases of
the processes we engage in as part of our digital
humanities work. In particular, he highlighted the
implicit messages that we might be giving through
D. Archer
106 Literary and Linguistic Computing, Vol. 23, No. 1, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
the terminology we utilize (for example, ‘user’), and
encouraged participants to draw from the insights
of Gibson (1979), Davis (1989), and others in
respect to the human experience when engaging
with technology (Ruecker particularly emphasized
the importance of an approach that seeks to
combine functionality and usability with a strong
‘pleasure’ component).
6 A Note about . . . the Social
Aspect
I cannot write a report about a conference in Paris
without saying something about the city itself. As
well as being able to enjoy the beauty of the
buildings in which the different parts of the
conference were held, the Local Organizing
Committee organized some excellent events, includ-
ing a visit to the Louvre, a banquet on the Bateaux-
Mouches, and (for those who could stay beyond the
conference proper) an excursion to Monet’s house
at Giverny, all of which showed off Paris and its
surroundings to perfection. Indeed, it provided one
more reason why this conference was, quite simply,
not-to-be-missed!
Those unfortunate readers who were not able to
attend the conference missed not only the social
events but also the animated discussions that
followed many of the presentations. However, one
can catch a sense of the quality of the conference by
reading the conference abstracts (available from
http://www.allc-ach2006.colloques.paris-sorbonne.
fr/DHs.pdf).
7 And Finally . . . Looking Back to
Look Forward
As this report has revealed, there is a strong
international element to both this conference and
to ADHO and its affiliated organizations. For
example, in addition to the countries represented
by the papers mentioned above, there were
representatives at Digital Humanities 2006 from
Austria, Bulgaria, China, Gambia, Germany, Greece,
Ireland, Japan, Netherlands, Norway, Poland,
Serbia, Slovenia, Sweden and Switzerland.
What is also clear is that, although the newly
established Digital Humanities conference seems to
have maintained its strong links with its literary and
linguistic computing past, there is a real desire to
embrace digital humanities in its broadest sense
(hence the choice of plenary speakers).
Interestingly, in her report of the ACH/ALLC
2001 conference, Terras emphasized that ‘Human-
ities Computing [was] still a very young, developing
field’, which nevertheless was driving (rather than
being driven by) humanities research. She also
reported the excitement felt at that conference, as
participants contemplated the future. Five years
later, Terras was among many to adopt a seemingly
more ‘restrained’ stance in respect to the influence
of the Digital Humanities within the wider Human-
ities community. But the awareness that there are
potential struggles still to overcome should be
viewed as constituting a positive (rather than a
negative) step, in my view. Indeed, I would argue
that such critical self-evaluation is a sign that we are
beginning to truly understand the importance of
‘learning to speak one another’s language’ in a way
that leads to multi-cultural competence. And, once
we can speak cross-culturally with clarity, we will
have a better understanding of the collective wants,
needs and aspirations of the wider Humanities
community, as well as our own.
