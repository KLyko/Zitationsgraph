
1 The Question that Won’t Go
Away
I begin with a question that won’t go away: what’s
going on? This is the question with which Canadian
sociologist Erving Goffman would typically begin.
Like him, though with different interests, I acknowl-
edge that by asking it I bias matters toward a single
explanation and imply a degree of authority no
answer to such a question could possibly have
(Goffman 1997/1974, pp. 153–5). But one must
start somewhere. Where we begin is with the
claims, counter-claims, failures, and achievements
in computing. Here the work to be done is to
make sense of experience and to form an idea of
what matters for the humanities in the long haul.
My own conclusion is that it is actually quite impor-
tant that we pay attention. ‘For the interesting
puzzle in our times’, Langdon Winner (1997/1986)
has written,‘is that we so willingly sleepwalk through
the process of reconstituting the conditions of
human existence’ (p. 61). That is an enormous
claim but, I think, no exaggeration: ‘in designing
tools we are designing ways of being’ (Winograd
and Flores, 1986, p. xi). Computing is not the
only means we have of doing that, but it is certainly
a big part of the toolkit present at hand.
There is so much going on now, so much at stake
and so many interpreters about, that I think it is best
to be cautious and, where possible, to speak plainly,
though so much is poorly understood that plain
speaking on this subject is difficult. Such, anyhow,
is my aim. I will take a relatively conservative reading
on the trajectory of computing for the humanities,
say what I think our choices are and conclude
with the simple recommendation that we design
institutional structures for the digital humanities
according to the nature of their emergent practice.
2 Modelling
The question of what is happening simply won’t go
away for a couple of reasons. First, to quote Jerome
Bruner’s (1986) characterization of the humanities,
when asked persistently enough, this question opens
up ‘the alternativeness of human possibility’ (p. 53),
Correspondence:
Willard McCarty, Professor
of Humanities Computing,
Centre for Computing in the
Humanities, King’s College
London, 26-29 Drury Lane,
London WC2B 5RL, UK.
E-mail:
Willard.McCarty@kcl.ac.uk
Literary and Linguistic Computing, Vol. 23, No. 3, 2008.  The Author 2008. Published by Oxford University Press on
behalf of ALLC and ACH. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org
253
doi:10.1093/llc/fqn014
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
which is without end. Second, it is being asked of
computing—by which I do not mean particular
hardware or software in front of you, or in you,
or in your car, wrist-watch, refrigerator or ubiqui-
tously wherever, or what we can expect in the next
release, or even what we can predict. When I say
‘computing’ I mean the evolving manifestations of
Alan Turing’s scheme for the design of indefinitely
many devices, limited only by human ingenuity. In
the history of inventions, computing is in its
infancy, and its products incunabular. The point I
am making is that they always will be, however pro-
gressively better they get.
But, as with the fairytale pouch that can never be
emptied of coins, the consequential matter is what
is done with computing. To know that, we must
know what it can do for the humanities, given the
kind of coin that it is.
First some basics. For a computer to do anything
useful at all, it must have a formalized plan of action
(which we call a ‘program’) that represents in sim-
plified form whatever object or process one is inter-
ested in. Devising a model, as we call such a
representation, involves a trade-off between what
the medium can do and what we can envision
doing—or in Bill Wulf’s (2000) elegant definition
of what an engineer does—‘design under con-
straint’. Aside from money and time, computing
brings two interlocking constraints into sharp
focus—absolute consistency and complete explicit-
ness. Anything we wish to model in the computer,
and so to compute, must be measured against them.
Two closely interrelated problems immediately
surface. One is the inevitable and radically severe
loss-in-translation which these constraints impose
on any real-world object, especially severely on
works of art and literature. The other problem,
Brian Cantwell Smith (1995) has pointed out, is
that we have no theory to tell us in any reliable
way whether a given model corresponds with that
which it models (p. 462). The crunch comes not so
much in determining whether the programmer has
done his or her job correctly (though that is a con-
cern), rather with the very idea of ‘correctness’ in
any computationally rigorous description of the
world (Smith, 1995). These two problems add up
to what is known as the ‘software crisis’, which
was proclaimed at the end of the 1960s and is
still very much with us (Mahoney, 1996, p. 779).
It frustrates us technically because it stumps us
philosophically.
A moment ago I put aside the constraints of
money and time. Allow me also to put aside the
causes of the software crisis. I promise to return
to all of them. But for the moment I want to
direct your attention to the divergence of computa-
tional models from that which we have always done.
In a loose sense models are not new to scholar-
ship. To render a cultural artefact intellectually
tractable, we must ignore some aspects of it and
highlight others according to our abilities, interests,
and purposes. But calling this and a mechanical
construct both ‘models’ is badly misleading: the
mental kind, which Max Weber wisely preferred to
call an ‘ideal type’ (Leff, 1972, p. 148), is without
physical form and direct means of access. All that we
can witness are its results in language, normally the
ahistorical sequences of argument in the very slowly
changeable medium of another’s prose. In contrast,
the digital model of an artefact is more nearly com-
parable to an argument than to the idea behind it.
The digital model likewise begins with a selective,
simplified conception of the thing and works this
conception out in an external medium of expres-
sion, namely software. But again, software is not
well understood. What is clear is the difference we
experience in the change from writing to modelling:
the ability to manipulate something directly without
reference to the author or even without thinking-in-
the-head. Leaving aside the postponed causes of the
software crisis, then, rapid manipulation is what
separates modelling from writing. Because tinkering
with the model is so easy, so tempting, the finality
we have known from the written and published
word recedes into the future. We do not finish
something, we call a halt to it. This inevitably
undermines the authority of end-products, though
we may choose (unwisely, I am arguing) to pretend
that nothing has changed.
The fallout is this, in a nutshell: models of
whatever kind are far less important to the digital
humanities than modelling. Modelling is crucial. If
you only remember a single sentence from this brief
essay, remember this one: the word ‘computing’ is
W. McCarty
254 Literary and Linguistic Computing, Vol. 23, No. 3, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
a participle—a verbal adjective that turns things into
algorithmic performances.
Considering the implications of modelling, we
may be tempted to say that it represents an unpre-
cedented change in how we do the humanities. But
saying that begs the question at hand. ‘Nothing is
really unprecedented’, the late historian of science
Michael Mahoney (1990) has pointed out (p. 326).
Finding precedents in past things and experiences is
how we make sense of what is new, perhaps even
how we perceive the new at all. A new object or idea,
once it catches our attention and starts connecting
with what we know, tends to bring to the fore-
related phenomena previously undervalued or
ignored, to suppress other things once prominent
and to put still others into a different light. A new
past is created. So, rather than hypnotizing ourselves
with supposedly unprecedented marvels, we must
learn to see computing in its historical and social
contexts, and so be able to ground modelling in
something larger than itself. For computing to be
of the humanities as well as in them, we must get
beyond catalogues, chronologies, and heroic firsts to
a genuine history. There are none yet.
But, let me give you a rough idea of how writing
a tiny piece of that history might begin. About 30
years ago, Northrop Frye (1976) noted a striking
technological change in scholarly resources, from
the ‘portly . . . tomes’ of his childhood, communi-
cating in their physical stature ‘immense and defi-
nitive authority’, to the ‘paperback revolution’ of his
adulthood, which better matched the speed of scho-
larship (p. 49ff) and gave wing to the diversification
of publishing. The demotic character and relative
impermanence communicated by these paperbacks
also implied the undermining of authority I just
mentioned, in this case a weakening of the barrier
between author and reader. Running in parallel if
not cognate with this physically mediated change
came the theoretical changes in ideas of textuality,
for example Mikhail Bakhtin’s (1986/1979) ‘dialogic
imagination’, reader-response theory and, more
recently, in anthropological linguistic studies of
context. Meanwhile various parts of computer
science have developed congruently, from design
of black-boxed, batch-orientated systems of former
times to toolkits and implementations of
‘interaction design’ (Winograd, 1997). Computing
has become literally and figuratively conversational.
Certain projects in the digital humanities, such as
the prosopographical tools built in my department
at King’s College London, have followed suit by
making the individual an active co-maker of that
which he or she would previously been merely the
user. Projects that create communal spaces for
scholarship or involve collaborators in game-playing
exchanges are responding in like style.
In view of such changes throughout the huma-
nities and in both computer science and humanities
computing, it makes less and less sense to be think-
ing in terms of ‘end-users’ and to be creating
knowledge-jukeboxes for them. It makes more and
more sense to be designing for ‘end-makers’ and
giving them the scholarly equivalent of Tinker
Toys. But we must be aware not to be taking away
with one hand what we have given with the other.
To use Clifford Geertz’s (1993/1973) vivid phrase
(p. 27), we need rigorous ‘intellectual weed-control’
against the Taylorian notions that keep users in
their place—notions of knowledge ‘delivery’, scho-
larly ‘impact’, learning ‘outcomes’ and all the rest of
the tiresome cant we are submerged in these days.
The whole promise of computing for our time—
here is my historical thesis—is directly contrary to
the obsolete nineteenth century cosmology implicit
in such talk. It is especially dangerous because it is
congruent with the conveyor-belt mentality
Langdon Winner alerts us to. No need, go the mur-
muring implications, to put serious resources into
awakening the technological imaginations and
improving the skill-set of academics and their stu-
dents; no need to alter the design of the curriculum
in response; no need to answer Jaroslav Pelikan’s
(1992) challenging call, in The Idea of the
University some 16 years ago, to change institutional
structures and practices so that suitably qualified
technical staff become equal colleagues in the
research enterprise (p. 62).
3 Analytic, Synthetic, and
Improvisational Modelling
I have not forgotten those hanging problems I put
on hold a while back. Let me begin to deal with
What’s going on?
Literary and Linguistic Computing, Vol. 23, No. 3, 2008 255
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
some of them now, by distinguishing the three kinds
of modelling I just named.
To analyse is to figure out how something works
by taking it apart, for example when a literary critic
dissects a poem to understand how it does what
it does. Constructing and manipulating a digital
model of a poem—more accurately, of an interpre-
tative understanding of the poem—allows the critic
to probe his or her particular understanding, per-
haps even change it significantly. To synthesize is to
make something according to a plan or idea, as
when a scholar writes a book or builds a database
to represent a structure of interrelationships among
disparate but somehow connected historical events
and people. In this case, the database allows the
designer and others to model for possible connec-
tions and patterns. To improvise, as London musi-
cian and musicologist Derek Bailey (1993) has said,
‘means getting from A to C when there is no B’
(p. 136). Improvisational modelling, represented
partially by Jerome McGann’s IVANHOE, is an
inchoate, highly speculative notion whose discourse
is work in progress.
I have written extensively elsewhere about analy-
tic modelling (McCarty, 2005, pp. 20–72), but
let me summarize briefly here. It is imitative, there-
fore severely affected by the problem of loss-
in-translation, which I mentioned earlier. In fact,
the constraints of complete explicitness and absolute
consistency would seem to render analytic model-
ling useless for our purposes. We have been slow to
address this question in recent years if not utterly
tongue-tied, but the obvious answer has been star-
ing us full in the face all along: that the digital model
illumines analytically by isolating what would not
compute. In other words, the failures of analytic
modelling are where its success is to be found. Its
great and revolutionary success for the humanities is
to force the epistemological question—how is it that
we know what we somehow know—and to give us
an instrument for exploring it. I suspect that with
the emphasis on standards and best practices, most
analytic modelling has gone unnoticed in the man-
ufacture of standard digital resources. Minimizing
interpretation in order to ensure wide acceptance
and use rules it out. Here, there is considerable
work to be done on tools to allow sites for
interpretation within an object of study to be iden-
tified, annotated, and manipulated rapidly.
Synthetic modelling is the great virtue of the
scholarly thematic collections, such as the Blake
Archive, the historical databases, such as the
Prosopography of the Byzantine World, and VR
reconstructions of lost, damaged, or altered objects,
such as the Pompey Project. What makes these
diverse things exemplary of synthetic modelling,
and marks their departure from the knowledge-
jukebox, is the specific implementation of tools for
comparing, connecting, experimenting with objects
of study. Strong factual or factoidal constraints
govern what the user (or, more accurately, user-
maker) can do, but mechanisms for conjectural
play embed standard ways of reasoning from the
given to the doorstep of argument. Progress here
hinges on tools for bridging, on-the-fly, manipula-
tory possibilities to software options.
Improvisation differs from creativity by an
emphasis on what is given. It is, again, ‘getting
from A to C when there is no B; it implies a void
which has to be filled’.2 Like synthesis it is con-
strained, but like creativity it seems mostly to take
place below stairs, or where no stairs go. While we do
not have to go places to know something about what
happens there, nevertheless we should be asking why
improvisation has received so little attention. The
term I borrow from musicology, where it is used to
describe world-wide forms of musical performance,
but the idea is closely akin to inventive behaviour in
the experimental sciences, in the arts generally, and
in ordinary conversation. It is quite clear what
people everywhere do all the time, and what
some people do brilliantly well in particular media.
Theoretical biologist Robert Rosen (2000) sug-
gests in Essays on Life Itself that in the sciences ‘a
mind-set of reductionism, of looking only down-
ward toward subsystems, and never upward and
outward’ is to blame (p. 2). He is speaking primarily
of physics, which as the dominant science until
recently has had an enormous influence on how
we think in all areas of life. He and biological
anthropologist Terrence Deacon (2006), for exam-
ple, make a strong case for the self-organizing, non-
deterministic, quasi-teleological systems of biology
as better, more generous models for us to think with.
W. McCarty
256 Literary and Linguistic Computing, Vol. 23, No. 3, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
UCLA anthropologist Alessandro Duranti (2005)
suggests that we look to jazz improvisation for
the human capacity to imagine systems with ‘a life
of their own’ (p. 420ff), which like biological
systems use constraints to stimulate and leverage
further growth rather than, as with analytical mod-
elling, to illumine their incapacities. Literary scholar
Lisa Samuels (1997) similarly directs us to the arts:
‘Beauty’, she declares, ‘wedges into the artistic space
a structure for continuously imagining what we do
not know’ (p. 1).
The cusp of change from synthetic to improvisa-
tional modelling occurs where strict truth to the arte-
fact or to what is supposed actually to have happened
loosens into versimilitude. Here again Bruner’s idea
of the humanities is useful. The aim of these disci-
plines, he writes, is that their hypotheses ‘fit different
human perspectives and that they be recognizable as
‘‘true to conceivable human experience’’: that they
have verisimilitude’ (Bruner, 1986, p. 52). Here is
indeed a slippery slope, or more accurately, a skate-
boarder’s half-pipe, namely, again, the problem of
imagination. On the one side, with the scholars, is
reconstruction faithful to evidence; on the other,
with the artists, is vision enabled by media. In
between the kinds of modelling blur into each
other. As we move from the one to the other,
from the known to the unknown, the less the
analytic struggle against the constraints of digital
representation matters. Interest shifts to what you
can project from computing into the world, what
possible worlds you can realize and operate within.
Computing, then, puts us into very interesting
company. On the analytic side, with our reductive
equipment increasingly able to converge on the phy-
sical origins of textual, pictorial, and musical experi-
ence, we find ourselves with physical scientists. On
the improvisational side, equipped to make the
absent present, we find ourselves in the equally
interesting company of biologists and artists. More
about this later, with examples.
4 Disciplines and What is
Happening in Them
In the early 1980s, Clifford Geertz and Richard
Rorty drew attention to the fact that disciplines
are living epistemic cultures of their own, and that
to understand what is happening in them, and
therefore to learn from them, we must not only
respect their integrity but also pay attention to
their normal discourse—to their ‘tropes and ima-
geries of explanation’, as Geertz said (2000/1983,
p. 22; cf. Rorty 1980, p. 320). In other words, inter-
disciplinarity requires ethnography.3
Claiming interdisciplinarity is faddish, but com-
puting sets the stage for the real thing because as an
applied field its principal focus is methodological.
Methods migrate irrespective of discipline, accord-
ing to the kind of data and transformations
involved. Likewise, much concerning the applica-
tion and outcomes of particular methods is shared
in common. Here is where humanities computing
enters the picture. This interdiscipline (as I like to
call it) has developed over the last few decades as the
incubator, steward, merchant trader, and critic of
digital methods for the humanities. It has also
begun to make deep connections into the disciplines
for practical wisdom on how to do its job and for
theoretical understanding of its practice. All the dis-
ciplines are part of its conversation—including, as
I have said, the sciences.
What sort of digital work gets done where is not
only a function of data type but also of what a given
discipline chiefly does: report and publish what prac-
titioners find; interpret primary sources of knowl-
edge; or invent new genres of expression. Current
priorities also weigh in heavily. Particularly since the
advent of the Web, our attention and energy have
been involved with the exponential growth of
digitization. The benefits for scholarship here are
unarguably great. But as ever larger amounts of
searchable and otherwise computable material
become available, we do not simply have more evi-
dence for this or that business-as-usual. We have
massively greater ecological diversity to take account
of, and so can expect inherited ways of construing
reality and of working, alone and with each other,
to need basic renovation. Here is work to be done.
It is not a matter of breaking down disciplinary
boundaries—the more we concentrate on breaking
these down, the more they are needed for the break-
ing down. Rather the point is the reconfiguration of
disciplinarity. From computing’s prospect at least,
What’s going on?
Literary and Linguistic Computing, Vol. 23, No. 3, 2008 257
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
the feudal metaphor of turf and the medieval tree
of knowledge in its formal garden of learning
make no sense. We need other metaphors. Here is
work to be done.
So also, at this historical moment the inven-
ting function is or should be prominent, since it
is quite clear that imitation of the codex is for
us a mug’s game, e-paper notwithstanding. This is
becoming especially evident in digital scholarly edi-
tions and other reporting projects. Codex-depen-
dent genres simply need to be rethought, perhaps
dissolved or merged, but certainly re-formed. As
Jerome McGann (2001) has pointed out, we have
not done very well in this area to date (p. 74). We
have exhilarating theoretical visions of decentric
scholarly forms—to paraphrase Roland Barthes
(1990/1973) from the early 1970s, ‘networks with
thousands of entrances’ (p. 12)—but weak means
of attempting their realization. We can see that
our tools for the analysis of text strike on both
sides of a target we can vaguely describe, but we
have no idea of how to build them. We know
from careful analysis of traditional scholarly
genres, such as the commentary, how powerful
and subtle they could be in the hands of a
master practitioner in an appreciative community
of readers, but we are flummoxed by their
challenge to the digital medium (McCarty,
2002). And so on, and so forth. More work to be
done.
The interpreting function is dependent on the
co-evolution of tools and of theory emergent
from experience in the digital medium. On the
analytic side, what is important is, as I said, getting
to the origins of what we experience before we
experience it. What we live for analytically is that
residue of anomalies over there in the corner of the
problem space, which is where we find (again I
quote Jerome McGann) ‘the hem of a quantum gar-
ment’ (2004, p. 201)—the transforming agent of
an altogether new way of seeing the world. On the
synthetic side, what is important is creative engage-
ment with consensual scholarly discoveries and
understandings. For improvisation, what matters is
imagining the might-have-been/done/written,
taking as given whatever artefact is better to be
understood. Again, more about this later.
What, then, about those other constraints I put
on hold: the economic constraints of money and
time? In a recent book that deserves your attention,
Thing Knowledge: A Philosophy of Scientific
Instruments (2004), Davis Baird, son of a commer-
cial instrument maker, points out that by putting
human knowledge and skill into objects, as we are
doing, we make strange bedfellows of two very
different economies. Even the small-scale manu-
facture of ‘thing knowledge’ requires money and
expensive people’s time, hence the activity tends
to reflect organizational models from ‘the commod-
ity market where . . . knowledgeable things are sold’
or even gets tied to this market (Baird, 2004, p. 213).
At the same time, our epistemic goods are not
by nature commodities but gifts. New thinking
and new institutional structures are needed to
reconcile commodity and gift economies more
productively.
5 What is to be Done
Computing’s trajectory, I said at the outset, calls
upon us to rethink the craft of our own research
and to connect it up with whatever informs its goals.
I noted that in computing’s emphasis on modelling
rather than models, the already existing movement
in scholarship away from the monumental toward
the conversational is accelerated. I argued that by
virtue of its rigorous constraints, computing gives
new form to the ancient realization that transcen-
dence is knowable only by what it is not. Thus the
computational ‘way of denial’ leaves us with ever
better questions, and so gives us reason and
motive to converse. Modelling improvisationally
toward new forms of expression, I said, is perfor-
mative, moving into and involving the artistic. But
(to paraphrase Abby Hoffman), if we are to sustain
and develop the theatre in this crowded fire, what
sort of a theatre should we be building?
To answer this question in general, with the
expectation that it will be applicable to someone
else’s institution, is foolhardy. Institutional struc-
tures are notoriously local and do not translate
easily, even within a single academic system.
But looking across the highly various institutional
W. McCarty
258 Literary and Linguistic Computing, Vol. 23, No. 3, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
models for humanities computing that have been
created in the last couple of decades or so, it
seems clear that Pelikan was right on: the support
of research simply must become a collegial, colla-
borative activity, which is to say, not ‘support’ as
it has been known. The major centres built to
support—at Toronto, Oxford, Bergen and else-
where—have vanished, replaced in prominence
and influence by those for which collaborative col-
legiality obtains. The largest and most influential
centre to date (which happens to be my own) is
an academic entity, with junior and senior appoint-
ments in humanities computing, teaching pro-
grammes offering BA courses and MA and PhD
degrees and a large research staff employed in
grant-funded collaborative projects. The details are
important, but what matters most in this story is the
correlation of institutional success with the intellec-
tual case I have attempted to make. In other words,
as I said, the institutional structures we build for the
digital humanities should reflect the nature of the
practice as it has emerged in the last few decades.
This is simple to say but quite a challenge to realize.
6 Bridge Building
What, then, is going on? In summary, the picture
that I see emerging from current work looks like
this:
 A world-wide, semi-coordinated effort to create
large online scholarly resources, the best of which
allow synthetic modelling;
 Out of this activity, the slow development of new
genres in something like a digital library, though
considerably more complex and various than
once was imagined;
 Analytic and synthetic modelling, on the one
hand to probe for contributions to the construc-
tion of meaning, on the other to reconstruct lost
artefacts from fragmentary evidence, blurring
gradually into improvisational modelling for
what Bruner has called ‘possible castles’.
Recall that to model is to create a useful fiction. In
analytic modelling, we privilege what we know and
use the model to advance the question of how we
know it. I pointed out earlier that the strongly
reductive character of our equipment-orientated
analysis gives it a certain kinship with the physical
sciences. This is a kinship that computing strength-
ens further by forcing us to reduce our objects of
study to digital proxies. These proxies (data being
data) are indistinguishable from anything else in a
computer, hence as much candidates for scientific
reasoning as any other data. Thus, we acquire access
to a bridge (already under construction by histor-
ians and philosophers of science) into the scientific
heartland. But the point I wish to emphasize is what
allows this to happen: not physics-envy but the
computational ability to implement the conjectural,
that is, to construct possible worlds and explore them.
I said earlier that as we move away from reliable
knowledge of artefacts, through degrees of uncer-
tainty, to the point at which pre-existing artefacts
disappear altogether, analytic is supplanted by syn-
thetic, then synthetic by improvisational modelling.
In the case of VR reconstruction, the artefact is
enigmatic, fragmentary or missing, so we combine
historical evidence and creative inference to produce
an illusion of its presence. In such work, the danger
of imagining the artefact other than what it was is
obvious. But done deliberately, the result becomes
more a work of art than of scholarship, or in socio-
philosophical terms, a possible world.
Imagine, for example, a model of literary allusion
turned to the synthetic purpose of generating all pos-
sible connections from, say, the 1611 King James
Bible to subsequent English literature, evolving to
account for the possibility that each new allusion
might affect the range of subsequent possibilities.
Imagine an explorable map of these allusions. How
would we regard this simulation of a literary reality
that no one or very, very few, would even claim to be
able to reach otherwise? Imagine further. Imagine
entirely fictional participatory worlds, of which
Second Life and its kind give but a dim inkling,
turned to artistic purposes. What would the scholarly
interpretation of the artistic works thus created,
taking place in those worlds, be like?
The trajectory toward the fictional holodeck is
clear, and some of that only a matter of time. It
raises all sorts of interesting questions, for example,
about re-enactment of the past and the objectivity of
What’s going on?
Literary and Linguistic Computing, Vol. 23, No. 3, 2008 259
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
what we take to be the evidence. Important work
has shown objectivity to be recent historical creation
and so deconstructed a once seemingly impene-
trable barrier between the sciences and the huma-
nities (Daston and Galison, 2007). We are given the
opportunity to redraw or to erase old lines of
demarcation in academic work. We are given the
chance to make new friends on a level playing
field. We are given the wherewithal to refurbish
the humanities.
But so what? Is any of this totally new? Perhaps
not. Perhaps everything can all be done on paper—
given a sufficiently long life, inexhaustible resources
and no concern for what one’s students, and the
world, are doing. The important question is not ‘is
this new?’ but in fact ‘what’s going on?’
References
Autonomy, Singularity, Creativity: A Project of the National
Humanities Center. asc.nhc.rtp.nc.us/ (accessed 24
March 2007).
Bailey, D. (1993). Improvisation: Its Nature and Practice in
Music. Cambridge MA: Da Capo Press.
Baird, D. (2004). Thing Knowledge: A Philosophy of
Scientific Instruments. Berkeley: University of
California Press.
Bakhtin, M. M. (1986/1979). Toward a methodology for
the human sciences. In Emerson, C. and Holquist, M.
(eds.), Speech Genres & Other Late Essays. (McGee, V.
W., Trans.). Austin TX: University of Texas Press,
pp. 159–72.
Barthes, R. (1990/1973). S/Z (Miller, R., Trans.) Oxford:
Blackwell.
Boden, M. A. (2005). The Creative Mind: Myths and
Mechanisms. London: Routledge.
Bruner, J. (1986). Possible castles. In Actual Minds,
Possible Worlds. Cambridge MA: Harvard University
Press, pp. 44–54.
Daston, L. and Peter, G. (2007). Objectivity. New York:
Zone Books.
Deacon, T. W. (2006). Emergence: the hole at the wheel’s
hub. In Clayton, P. and Davies, P. (eds), The
Re-Emergence of Emergence: The Emergentist
Hypothesis from Science to Religion. Oxford: Oxford
University Press, pp. 111–50.
Duranti, A. (2005). On theories and models. Discourse
Studies, 7: 409–29.
Frye, N. (1976). The renaissance of books. In Spiritus
Mundi: Essays on Literature, Myth, and Society.
Bloomington IN: Indiana University Press, pp. 49–65.
Geertz, C. (1993/1973). The Interpretation of Cultures:
Selected Essays. London: Fontana Press.
Geertz, C. (2000/1983). Local Knowledge. New York: Basic
Books.
Ginzburg, C. (1996). Making things strange: the pre-
history of a literary device. Representations, 56: 8–28.
Goffman, E. (1997/1974). Frame analysis. In Lemert, C.
and Branaman, A. (eds), The Goffman Reader. Oxford:
Blackwell Publishing, pp. 149–66.
Leff, G. (1972). Models inherent in history. In Shanin, T.
(ed.), Rules of the Game: Cross-Disciplinary Essays on
Models in Scholarly Thought. London: Tavistock,
pp. 148–60.
Mahoney, M. S. (1990). The roots of software engineer-
ing. CWI Quarterly, 3.4: 325–34. www.princeton.edu/
mike/articles/sweroots/sweroots.pdf (accessed 24
March 2007).
Mahoney, M. S. (1996). Issues in the history of comput-
ing. In Bergin, T. J. and Gibson, R. G. (eds), History of
Programming Languages II. New York: ACM Press, pp.
772–81. www.princeton.edu/mike/articles/issues/
issuesfr.htm (accessed 24 March 2007).
McCarty, W. (2002). A network with a thousand
entrances: commentary in an electronic age? In
Gibson, R. K. and Kraus, C. S. (eds), The Classical
Commentary: Histories, Practices, Theory. Leiden: Brill,
pp. 359–402.
McCarty, W. (2005). Humanities Computing. Basingstoke:
Palgrave.
McCarty, W. (2006). Tree, Turf, Centre, Archipelago—or
Wild Acre? Metaphors and Stories for Humanities
Computing. Literary and Linguistic Computing, 21:
1–13.
McGann, J. (2001). Radiant Textuality: Literature after the
World Wide Web. New York: Palgrave.
McGann, J. (2004). Marking texts of many dimensions. In
Schreibman, S., Siemens, R. and Unsworth, J. (eds),
A Companion to Digital Humanities. Oxford:
Blackwell, pp. 198–217. www.digitalhumanities.org/
companion/, 16. (accessed 3 April 2007).
Pelikan, J. (1992). The Idea of the University: A
Reexamination. New Haven CT: Yale University Press.
W. McCarty
260 Literary and Linguistic Computing, Vol. 23, No. 3, 2008
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
Rorty, R. (1980). Philosophy and the Mirror of Nature.
Princeton NJ: Princeton University Press.
Rosen, R. (2000). Essays on Life Itself. Complexity in
Ecological Systems. New York: Columbia University Press.
Samuels, L. (1997). Introduction to poetry and the
problem of beauty. Modern Language Studies, 27: 1–7.
Smith, B. C. (1995). The limits of correctness in compu-
ters. In Johnson, D. G. and Nissenbaum, H. (eds),
Computers, Ethics & Social Values. Englewood Cliffs
NJ: Prentice Hall, pp. 456–69.
Winner, L. (1997/1986). Technologies as forms of life.
In Westra, L. and Shrader-Frechette, K. (eds)
Technology and Values. Oxford: Rowman and
Littlefield, pp. 55–69.
Winograd, T. and Flores, F. (1986). Understanding
Computers and Cognition: A New Foundation for
Design. Boston: Addison-Wesley.
Winograd, T. (1997). Design of interaction. In
Denning, P. J. and Metcalfe, R. M. (eds), Beyond
Calculation: The Next Fifty Years of Computing. New
York: Copernicus, pp. 149–60.
Wulf, W. A. (2000). The nature of engineering, the
science of the humanities, and Go¨del’s
theorem (Interview). Ubiquity 1.28. www.acm.org/ubi
quity/interviews/w_wulf_1.html (accessed 24 March
2007).
Notes
1 This was originally given as a plenary address for
‘Countries, Cultures, Communication: Digital
Innovation at UCLA’, University of California at Los
Angeles, 10 May 2007. I am grateful to the two anon-
ymous reviewers for their helpful comments and espe-
cially to Zoe Borovsky for the opportunity to give the
original talk.
2 Bailey, 1993, p. 136. On creativity in the context of
computing, see Boden, 2005.
3 For a fuller treatment, see McCarty, 2005: 114–57; 2006.
What’s going on?
Literary and Linguistic Computing, Vol. 23, No. 3, 2008 261
 at U
B Leipzig on D
ecem
ber 9, 2011
http://llc.oxfordjournals.org/
D
ow
nloaded from
 
