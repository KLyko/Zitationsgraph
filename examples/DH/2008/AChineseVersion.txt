A Chinese Version of an 
Authorship Attribution 
Analysis Program 
 Mengjia Zhao
zhaom@duq.edu
Duquesne University, USA
Patrick Juola 
juola@mathcs.duq.edu
Duquesne University, USA
 Authorship Attribution (Juola, in press) is a form of text 
analysis to determine the author of a given text. Authorship 
Attribution in Chinese (AAC) is that task when the text 
is written in Chinese. It can be considered as a typical 
classifi cation problem, where a set of documents with known 
authorship are used for training and the aim is to automatically 
determine the corresponding author of an anonymous text. 
Beyond simple questions of the identity of individual authors, 
the underlying technology may also apply to gender, age, social 
class, education and nationality.
JGAAP (Java Graphical Authorship Attribution Program) (Juola 
et al., 2006) is a program aimed at automatically determining 
a document’s author by using corpus linguistics and statistical 
analysis. It performs different types of analysis and gives the 
best results to the user while hiding the detailed methods and 
techniques involved. It can therefore be used by non-experts. 
We extend JGAAP to cover the special issues involved 
in Chinese attribution and present the results of some 
experiments involving novels and blogs.
Why can’t we just use the existing JGAAP software? Chinese 
introduces its own problems, caused by the different structures 
of English and Chinese. As with English, Chinese texts are 
composed of sentences, sentences are composed of words, 
and words are composed of characters. However, the Chinese 
character set is approximately 50,000 individually meaningful 
characters, compared with fi fty or so meaningless symbols for 
English. Furthermore, in Chinese texts words are not delimited 
by spaces as they are in English. As with English, the word is the 
basic meaningful unit in Chinese, but the meaning of a word 
may differ from the meaning of the characters compose this 
word.
Analysis at the character level is thus fundamentally different 
between the two languages. Studies in English show that analysis 
at the word level is likely to be a better way to understand 
the style and linguistic features of a document, but it is not 
clear whether this will apply to Chinese as well. So before we 
can analyze word-level features (for comparison) we need to 
segment sentences at word-level not by characters. Therefore 
the fi rst step for any Chinese information processing system 
is the automatically detection of word boundaries and 
segmentation.
The Chinese version of JGAAP supports Chinese word 
segmentation fi rst then followed by a feature selection process 
at word level, as preparation for a later analytic phase. After 
getting a set of ordered feature vectors, we then use different 
analytical methods to produce authorship judgements. 
Unfortunately, the errors introduced by the segmentation 
method(s) will almost certainly infl uence the fi nal outcome, 
creating a need for testing.
Almost all methods for Chinese word segmentation developed 
so far are either structural (Wang et al., 1991) and statistical-
based (Lua, 1990). A structural algorithm resolves segmentation 
ambiguities by examining the structural relationship between 
words, while a statistical algorithm usually compares word 
frequency and character co-occurrence probability to 
detect word boundaries. The diffi culties in this study are the 
ambiguity resolution and novel word detection (personal 
names, company names, and so on). We use a combination of 
Maximum Matching and conditional probabilities to minimize 
this error.
Maximum matching (Liu et al., 1994) is one of the most 
popular structural segmentation algorithms, the process 
from the beginning of the text to the end is called Forward 
Maximal Matching (FMM), the other way is called Backward 
Maximal Matching (BMM). A large lexicon that contains all the 
possible words in Chinese is usually used to fi nd segmentation 
candidates for input sentences. Here we need a lexicon that 
not only has general words but also contains as many personal 
names, company names, and organization names as possible 
for detecting new words.
Before we scan the text we apply certain rules to divide the 
input sentence into small linguistic blocks, such as separating 
the document by English letters, numbers, and punctuation, 
giving us small pieces of character strings. The segmentation 
then starts from both directions of these small character 
strings. The major resource of our segmentation system is 
this large lexicon. We compare these linguistic blocks with the 
words in the lexicon to fi nd the possible word strings. If a 
match is found, one word is segmented successfully. We do this 
for both directions, if the result is same then this segmentation 
is accomplished. If not, we take the one that has fewer words. 
If the number of words is the same, we take the result of 
BMM as our result. As an example : Suppose ABCDEFGH is 
a character string, and our lexicon contains the entries A, AB, 
ABC, but not ABCD. For FMM, we start from the beginning of 
the string (A) If A is found in the lexicon, we then look for AB 
in the lexicon. If AB is also found, we look for ABC and so on, 
till the string is not found. For example, ABCD is not found 
in the Lexicon, so we consider ABC as a word, then we start 
from character D unti the end of this character string. BMM 
is just the opposite direction, starting with H, then GH, then 
FGH, and so forth.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
230
Suppose the segmentation we get from FMM is
(a) A \ B \ CD \ EFG \ H
and the segmentation from BMM is
(b) A \ B \ C \ DE \ FG \ H
We will take result (a), since it has fewer 
words. But if what we get from BMM is
(c) AB \ C \ DE \ FG \ H
We will take result (c), since the numbers 
of words is same in both method.
After the segmentation step we take the advantage of 
JGAAP’s features and add different event sets according to 
the characteristics of Chinese, then apply statistical analysis 
to determine the fi nal results. It is not clear at this writing, 
for example, if the larger character set of Chinese will make 
character-based methods more effective in Chinese then they 
are in other languages written with the Latin alphabet (like 
English). It is also not clear whether the segmentation process 
will produce the same type of set of useful “function words” 
that are so useful in English authorship attribution. The JGAAP 
structure (Juola et al, 2006;Juola et al., submitted), however, 
will make it easy to test our system using a variety of different 
methods and analysis algorithms.
In order to test the performance on Chinese of our software, 
we are in the process of constructing a Chinese test corpus. 
We will select three popular novelists and ten novels from 
each one, eight novels from each author will be used as training 
data, the other two will be used as testing data. We will also 
test on the blogs which will be selected from internet. The 
testing procedure will be the same as with the novels.
This research demonstrates, fi rst, the JGAAP structure can 
easily be adapted to the problems of non-Latin scripts and not 
English languages, and second, provides somes cues to the best 
practices of authorship attribution in Chinese. It can hopefully 
be extended to the development of other non-Latin systems 
for authorship attribution.
References:
Patrick Juola, (in press). Authorship Attribution. Delft:NOW 
Publishing.
Patrick Juola, John Noecker, and Mike Ryan. (submitted). 
“JGAAP3.0: Authorship Attribution for the Rest of Us.” 
Submitted to DH2008.
Patrick Juola, John Sofko, and Patrick Brennan. (2006). “A 
Prototype for Authorship Attribution Studies.” Literary and 
Linguistic Computing 21:169-178
Yuan Liu, Qiang Tan, and Kun Xu Shen. (1994). “The Word 
Segmentation Rules and Automatic Word Segmentation 
Methods for Chinese Information Processing” (in Chinese). 
Qing Hua University Press and Guang Xi Science and 
Technology Press, page 36.
K. T. Lua. (1990). From Character to Word. An Application 
of Information Theory. Computer Processing of Chinese and 
Oriental Languages, Vol. 4, No. 4, pages 304--313, March.
Liang-Jyh Wang, Tzusheng Pei, Wei-Chuan Li, and Lih-Ching 
R. Huang. (1991). “A Parsing Method for Identifying Words 
in Mandarin Chinese Sentences.” In Processings of 12th 
International Joint Conference on Artifi cial Intelligence, pages 
1018--1023, Darling Harbour, Sydney, Australia, 24-30 August
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
231
Posters
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
232
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
233
The German Hamlets: An 
Advanced Text Technological 
Application
 Benjamin Birkenhake 
University of Bielefeld, Germany
Andreas Witt
andreas.witt@uni-tuebingen.de
 University of Tübingen, Germany 
The highly complex editorial history of Shakespeare’s Hamlet 
and its many translations to German forms a interesting 
corpus to show some of the major advantages current 
texttechnologies. In the following we want to present the 
corpus of four English editions an several German translations 
of the play we have gathered together and annotated and 
crosslinked in different ways.
Although Shakespeare’s Hamlet is obviously not a unique 
hypertext, it is an interesting object to test advanced hypertext- 
and text technologies. There is no original edition of Hamlet, 
which was authorized by Shakespeare during his lifetime. We 
only have different print editions, which all have a different 
status concerning their quality, overall length, content and 
story-line. The most important among these are the so called 
fi rst folio, the fi rst quattro and the second quattro edition of 
Hamlet. During the centuries editors tried to combine these 
early editions to the best edition possible. The famous Arden-
editons as well as the in the internet widespread Moby-editon 
are such compositions.
A comparable but a bit more complex situation exists within 
in the fi eld of german translations of the play. The earliest 
translation is by Christoph Martin Wieland from about 
1766. After this at least 18 translations have been published 
which are accompanied by countless translations of theatre 
directors, which are mostly not documented. The corpus 
contains 8 digitalized Translations. 2 further translation are 
already scanned but not yet digitalized, because they are 
printed in fraktur - a old german typeface - which can not be 
recognized by common OCR-programs yet. The remaining 10 
Translations are available in print, but not yet digitalized, too. 
Of the 8 digitalized translations we chose 4 for further text 
technological use.
What makes the corpus so interesting is the fact, that almost 
every translator used several of the early english editions 
as a basis for a new translation. This leads to a situation in 
which almost every german or english edition of Shakespeare’s 
Hamlet is a composition of several sources. The relation the 
editions have with their sources and with each other form 
a wide network, which could be presented in a hypertext 
system.
Another interesting aspect of Shakespeare’s Hamlet is the 
outstanding position the play has within the western culture 
for centuries. Hamlet is the single most researched piece of 
literature, has been analyzed from various perspectives and is a 
part of western common education. This leads to the request, 
that a digital environment should represent the variety of 
perspectives on the play. This lead us to a corpus of Hamlet 
editions in which each text may exist in multiple forms.
Basis for the XML-annotations are text fi les, which are 
transformed to XML using regular expressions. The basic XML-
format is TEI 4 drama base tag set. TEI 4 is a major open source 
concept of the Text Encoding Initiative. The drama base tag set 
offers almost all tags needed for a general, formal annotation 
of a play. In order to provide an easy to annotate mechanism 
we added some attributes to represent the translation- or 
origin-relation between lines, paragraphs or speeches within 
the editions on the one hand and the sources on the other 
hand.
The TEI-annotated documents are used for further annotations 
an presentation. The TEI-documents were automatically 
enriched with further markup, using an open source auto-
tagger. This auto-tagger annotates single words, including the 
part of speech and the principle form. The TEI-documents are 
also the basis for the XHTML-presentation. As the TEIstructure 
contains all information necessary for a graphical presentation, 
these documents are transformed to XHTML, which is used to 
present the corpus. This transformation is made with several 
XSLT-Stylesheets. In the same way XSLFO is used to generate 
PDF-versions of each edition.
edition Txt TEI XHTML STTS Narration
1st Folio x x x
1st Quattro x x x
2nd Quattro x x x
Moby x x x
Wieland x x x x
Schlegel x x x x x
Fontane x x x x x
Hauptmann x x x x
Table 1: The different layers of the mayor 
editions within the corpus
In many cases translators have re-arranged the fl ow of stanzas 
or the course of action. Therefore it is useful to provide an 
alternative linking mechanism, which does not only focus on 
the language and the formal structure, but also on the plot. To 
provide this reference the narrative information is annotated 
in another layer. This allows to fi nd the same event in different 
translations of the play. The narrative annotation layer basically 
consists of events, which can be seen as the smallest elements 
of the plot.
Obviously, events may start within one line and end several 
lines or even speeches later. Since the narrative structure 
is overlapping with the TEI, both are stored in separate 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
234
annotations. Scenes can provide a meaningful unit for basic 
parts of the plot. Thus the formal and the narrative annotation 
are semantically aligned - in addition to their reference on 
identical textual data. This relation can be exploited by creating 
links between the concept of a scene and the concept of 
specifi c actions. The respective linking mechanism is located 
on a meta level: it operates on the schemas themselves 
and not on their instances. The references are generated 
mechanically on the meta level, linking different perspectives 
together. Readers can explore the relations between events 
and scenes. The procedure could also be used to create a 
recommendation system as e.g. proposed by Macedo et al. 
(2003): the annotation integrates the knowledge of experts 
on narrative structures in the play Hamlet and provides this 
information to the reader. This leads to a multi rooted tree, 
each tree represents one level of information, i.e. textual 
structure and linguistic, philological or narrative information. 
This allows for creating a network of multiple perspectives on 
one text being linked to one another. As a result, hypertext is 
no longer based on links between nodes, but offers a reference 
mechanism between perspectives.
Figure 1: A multi rooted tree above a single textual data
As a fi rst result of these multiple annotations, we got a corpus 
that is based on XML-technology and available via the web. As 
a second result we developed methods to cope with multiple 
annotated documents, which is a task, that has to be performed 
more often with the growing popularity of XML-technologies. 
Especially the integration of the narration annotation layer has 
to bee seen as a example for further parallel annotations. In 
detail these methods described above lead to an environment, 
which offers different types of user different perspectives on a 
single, textual object or a corpus. Some of these benefi ts will 
be presented in the following:
1. The common TEI-annotation allows a structural linking-
mechanism between the editions. This allows a user to jump 
from the fi rst scene in the second act of one edition to the 
same scene in another edition.
2. Alternatively this annotation can be used to present the 
user a part of the play in on or more editions of his choice 
for direct comparison.
3. The narration annotation layer allows several ways to 
explore a single text or compare some texts with each 
other. In the fi rst case, the annotation of events and 
actions provides a way of comparing different editions esp. 
translations.
4. Using SVG - an XML-based format for graphics - the 
narrative structure of each translation could be visualized, 
ignoring the textual basis. This gives an »overview« of plot 
of the current edition.
5. The introduced concept of cross annotation linking allows 
us to offer the user automatically generated links from one 
annotation to another.
With this set of different linking-concepts we offer users a 
new freedom to explore the corpus in a way that fi ts to their 
needs. We ensured, that every layer of information offers 
a way to access information of another layer in a different 
perspective. We assume that this method can be transferred 
to any kind of multiple annotation.
References
Macedo, A. A., Truong, K.N. and Camacho-Guerrero, J. A. 
(2003). Automatically Sharing Web Experiences through a 
Hyperdocument Recommender System. In: Proceedings 
of the 14th conference on Hypertext and Hypermedia 
(Hypertext03), (Nottingham, UK, August, 26-30, 2003). Online: 
http://www.ht03.org/papers/pdfs/6.pdf [Available, Last checked: 
6/16/2005]
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
235
 Fine Rolls in Print and on the 
Web: A Reader Study 
 Arianna Ciula
arianna.ciula@kcl.ac.uk
King’s College London, UK
Tamara Lopez 
tamara.lopez@kcl.ac.uk
 King’s College London , UK
Introduction
A collaboration between the National Archives in the UK, 
the History and Centre for Computing in the Humanities 
departments at King’s College London, the Henry III Fine Rolls 
project (http://www.frh3.org.uk) has produced both a digital 
and a print edition (the latter in collaboration with publisher 
Boydell & Brewer) of the primary sources known as the Fine 
Rolls. This dual undertaking has raised questions about the 
different presentational formats of the two resources and 
presented challenges for the historians and digital humanities 
researchers involved in the project, and, to a certain extent, 
for the publisher too. These challenges, and the adopted 
solutions for the two types of published resource present a 
novel starting point from which to examine how the artefacts 
of digital humanities are used.
This poster will report on the progress to-date of an ongoing 
exploratory study that examines the information-seeking 
behavior of historians, with the aim of developing a clearer 
picture of how research is conducted using sources like the 
Calendar of Fine Rolls. As with other digital humanities efforts 
(Buchanan, Cunningham, Blandford, Rimmer, & Warwick; 2005), 
this study focuses on the ways in which interactions occur 
using physical and virtual environments, and in particular on 
the ways in which the components of hybrid scholarly editions 
are used in combination to answer research questions. A 
secondary pragmatic component of the study seeks to adapt 
methodologies from the fi elds of information seeking research 
and human-computer interaction to the evaluation of digital 
humanities and in particular, to the Fine Rolls of Henry III 
project resources.
Reading the Fine Rolls of Henry III
The two publication formats of the Fine Rolls of Henry III 
are drawn from the same data substrate. Given the nature 
of the materials, reading is expected to be the primary 
activity performed using both. A stated design goal for the 
project is that the two publications will form a rich body of 
materials with which to conduct historical research. Our fi rst 
research goal is to establish the context for work done using 
historical sources like the Fine Rolls: to establish the kinds of 
research questions asked, to articulate the methods followed 
in answering these questions, and to develop profi les of the 
researchers who perform the work.
Given the heterogeneity of the materials and the different focus 
of each medium, the question arises whether the materials do 
in fact form a single body of work, and how the experience 
of using them comprises a continuous experience. It suggests 
that working with the texts will of necessity also involve 
periods of information seeking: moments encountered while 
reading that give rise to questions which the material at hand 
cannot answer and the subsequent process embarked upon 
in order to answer them. We hypothesize that to fi ll these 
information gaps, readers of the Fine Rolls will seek particular 
text in the alternative medium to fi nd answers. To answer a 
question about a translation found in a printed volume, for 
example, we suggest that the researcher will seek the image 
of the membrane in the web edition in order to consult the 
original language of the text.
Having established the impetus for using the two formats 
together, one questions the effectiveness of particular features 
of each medium in creating bridges between the formats. One 
implicit design goal of the project has been to optimize the 
movement between states of reading (within a medium) and 
moments of seeking (between media). Our fi nal research goal, 
therefore is to discover the ways in which design choices taken 
in each facilitate or hinder movement between the website 
and the books, thereby enriching or diminishing the utility of 
the body of work as a whole.
Methodology
As suggested above, our analytical framework for evaluating 
the experience of reading the Fine Rolls of Henry III is drawn 
from the fi eld of information seeking behavior research, and in 
particular by the conceptual framework developed in Brenda 
Dervin’s Sense-Making Theory (Dervin, 1983 as summarized by 
Wilson; Wilson, 1999).
Our methodological approach will similarly be guided by data 
collection methods established in this fi eld. Structured in three 
phases, our study will fi rst identify a representative sample of 
scholars from leading institutions who perform research using 
sources like the Fine Rolls. In the fi rst phase of data collection, 
we intend to use established fi eld and (where necessary) online 
questionnaires to establish the kinds of research questions 
asked of materials like the Fine Rolls, and to articulate the 
methods followed in answering these questions. Given the 
hybrid nature of the Fine Rolls edition, we will seek to elicit 
comments on the use of secondary, primary and surrogate 
(web, microfi lm and microfi che) formats.
A phase of analysis will follow in which we develop profi les of 
the researchers who perform this kind of work to identify the 
information seeking features that exist within materials related 
to a range of different tools, such as indexes, search engines, 
front matter materials, printed catalogues and fi nding aids, and 
the perceived strengths and weaknesses of doing work with 
existing support materials. As the fi nal part of this stage, we 
will identify a series of open-ended research questions that can 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
236
be answered using the Fine Rolls materials. These questions 
will be formulated to encourage use of a range of formats and 
informational features of the components that comprise the 
Fine Rolls resource.
Given our focus on a single scholarly edition and our 
corresponding pragmatic need to develop an understanding of 
the effectiveness and utility of same, the third phase of our work 
will utilize established usability testing techniques to evaluate 
project resources. Drawing upon the profi les developed in the 
fi rst phase of data collection, a sample of representative users 
of the Fine Rolls materials will be selected. In a series of guided, 
task-based sessions, participants will be asked to answer the 
research questions formulated during analysis. Data collection, 
expected to include a combination of direct observation and 
interview will be used to quantitatively identify the information 
features of both editions that are used to answer research 
questions. To generate a more qualitative assessment of the 
effectiveness of the features, users will be encouraged to 
“think-aloud” (Preece, Rogers, & Sharp.; 2002, p. 365) about 
the process and their observations will be recorded.
Sample size permitting, sessions are to be held to evaluate use 
of the book only, the web materials only, and both web and 
book.
Conclusions
With this research, we hope to elicit specifi c information 
about design improvements that can be made to support 
information seeking activities that span the Fine Rolls digital 
and print materials, and to articulate general heuristics that 
can be used in the design of other hybrid digital humanities 
publications. With our focus on a single scholarly edition, we 
also contribute to work begun elsewhere (Buchanan, et. al.; 
2005) to explore how established methods for evaluating 
the use of digital materials can be adapted and applied to 
the work of humanities researchers. Finally, we contribute to 
understanding of the evolution of the scholarly edition as a 
resource that extends beyond the self-contained print edition, 
and of the deepening interdependence between humanities 
research activities in digital and traditional environments.
Notes
[1] The fi rst volume was published in September 2007 (Dryburgh et 
al. 2007).
[2] Our understanding here will both draw upon and contribute to 
the dialog regarding use of electronic sources by humanities scholars 
begun elsewhere, see e.g. Bates, Wiberley, Buchanan, et. al.
Bibliography
Bates, M.; Wilde, D. & Siegfried, S. (1995), ‘Research practices 
of humanities scholars in an online environment: The Getty 
online searching project report no. 3’, Library and Information 
Science Research 17(1), 5--40.
Buchanan, G.; Cunningham, S.; Blandford, A.; Rimmer, J. & 
Warwick, C. (2005), ‘Information seeking by humanities 
scholars’, Research and Advanced Technology for Digital Libraries, 
9th European Conference, ECDL, 18--23.
Dervin, B. (1983), ‘An overview of sense-making research: 
Concepts, methods, and results to date’, International 
Communication Association Annual Meeting, Dallas, TX.
Dryburgh, P. & Hartland, B. eds. Ciula A. and José Miguel Vieira 
tech. Eds. (2007) Calendar of the Fine Rolls of the Reign of Henry 
III [1216-1248], vol. I: 1216-1224, Woodbridge: Boydell & 
Brewer.
Jones, A. ed. (2006) Summit on Digital Tools for the Humanities: 
Report on Summit Accomplishments. Charlottesville, VA: 
University of Virginia.
Preece, J.; Rogers, Y. & Sharp, H. (2002), Interaction design: 
beyond human-computer interaction, Wiley.
Siemens, R.; Toms, E.; Sinclair, S.; Rockwell, G. & Siemens, L. 
‘The Humanities Scholar in the Twenty-fi rst Century: How 
Research is Done and What Support is Needed.’ ALLC/ACH 
2004 Conference Abstracts. Göteborg: Göteborg University, 
2004. <http://www.hum.gu.se/allcach2004/AP/html/prop139.
html>
Wiberley, S. & Jones, W. (2000), ‘Time and technology: A 
decade-long look at humanists’ use of electronic information 
technology’, College & Research Libraries 61(5), 421--431.
Wiberley, S. & Jones, W. (1989), ‘Patterns of information 
seeking in the humanities’, College and Research Libraries 50(6), 
638--645.
Wilson, T. (1999), ‘Models in information behaviour research’, 
Journal of Documentation 55(3), 249--270.
 
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
237
PhiloMine: An Integrated 
Environment for Humanities 
Text Mining 
Charles Cooney
cmcooney@diderot.uchicago.edu,
University of Chicago, USA
Russell Horton
russ@diderot.uchicago.edu
University of Chicago, USA
Mark Olsen
mark@barkov.uchicago.edu
University of Chicago, USA
Glenn Roe
glenn@diderot.uchicago.edu
University of Chicago, USA
Robert Voyer
rlvoyer@diderot.uchicago.edu
University of Chicago, USA
PhiloMine [http://philologic.uchicago.edu/philomine/] is a set 
of data mining extensions to the PhiloLogic [http://philologic.
uchicago.edu/] full-text search and retrieval engine, providing 
middleware between PhiloLogic and a variety of data mining 
packages that allows text mining experiments to be run on 
documents loaded into a PhiloLogic database. We would 
like to present a poster describing and demonstrating how 
PhiloMine works.
The text mining process under PhiloMine has three main 
components -- coprus selection, feature selection and 
algorithm selection. Experimental corpora can be constructed 
from the documents in the PhiloLogic database using standard 
bibliographic metadata criteria such as date of publication or 
author gender, as well as by attributes of sub-document level 
objects such as divs and paragraphs. This makes it possible, 
for example, to compare poetry line groups in male-authored 
texts from 1800 - 1850 with those in female-authored texts 
from that period or any other. The PhiloMine user then selects 
the features to use for the experiment, choosing some or all 
feature sets including surface forms, lemmas, part-of-speech 
tags, and bigrams and trigrams of surface forms and lemmas. 
Once the corpus and feature sets are selected, the machine 
learning algorithm and implementation is chosen. PhiloMine 
can talk to a range of freely available data mining packages such 
as WEKA, Ken William’s Perl modules, the CLUTO clustering 
engine and more. Once the learning process has executed, the 
results are redirected back to the browser and formatted to 
provide links to PhiloLogic display of the documents involved 
and queries for the individual words in each corpus. PhiloMine 
provides an environment for the construction, execution 
and analysis of text mining experiments by bridging the gap 
between the source documents, the data structures that form 
the input to the learning process, and the generated models 
and classifi cations that are its output.
Corpus Selection
Under PhiloMine, the text mining corpus is created by 
selecting documents and sub-document level objects from a 
particular PhiloLogic database. PhiloLogic can load documents 
in a number of commonly used formats such as TEI, RTF, 
DocBook and plain text. From all documents in a particular 
PhiloLogic collection, the text mining corpus is selected using 
bibliographic metadata to choose particular documents, and 
sub-document object selectors to choose objects such as divs 
and line groups. The two levels of criteria are merged, so that 
the experimenter may easily create, for example, a corpus of 
all divs of type “letter” appearing within documents by female 
authors published in Paris in the 19th century. For a supervised 
mining run, the PhiloMine user must enter at least two sets of 
such criteria, and a corpus is created which contains multiple 
sub-corpora, one for each of the classes. For an unsupervised 
mining run, such as clustering, one corpus is created based on 
one set of criteria.
The PhiloMine user is also able to specify the granularity of 
text object size which is presented to the learning algorithm, 
the scope of the “instance” in machine learning terminology. 
Under PhiloMine, an instance may be either an entire 
document, a div or a paragraph. A single document consisting 
of a thousand paragraphs may be presented to a machine 
learner as a thousand distinct text objects, a thousand vectors 
of feature data, and in that case the learner will classify or 
cluster on the paragraph level. Similarly, even if the user has 
chosen to use sub-document level criteria such as div type, 
the selected text objects can be combined to reconstitute a 
document-level object. Thus the PhiloMine experimenter can 
set corpus criteria at the document, div and/or paragraph level 
and independently decide which level of text object to use as 
instances.
Several fi lters are available to ensure that selected text 
objects suit the experimental design. PhiloMine users may set 
minimum and maximum feature counts per instance. They may 
also balance instances across classes, which is useful to keep 
your machine learner honest when dealing with classifi ers 
that will exploit differential baseline class frequencies. Finally, 
instance class labels may be shuffl ed for a random falsifi cation 
run, to make sure that your accuracy is not a result of an over-
fi tting classifi er.
Feature Selection
In machine learning generally, features are attributes of an 
instance that take on certain values. The text mining process 
often involves shredding the documents in the corpus into 
a bag-of-words (BOW) representation, wherein each unique 
word, or type, is a feature, and the number of tokens, or 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
238
occurrences of each type in a given document, is the value 
of that feature for that document. This data structure can 
be envisioned as a matrix, or spreadhsheet, with each row 
corresponding to a text object, or instance, and each column 
representing a type, or feature, with an extra column for class 
label if supervised learning is being undertaken. PhiloMine 
generates a BOW matrix for the user-selected corpus which 
serves as the input to the machine learner.
Because PhiloLogic creates extensive indices of document 
content as part of its loading process, its internal data structures 
already contain counts of words for each document in a given 
database. PhiloMine extends PhiloLogic so that is available for 
divs and paragraphs. In addition to the surface forms of words, 
PhiloMine will also generate vectors for lemmas or parts-of-
speech tags, provided by TreeTagger, and bigrams and trigrams 
of surface forms or lemmas. The user may select one or more 
of these feature sets for inclusion in a given run.
One practical concern in machine learning is the dimensionality 
of the input matrix. Various algorithms scale in different ways, 
but in general adding a new instance or feature will increase the 
time needed to generate a classifi catory model or clustering 
solution, sometimes exponentially so. For this reason, it can 
be very helpful to limit a priori the number of features in 
the matrix before presenting it to the machine learner, and 
PhiloMine provides the capability to fi lter out features based 
on a number of criteria. For each featureset, the user may 
limit the features to use by the number of instances in which 
the feature occurs, eliminating common or uncommon 
features. Additionally, include lists and/or exclude lists may be 
submitted, and only features on the include list and no features 
on the exclude list are retained. Finally, features may be fi ltered 
by their value on a per-instance basis, so that all features 
that occur more or less times than the user desires may be 
removed from a given instance, while remaining present in 
other instances.
Algorithm and Implementation 
Selection
PhiloMine can wrap data for, and parse results from, a 
variety of machine learning implementations. Native Perl 
functions currently include Ken William’s naive Bayesian and 
decision tree classifi ers, a vector space implementation and a 
differential relative rate statistics generator. The WEKA toolkit 
provides numerous implementations and currently PhiloMine 
works with the information gain, naive Bayes, SMO support 
vector machine, multilayer perceptron, and J48 decision tree 
WEKA components. PhiloMine also can talk to the compiled 
SVMLight support vector machine and CLUTO clustering 
engine. Relevant parameters for each function may also be set 
on the PhiloMine form.
When the user selects an implementation, the feature vectors 
for each instance are converted from PhiloMine’s internal 
representation into the format expected by that package, 
generally a sparse vector format, such as the sparse ARFF 
format used by WEKA. The mining run is initiated either by 
forking a command to the system shell or by the appropriate 
Perl method call. Results of the run are displayed in the 
browser, typically including a list of text objects instances with 
classifi cation results from the model and a list of features used 
by the classifi er. Each instance is hyperlinked to the PhiloLogic 
display for that text object, so that the user can easily view 
that document, div or paragraph. Similarly, the user can push a 
query to PhiloLogic to search for any word used as a feature, 
either in the entire corpus or in any of the classed sub-corpora. 
If results show, for instance, that a support vector machine has 
heavily weighted the word “power” as indicative of a certain 
class of documents, the experimenter can quickly get a report 
of all occurrences of “power” in that class of documents, any 
other class or all classes.
This ability to easily move between the text mining results 
and the context of the source documents is meant to mitigate 
some of the alienating effects of text mining, where documents 
become anonymous instances and words are reduced to 
serialized features. For industrial applications of text mining, 
the accuracy of a certain classifi er may be the only criterion 
for success, but for the more introspective needs of the digital 
humanist, the mining results must be examined and interpreted 
to further the understanding of the original texts. PhiloMine 
allows researchers to frame experiments in familiar terms 
by selecting corpora with standard bibliographic criteria, and 
then relate the results of the experiment back to the source 
texts in an integrated environment. This allows for rapid 
experimental design, execution, refi nement and interpretation, 
while retaining the close association with the text that is the 
hallmark of humanistic study.
 
  
  
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
239
The Music Information 
Retrieval Evaluation 
eXchange (MIREX): 
Community-Led Formal 
Evaluations
 J. Stephen Downie
jdownie@uiuc.edu
University of Illinois, USA
Andreas F. Ehmann
aehmann@uiuc.edu
University of Illinois, USA
Jin Ha Lee 
jinlee1@uiuc.edu
 University of Illinois , USA
Introduction
This paper provides a general overview of the infrastructure, 
challenges, evaluation results, and future goals of the Music 
Information Retrieval Evaluation eXchange (MIREX). MIREX [1] 
represents a community-based formal evaluation framework 
for the evaluation of algorithms and techniques related to 
music information retrieval (MIR), music digital libraries (MDL) 
and computational musicology (CM). MIREX is coordinated 
and managed by the International Music Information Retrieval 
Systems Evaluation Laboratory (IMIRSEL) at the University of 
Illinois at Urbana-Champaign. To date, since its inception in 
2005, three annual MIREX evaluations have been performed 
covering a wide variety of MIR/MDL/CM tasks. The task 
defi nitions and evaluation methods for each annual MIREX are 
largely determined by community discussion through various 
communication channels with dedicated Wikis [4] playing a 
special role. Section 2 presents and defi nes the tasks associated 
with each of the past three MIREX evaluations.
In many respects, MIREX shares similarities to the Text 
Retrieval Conference (TREC) [5] in its overall approach to 
handling the evaluation of algorithms designed by the research 
community. Both MIREX and TREC are predicated upon the 
standardization of data collections; the standardization of the 
tasks and queries to be performed on the collections; and the 
standardization of evaluation methods used on the results 
generated by the tasks/queries [1]. However, associated with 
MIREX, there exist a unique set of challenges that cause 
MIREX to deviate from many of the methodologies associated 
with TREC. Section 3 covers some of these challenges, and 
the resultant solutions that comprise the overall framework 
and methodology of how MIREX evaluations are executed. 
Since MIREX is an ever-evolving entity, Section 4 will present 
key advances made between MIREX 2005 and MIREX 2006, as 
well as future goals for MIREX 2007 and beyond.
MIREX 2005, 2006, and 2007 tasks
The tasks associated with MIREX 2005, 2006 and 2007 are 
shown in Table 1. 
Table 1. Task lists for MIREX 2005, 2006, and 2007 
(with number of runs evaluated for each)
TASK 2005 2006 2007
Audio Artist Identifi cation 7 7
Audio Beat Tracking 5
Audio Classical Composer 
Identifi cation
7
Audio Cover Song Identifi cation 8 8
Audio Drum Detection 8
Audio Genre Classifi cation 15 7
Audio Key Finding 7
Audio Melody Extraction 10 10
(2 subtasks)
Audio Mood Classifi cation 9
Audio Music Similarity 
and Retrieval
6 12
Audio Onset Detection 9 13 17
Audio Tempo Extraction 13 7
Multiple F0 Estimation 16
Multiple F0 Note Detection 11
Query-by-Singing/Humming 23
(2 subtasks)
20
(2 subtasks)
Score Following 2
Symbolic Genre Classifi cation 5
Symbolic Key Finding 5
Symbolic Melodic Similarity 7 18
(3 subtasks)
3
A more detailed description of the tasks, as well as the formal 
evaluation results can be found on each year’s associated 
Wiki pages [4]. The tasks cover a wide range of techniques 
associated with MIR/MDL/CM research, and also vary in 
scope. Tasks such as “Audio Onset Detection” (i.e., marking 
the exact time locations of all musical events in a piece of 
audio) and “Audio Melody Extraction” (i.e., tracking the pitch/
fundamental frequency of the predominant melody in a piece 
of music) can be considered low-level tasks, in that they are 
primarily concerned with extracting musical descriptors from 
a single piece of musical audio. The motivation for evaluating 
such low-level tasks is that higher level MIR/MDL/CM systems 
such as “Audio Music Similarity and Retrieval” (i.e., retrieving 
similar pieces of music in a collection to a specifi ed query 
song) or “Audio Cover Song Identifi cation” (i.e., fi nding all 
variations of a given musical piece in a collection) can be built 
using many of the techniques involved in the low-level audio 
description tasks.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
240
Table 2 provides a summary of participation in the past three 
MIREX evaluations. To date, 300 algorithm runs have been 
performed and evaluated.
Table 2. Summary data for MIREX 2005, 2006, and 2007 
COUNTS 2005 2006 2007
Number of Task (and Subtask) “Sets” 10 13 12
Number of Teams 41 46 40
Number of Individuals 82 50 73
Number of Countries 19 14 15
Number of Runs 86 92 122
Challenges and Methodology
Although largely inspired by TREC, MIREX differs signifi cantly 
from TREC in that the datasets for each task are not freely 
distributed to the participants. One primary reason for the 
lack of freely available datasets is the current state of copyright 
enforcement of musical intellectual property preventing the 
free distribution of many of the collections used in the MIREX 
evaluations. In addition, MIREX relies heavily on “donated” 
data and ground-truth. For tasks which require extremely 
labor-intensive hand-annotation to generate ground-truth — 
most notably low-level description tasks such as “Audio Onset 
Detection” — there is an overall reluctance of contributors 
to make their data and annotations freely available. As a result, 
it is nearly impossible to generate a representative dataset 
that encompasses all possible varieties, instrumentations, 
etc. of music. As such, there exists the potential of “tuning” 
or “overfi tting” to a specifi c dataset at the expense of 
generalizablity of the algorithm to all varieties and types of 
music.
Due to the inability to freely distribute data, MIREX has 
adopted a model whereby all the evaluation data are housed in 
one central location (at IMIRSEL). Participants in MIREX then 
submit their algorithms to IMIRSEL to be run against the data 
collections. This model poses a unique set of challenges for the 
IMIRSEL team in managing and executing each annual MIREX. 
Firstly, data must be gathered and managed from various 
sources. For some tasks, differing formats for both the data 
and ground truth exist, as well as the potential for corrupted 
or incorrectly annotated ground-truth necessitating testing 
of the integrity of the data itself. The music collections used 
for MIREX tasks have already surpassed one terabyte and are 
continuously growing. In addition, many algorithms generate 
a large amount of intermediate data in their execution which 
must also be managed. In some cases, the intermediate data 
are larger in size than the actual music they describe and 
represent.
Moreover, IMIRSEL is responsible for supporting a wide 
variety of programming languages (e.g., MATLAB, Java, C/C++, 
PERL, Python, etc.) across different platforms (e.g., Windows, 
*NIX, MacOS, etc.). Despite guidelines dictating fi le input/
output formats, coding conventions, linking methods, error 
handling schemes, etc., the largest amount of effort expended 
by IMIRSEL is in compiling, debugging, and verifying the output 
format and validity of submitted algorithms. Collectively, 
submissions to MIREX represent hundreds of hours of CPU 
computation time and person-hours in managing, setting up, 
and performing their execution.
Advances and Future Goals
One of the most signifi cant advances made after MIREX 
2005 was the incorporation of musical similarity evaluation 
tasks contingent upon subjective, human evaluation (MIREX 
2006 and 2007). The addition of human evaluations of music 
similarity systems was born out of a community desire to 
refl ect real-world applications and needs, and culminated 
in the “Audio Music Similarity and Retrieval” and “Symbolic 
Melodic Similarity” tasks. Both similarity tasks involve 
retrieving the top-N relevant or “similar” musical pieces in 
a collection using a specifi c musical piece as a query. A web 
interface with embedded audio players called the Evalutron 
6000 was designed to allow evaluators to judge the similarity 
of a query “seed” with a retrieved “candidate” on both a broad 
scale (i.e., Not Similar, Somewhat Similar, Very Similar) and a 
fi ne, continuous, 10-point scale [3]. 
Another signifi cant advance made manifest in MIREX 2006, 
and then repeated for MIREX 2007, was the application of 
formal statistical signifi cance testing of returned results. These 
tests were applied in order to test whether performance 
differences between systems were truly signifi cant. Because 
of its non-parametric nature, Friedman’s ANOVA test was 
used on a variety of tasks to compare system performances. 
In general, these tests have shown that there are clusters of 
top performing techniques but these top-ranked techniques 
are not performing signifi cantly better than their other top-
ranked peers.
For future MIREX evaluations, IMIRSEL is presently developing 
a web service system that intends to resolve some of the key 
challenges associated with the execution of submitted algorithms 
by placing many of the responsibilities in the participant’s hands. 
The web service, called MIREX DIY, represents a “black box” 
architecture, whereby a participant submits their algorithm/
code through the web service, remotely begins its execution, 
and receives real-time feedback regarding the execution state 
of their algorithm. Execution failures can be monitored by the 
participant, and fi xed if necessary. Upon successful execution 
and completion of the algorithm, performance results are 
returned to the participant. Eventually, such a system would 
allow submission and evaluation of algorithms year-round. Feel 
free to explore the MIREX DIY demo at http://cluster3.lis.
uiuc.edu:8080/mirexdiydemo. 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
241
Acknowledgments
MIREX has received considerable fi nancial support from both 
the Andrew W. Mellon Foundation and the National Science 
Foundation (NSF) under grant numbers NSF IIS-0340597 and 
NSF IIS- 0327371.
References
[1] Downie, J. S. 2006. The Music Information Retrieval 
Evaluation eXchange (MIREX). D-Lib Magazine, 12, 12 
(December 2006: http://www.dlib.org/dlib/december06/
downie/12downie.html.
[2] Downie, J. S., West, K., Ehmann, A., and Vincent, E. 2005. 
The 2005 Music Information Retrieval Evaluation eXchange 
(MIREX 2005): Preliminary overview. In Proceedings of the 6th 
International Conference on Music Information Retireval (ISMIR 
2005), Queen Mary, UK, 2005, pp. 320-323.
[3] Gruzd, A. A., Downie, J. S., Jones, M. C., and Lee, J. H. 2007. 
Evalutron 6000: Collecting music relevance judgments. ACM 
IEEE Joint Conference on Digital Libraries 2007, p. 507.
[4] MIREX Wiki: http://music-ir.org/mirexwiki/.
[5] TREC: http://trec.nist.gov/.  
 
Online Collaborative 
Research with REKn and 
PReE
 Michael Elkink
melkink@uvic.ca
University of Victoria, Canada
Ray Siemens
siemens@uvic.ca
University of Victoria, Canada
Karin Armstrong
karindar@uvic.ca
 University of Victoria, Canada 
The advent of large-scale primary resources in the humanities 
such as EEBO and EEBO-TCP, and similarly large-scale 
availability of the full-texts secondary materials through 
electronic publication services and amalgamators, suggests 
new ways in which the scholar and student are able to interact 
with the materials that comprise the focus of their professional 
engagement. The Renaissance English Knowledgebase (REKn) 
explores one such prospect. REKn attempts to capture and 
represent essential materials contributing to an understanding 
of those aspects of early modern life which are of interest to the 
literary scholar - via a combination of digital representations 
of literary and artistic works of the Renaissance plus those of 
our own time refl ecting our understanding of earlier works. 
REKn contains some 13,000 primary sources at present, plus 
secondary materials such as published scholarly articles and 
books chapters refl ecting our understanding of these earlier 
works (some 100,000). These materials are accessed through 
a Professional Reading Environment (PReE), supported by a 
database system that facilitates their navigation and dynamic 
interaction, also providing access to inquiry-oriented analytical 
tools beyond simple search functions. The effect is that of 
providing an expert reading environment for those in our fi eld, 
one that encourages close, comprehensive reading at the same 
time as it provides, conveniently, the building blocks of broad-
based research inquiry. We are currently moving beyond the 
stage of proof-of-concept with these projects.
Our current research aim with these projects is to foster 
social networking functionality in our professional reading 
environment. For DH2008 we propose a poster/demo that 
details the current status of both the knowledgebase (REKn) 
and the reading environment (PREe) in relation to social 
networking, the direction each will take in the future, and a 
demonstration of the functional technologies we employ and 
our current implementation.
Rather than leveraging the power of an individual computer 
to perform complex computation on a personalized data set 
- which is the way most academics appear to work (see, for 
example, Siemens, et al., 2004), and is an approach exemplifi ed 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
242
in our community by Bradley (2007) and the TAPoR research 
team (TAPoR 2007; Rockwell 2006) - our work complements 
that approach by attempting to harness the power of the social 
connectivity provided by current Web 2.0 practices to connect 
researchers and experts, authors and reviewers, computers 
and humans, all with the express goal of bringing a higher 
level of discourse, analysis and structure to the documents 
brought together in the REKn corpus (for one infl uential 
adaptation, slightly outside our domain, see Ellison 2007). We 
are considering more than just a simple full-text search as a 
starting point for research discovery, even sophisticated forms 
of search (as per Schreibman, et al.); rather, we are envisioning 
ways existing social technologies can be used in concert with 
search processes to facilitate the process of professional 
reading (as per Warwick, et al., 2005). A further goal of our 
current work is to integrate more readily a generalized subset 
of analytical tools, derived from the TAPoR project and others, 
so that other large corpora similar to REKn can benefi t from 
the computational and user-generated connections among 
material facilitated by our system; the beginnings of this system 
will be demonstrated as well (as per Elkink, et al., 2007).
References
Bradley, John. “Making a Contribution: Modularity, Integration 
and Collaboration Between Tools in Pliny”. Paper presented 
at Digital Humanities 2007 June, 2007. 
Ellison, Nicole B., Charles Steinfi eld and Cliff Lampe. “The 
Benefi ts of Facebook ‘Friends:’ Social Capital and College 
Students’ use of Online Social Network Sites.” Journal of 
Computer-Mediated Communication 12 (2007): 1143-1168.
Elkink, Michael, Ray Siemens, Karin Armstrong. “Building One 
To Thrown Away, Toward The One We’ll Keep: Next Steps 
for the Renaissance English Knowledgebase (REKn) and the 
Professional Reading Environment (PReE).” Presented at the 
Chicago Colloquium on Digital Humanities and Computer 
Science. October 2007.
Rockwell, Geoffrey. “TAPoR: Building a Portal for Text 
Analysis.” pp. 285-300 in Siemens and Moorman.
Schreibman, S., Sueguen, G., & Roper, J. “Cross-collection 
searching: A pandora’s box or the holy grail.” Literary and 
Linguistic Computing. In press.
Siemens, Ray, and David Moorman, eds. Mind Technologies: 
Humanities Computing and the Canadian Academic Community. 
Calgary: U Calgary P, 2006.
Siemens, Ray, Elaine Toms, Stéfan Sinclair, Geoffrey Rockwell, 
and Lynne Siemens. “The Humanities Scholar in the Twenty-
fi rst Century: How Research is Done and What Support is 
Needed.” ALLC/ACH 2004 Conference Abstracts. Göteborg: 
Göteborg U, 2004.
TAPoR: Text Analysis Portal for Research. 2007. http://portal.
tapor.ca/.
9. Warwick, C., Blandford, A., Buchanan, G., J. Rimmer, J. 
(2005) “User Centred Interactive Search in the Humanities.” 
Proceedings of the Joint Conference on Digital Libraries. Denver, 
Colorado, June 7-11, 2005. ACM Publications. pp. 279-81.
 
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
243
A Computerized Corpus of 
Karelian Dialects: Design and 
Implementation
 Dmitri Evmenov 
dmitri.evmenov@gmail.com
 St. Petersburg State University , Russian Federation
In my presentation, I intend to cover in detail the main 
project I am working on at the moment, namely designing and 
implementing the computerized corpus of Karelian language 
dialects.
During the decades of scientifi c study of Karelian language, 
initiated in mid-19th century by Finnish scholars and largely 
expanded by Russian/Soviet linguists later on, a large volume 
of material, most remarkably dialectal speech samples, was 
amassed. Those data are however to a large extent essentially 
inaccessible to research due to lack of representative and 
accessible solutions allowing for representation of that rich 
material. Therefore in my research project I aim at developing 
and building an annotated computerized corpus of Karelian 
dialects as well as developing recommendations regarding the 
corpus’ further expansion.
During the fi rst stage of implementation a moderately sized 
“pilot corpus” is to be built so that different strategies and 
tools for annotation could be developed and tested with its 
help. The pilot corpus is to be expanded later on by feeding in 
other available source materials.
The pilot corpus shall contain dialectal speech samples 
belonging to one dialectal group, the Olonets Karelian (Livvi), 
mostly because there’s more extant dialectal speech samples 
recorded for Olonets Karelian than for other groups, namely 
Karelian Proper and Ludik dialects. Also, albeit certainly 
endangered due to numerous reasons, Olonets Karelian 
yet shows less signs of attrition and interference with 
neighbouring languages (Veps, Finnish, and Russian) than the 
above mentioned two different dialectal groups.
The representativeness of the pilot corpus is to be achieved, 
above all, by proportional inclusion of dialectal speech samples 
from all language varieties found in the areal where Karelian 
language is spoken. In order to better account for dialectal 
variation in case of Karelian language, it appears reasonable to 
include into corpus dialectal material from each administrative 
division unit (volost), the volume being 100 000 symbols per 
one such unit).
It is intended to employ demographic criteria alongside with 
geographic ones during material selection. In grouping the 
informants in terms of their age, it appears reasonable to 
follow the division into “elder” (born in 1910-1920s), “middle” 
(born in 1930-1940s) and “younger” (born in 1950-1960s) 
groups. As for gender representativeness, equal representation 
of male and female informant’s speech in the corpus appears 
impossible, at least for elder groups. The informant’s education 
level, place of studies and career biography are all to be taken 
into consideration as well.
It is necessary also to include the information that the 
informant provides about her linguistic competence (which 
language she considers her native, how many language and to 
which extent she knows and can use) and performance (the 
domains where she uses Karelian language, in terms of Joshua 
Fishman’s domain theory).
In the beginning of pilot corpus’ implementation it is intended 
to use the already published samples of Karelian dialectal 
speech, while at later stages other published and specially 
transcribed materials are to be added, mostly those now 
stored in the archives of the Institute of Language, Literature 
and History of Karelian Research Center of Russian Academy 
of Sciences (Petrozavodsk, Russia).
Every block of dialectal material included into the corpus is to 
be accompanied by metadata, including the following:
- informant data (gender, age, place of birth, duration of stay 
in the locality where the record was made, duration and 
circumstances of stay away from Karelian language areal, 
native language according to informant’s own judgment, 
informant’s judgment regarding her mastery of Karelian, 
Russian and other languages, language choice habits for 
various usage domains)
- data on the situation of speech sample recording 
(researcher and informant dialogue, recording of the 
informant’s spontaneous monological speech, recording of a 
dialogue where two or more informants are participating); 
it appears reasonable to develop a taxonomy of situations in 
order to encode it later on in corpus
- the theme of the conversation recorded and transcribed; 
in this case it also appears reasonable to develop and 
enhance an appropriate taxonomy to be employed for data 
encoding at a later stage of corpus expansion.
The detailed way of representing the data in the corpus 
(“the orthography”) is to follow the universally accepted 
Standard Fenno-Ugric Transcription (Suomalais-Ugrilainen 
Tarkekirjoitus), although there are certain challenges, 
mainly stemming from not so easily encodable diacritic 
signs combinations, that require their own solutions to be 
developed; implementation details will surely depend on a 
chosen technology and/or software platform to be chosen 
for use. It should be mentioned though, that the borders of 
prosodic phrases normally marked in transcribed speech 
samples will be saved in the corpus as well and used later on 
for purposes of syntactic annotation.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
244
For morphological annotation, a united intra-dialectal 
morphological tag set is to be developed; pilot corpus will be 
annotated manually, while later stages might be annotated with 
the help of existing parsing and tagging software, inasmuch as 
it is applicable for our purposes.
Other design and implementation details, now being actively 
worked upon, are also to be included into the presentation.
 
 
  
 
 
 
Digitally Deducing 
Information about the Early 
American History with 
Semantic Web Techniques 
 Ismail Fahmi
i.fahmi@rug.nl
University of Groningen, The Netherlands
Peter Scholing
pscholing@gmail.com
University of Groningen, The Netherlands
Junte Zhang 
j.zhang@uva.nl
University of Amsterdam, The Netherlands
Abstract: We describe the Semantic Web for History (SWHi) 
System. The metadata about the Early American History 
is used. We apply Information Retrieval and Semantic Web 
technologies. Furthermore, we use Information Visualization 
techniques. Our resulting information system improves 
information access to our library’s fi nding aids.
Introduction
The Early American Imprints Series are a microfi che collection 
of all known existing books, pamphlets and periodical 
publications printed in the United States from 1639-1800, 
and gives insights in many aspects of life in 17th and 18th 
century America, and are based on Charles Evans’ American 
Bibliography. Its metadata consist of 36,305 records, which are 
elaborately described (title, author, publication date, etc) with 
numerous values, and have been compiled by librarians in the 
format MARC21, which we encoded in XML.
The Semantic Web for History (SWHi) project at the Digital 
Library department of the University Library Groningen has 
worked with the MARC21 metadata of this dataset. Our 
project aims to integrate, combine, and deduce information 
from this dataset to assist general users or historians in 
exploring American history by using new technology offered 
by the Semantic Web. Concretely, we developed a semantic 
search application for historical data, especially from a Digital 
Library point of view.
Semantic Web technologies seem ideally suited to improve and 
widen the services digital libraries offer to their users. Digital 
libraries rely heavily on information retrieval technology. The 
Semantic Web might be used to introduce meaningful and 
explicit relations between documents, based on their content, 
thereby allowing services that introduce forms of semantic 
browsing supplementing, or possibly replacing keyword-based 
searches. This is also a theme we address in SWHi project.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
245
Semantic Web and Information 
Retrieval
Digital libraries increase their amount of metadata each 
day, but much of these metadata is not used as a fi nding aid. 
We have used these metadata to create a Semantic Web 
compatible “historical” ontology. Ontologies are the backbone 
of the Semantic Web, and also play a pivotal role in the SWHi 
application. One of the ideas of the Semantic Web is that 
existing resources can be reused. This was also one of the key 
ideas of our project.
Our ontology is constructed by using the existing ontology 
PROTON as its skeleton, and is enriched with other schemas. 
We reuse existing topical taxonomies created by librarians 
and other experts. We also extracted topic hierarchies from 
the metadata. To describe objects with more semantics and 
create relationships between objects, we also enriched the 
ontology with the vocabularies Dublin Core and Friend of a 
Friend (FOAF).  For example, we link instances with time and 
space, but also topics and persons. This is useful for discovering 
historical social networks, exploring gazetteers, and clustering 
instances together by topic for faceted search, etc. We 
aligned the library metadata with schemas and vocabularies. 
Information is extracted from the metadata to populate the 
ontology. All of this is eventually processed and encoded in 
XML with RDF/OWL. In addition to the PROTON’s basic 
ontology modules, we get 152 new classes from this mapping. 
And in total, we get 112,300 ontology instances from the 
metadata.
 We also combine Information Retrieval technology with 
Semantic Web techniques.  We use the open-source search 
engine SOLR to index ontology instances, parse user input 
queries, and eventually retrieve matching ontology instances 
from the search results. It supports faceted search and has 
been been designed for easy deployment.
During the indexing stage of the data, we apply inference 
from the ontology as a propagation for the importance of 
the different metadata records and fi elds. Using the ontology, 
instances with higher relevance can have higher position in the 
order. For example, a person who is known by many people 
and created many documents would get a higher score. We 
use Sesame for storage and retrieve using the RDF query 
language SPARQL.
User Access
On top of our storage and retrieval components, we developed 
novel techniques for our semantic search application to 
visualize information and offer users to browse for that 
information in an interactive manner. We let users search for 
information semantically, which means that information is 
linked together with certain relations. Results are clustered 
together based on such relations which allows faceted search 
by categories, see fi gure 1. We provide context to relevant 
nuggets of information by enabling users to traverse related 
RDF graphs. We visualize interconnected results using network 
graphs with the TouchGraph tool.
Illustration 1: Result list for query “saratoga”.
We picked up this idea from the ‘berrypicking’ model: a user 
searches, picks a berry (a result), stores it in his basket, view 
the relations between the berries in the basket, and the search 
iteration continues. The purpose is to fi nd new information 
from the collected berries (results). 
As we are dealing with historical data, chronological timeline 
views are also presented using SIMILE’s Timeline, which lets 
users browse for information by time. Besides time, we 
also offer users to search by location using Google Maps. 
Geographical entities, mostly locations in the US, are aligned 
with Google Maps. 
In summary, we have four modes of visualization which gives 
users multiple views of the results: plain list, faceted search, 
timeline, and map. We have found that translating such an 
interface to an on-line environment offers interesting new ways 
to allow for pattern discovery and serendipitous information 
seeking. Adding information visualization tools like interactive 
and descriptive maps and time-lines to the electronic fi nding 
aid’s interface could further improve its potential to augment 
cognition, and hence improve information access. 
Conclusions
We presented the Semantic Web for History (SWHi) system, 
which deals with historical data in the form of library fi nding 
aids. We employed Semantic Web and Information Retrieval 
technologies to obtain the goal of improving user access to 
historical material.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
246
Bibliography
Grigoris Antoniou, Frank van Harmelen. A Semantic Web 
Primer. The MIT Press, 2004. 
Berners-Lee, T., Hendler, J., and Lassila, O. The semantic web. 
A new  form of web content that is meaningful to computers 
will unleash a revolution of  new possibilities. The Scientifi c 
American, 2001. 
Ismail Fahmi, Junte Zhang, Henk Ellermann, and Gosse Bouma. 
“SWHi System Description: A Case Study in Information 
Retrieval, Inference, and Visualization in the Semantic Web.” 
The Semantic Web: Research and Applications, volume 4519 of 
Lecture Notes in Computer Science, pages 769-778. Springer, 
2007.
Dieter Fensel, Wolfgang Wahlster, Henry Lieberman, James 
Hendler. Spinning the Semantic Web: Bringing the World Wide 
Web to Its Full Potential. MIT Press, 2002.
Lucene SOLR. http://lucene.apache.org/solr/
Semantic Web for History (SWHi). http://evans.ub.rug.nl/swhi
SIMILE Timeline. http://simile.mit.edu/timeline/
Junte Zhang, Ismail Fahmi, Henk Ellermann, and Gosse Bouma. 
“Mapping Metadata for SWHi: Aligning Schemas with Library 
Metadata for a Historical Ontology.” Web Information Systems 
Engineering -- WISE 2007 Workshops, volume 4832 of Lecture 
Notes in Computer Science, pages 102-114. Springer, 2007.
 
 
 
TauRo - A search and 
advanced management 
system for XML documents 
Alida Isolani
isolani@signum.sns.it
Scuola Normale Superiore -Signum, Italy
Paolo Ferragina
ferragina@di.unipi.it
Scuola Normale Superiore -Signum, Italy
Dianella Lombardini
dianella@signum.sns.it
Scuola Normale Superiore -Signum, Italy
Tommaso Schiavinotto
 Scuola Normale Superiore -Signum , Italy
With the advent of Web 2.0 we have seen a radical change 
in the use of Internet, which is no longer seen as a tool from 
which to draw information produced by others, but also 
as a means to collaborate and to share ideas and contents 
(examples are Wikipedia, YouTube, Flickr, MySpace, LinkedIn, 
etc.). It is with this in mind that TauRo1 was designed – a user-
friendly tool with which the user can create, manage, share, 
and search digital collections of XML documents via Web. TauRo 
is indeed a collaborative system through which the user who 
has access to Internet and to a browser can publish and share 
their own XML documents in a simple and effi cient manner, 
thus creating personal and/or shared thematic collections. 
Furthermore, TauRo provides extremely advanced search 
mechanisms, thanks to the use of a search engine for XML 
documents, TauRo-core – an open source software that offers 
implemented search and analysis functions to meet the current 
need of the humanity texts encoding.
TauRo-core: the search engine
The success of XML as an online data exchange format on the 
one hand, and the sucess of the search engines and Google on 
the other, offer a stimulating technological opportunity to use 
great quantities of data on standard PCs or on other portable 
devices such as smart-phones and PDA. The TauRo-core search 
engine is an innovative, modular, and sophisticated software 
tool which offers the user compressed storing and effi cient 
analysis/research of arbitrary patterns in large collections 
of XML documents that are available both on a single PC 
and distributed among several PCs which are dislocated 
in various areas of the network. The fl exibility of TauRo-
core’s modular architecture along with the use of advanced 
compression techniques for the storing of documents and for 
the memorization of indexes, makes it suitable to be used in 
the various scenarios illustrated in Figure 1.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
247
Figure 1 – Application scenario for TauRo-core: 
client-server (a), distributed (b), P2P (c).
In particular, the use of the system in centralized modality – 
that is, in which both the documents and the engine are located 
in the same server – is already operative and suitable tested 
via implementation of the system on the Web (TauRo). We are 
currently working on the structure of Web services – matching 
the distributed mode – in order to supply collection creation, 
submission of documents, search, and consultation services. 
Experiments have also been run to make it possible to consult 
collections via PDA or smart-phone: via a specifi c interface the 
user can make a query and consult the documents effi ciently 
by using the Nokia Tablet 770.
This way, we can also evaluate the behavior of the software 
with reduced computational and storing resources.
Compared to the search engines available on the international 
scene, TauRo-core offers added search and analysis functions 
to meet the current needs of the humanity texts encoding. 
Indeed, these may be marked in such a way as to make their 
analysis diffi cult on behalf of the standard search engines 
designed for non-structured documents (i.e. Web search 
engines), or for highly-structured documents (i.e. database), or 
for semi-structured documents (i.e. search engines for XML), 
but in which these are no assumptions on the semantics of the 
mark-up itself. 
TauRo-core, instead, allows the user to index XML texts for 
which the opportune tag categories have been defi ned. These 
tags are denominated smart-tag2, and they are associated with 
specifi c management/search guidelines. In order to appreciate 
the fl exibility of the smart-tag concept, we have illustrated the 
classifi cation here below: 
• jump-tag: the tags of this group indicate a temporary 
change in context – as in the case of a tag that indicates 
a note – and this way the tag content is distinct from the 
actual text and the search takes place while distinguishing 
the two semantic planes. 
• soft-tag: these tags involve a change of context, if the 
starting or ending element of the tag is present within a 
character string which is not separated by a space, the string 
forms a single word. 
• split-tag: the tags to which a meaning similar to the word 
separator is assigned, fall within this category. Therefore, 
the context does not change and the words are in effect 
considered as separate. 
Furthermore, TauRo-core offers its own query language, 
called TRQL, which is powerful enough to allow the user 
to carry out complex text analysis that take into account the 
above classifi cation and the relationship between content and 
structure of the documents. TRQL operates on document 
collections, manages the smart-tag and implements the main 
full-text search functions requested in the specifi cs of the 
W3C3 for XQuery.
This fl exibility allows TauRo-core to also be used in contexts 
that are different from the strictly literary one; for example, 
the collection of documents of the public administration, 
biological archives, manuals, legislative collections, news, etc. 
The literary context however remains the most complex and 
thus constitutes, due to its peculiar lack of uniformity, a more 
stimulating and meaningful trial.
TauRo: the system on the Web
TauRo is a collaborative system that allows any Web user, 
after free registration, to create and share collections of XML 
documents, and to exploit the potential of TauRo-core to run 
full-text searches for regular expressions, by similarity, and 
searches within the XML document structure. These searches 
can be run on a single collection at a time or on various 
collections simultaneously, independently from their nature. 
Aided screenshots, we show here below some characteristics 
of the system. 
Figure 2 – TauRo home page
The collections
Each registered user can upload on TauRo their own collection 
of XML documents. Once uploaded, the collection will be 
indexed by TauRo-core and made available to the next search 
operations. The user may, at any time, modify their collection 
by adding or deleting documents, by moving documents from 
one collection to the other, and share documents between 
various collections, or modify the status of a collection that 
can be: 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
248
• private: accessible and searchable only by the owner;
• public: searchable by all the users after registering and 
modifi able only by the owner;
• invite-only: this collections can be subscribed only after 
invitation by one of the collection administrators. However, 
the user has the possibility to ask for the invitation.
Figure 3 – Collection edit form. 
During the uploading or modifi cation of a collection, the 
user can select some parameter settings, such as the smart-
tag and page interruption tag specifi cs, for the purpose 
of exploiting to the fulltest the search engine’s potential. A 
further personalization option offered by TauRo consists in 
associating each collection with its own XSL stylesheet4 aimed 
at visualizing in a pleasant way the results of the searches run 
on them.
The documents
The system provides the user with a group of functions that 
can upload, classify, and remove XML documents from the 
user’s collections. During the upload, the system will try to 
run an automatic acknowledgment of the DTD and of the 
entity fi les used in the XML document by comparing the public 
name with those previously saved. If the acknowledgment fails, 
the user is give the option of entering the information. Every 
document can be freely downloaded by anyone or one of the 
Creative Commons5 licenses that safeguard the owner from 
improper use can be selected. 
Figure 4 – In the foreground is the document edit form, 
and in the background the list of documents. 
Search
TauRo offers two different search modes. The fi rst is designed 
to search for words within some documents (basic search), 
the second allows the user to also construct structural type 
of queries, namely on elements – tags and attributes – of the 
mark-up via a graphic interface (advanced search). In both cases 
the queries are translated into a syntax that can be interpreted 
by TauRo-core and sent to it. The search result is the list 
of documents of the collection which verify the query, set 
alongside the distribution of the results within the documents 
themselves. By selecting a document, the user accesses a list 
of contextualized occurrences, namely those entered in a text 
fragment which contains them, with the possibility of directly 
accessing the text as a whole. 
In both cases the search can be exact, by prefi x, suffi x, standard 
expression or by difference. The user can specify several words, 
and, in this case, they will appear next to the document. A basic 
search can also be run on several collections simultaneously. 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
249
Figure 5 – Search results
The search results still consist in a list of collection documents 
that verify the query. By selecting a document, the user accesses 
the list of occurrences (Figure 5 ) that represents the starting 
point for accessing the text.
The project has been designed to allow any user to try and 
exploit the potential of the search engine by using their PC, 
and without having to install complex software. Thanks to 
the potentials of TauRo, indeed any user in any part of the 
world may create and manage via Web their own XML text 
collection. 
Notes
1 http://tauro.signum.sns.it
2 L. Lini, D. Lombardini, M. Paoli, D. Colazzo, C. Sartiani, XTReSy: A Text 
Retrieval System for XML Documents. In Augmenting Comprehension: 
Digital Tools for the History of Ideas, ed by H. Short, G. Pancaldi, D. 
Buzzetti, luglio 2001. Offi ce for Humanities Communication 
Publications 2001.
3 http://www.w3.org/TR/xquery-full-text-requirements/ Specifi cs of 
the language and interrogation requisits, XQuery.
4 eXtensible Stylesheet Language (XSL), a language for expressing 
stylesheets.
5 http://www.creativecommons.it/
   
  
  
  
 
The Orationes Project 
 Anthony Johnson
anthony.johnson@oulu.fi 
University of Oulu, Finland
Lisa Lena Opas-Hänninen
lisa.lena.opas-hanninen@oulu.fi 
University of Oulu, Finland
Jyri Vaahtera 
 jyri.vaahtera@utu.fi 
Turku University, Finland 
The Orationes Project is an interdisciplinary initiative intended 
to bring an important unpublished manuscript into the scholarly 
arena. The manuscript, preserved as Lit. MS E41 in the archive 
of Canterbury Cathedral, was assembled by George Lovejoy, 
the Headmaster of the King’s School, Canterbury, after the 
English Civil War. The texts within it represent one of the 
most substantial unpublished sources of English School Drama 
from the period. As well as containing a previously unnoticed 
adaptation of a pre-war play by a major author (James Shirley), 
this large volume, comprising 656 folio pages and running to 
some 230,000 words, includes a number of short plays and 
dramatized orations written in English, Latin and Greek by the 
scholars and staff of the King’s School. Some of these celebrate 
the Restoration of Charles II to power or reconstruct the 
famous ‘Oak-apple’ adventure by which he escaped his 
enemies, during the Civil War, by hiding in an Oak tree. Some 
re-enact the Gunpowder Plot, which nearly destroyed Charles 
II’s grandfather, James I. And others engage with a wide range 
of topical issues, from discussions of religious toleration, or 
the teaching of classics and grammar in the Restoration, to a 
dramatized dialogue between Ben Jonson and Richard Lovelace 
and an alchemical allegory on the politics of state.
In the shorter term, the aim of the project has been to produce 
a pilot study. To this end we have begun by transcribing the 
main texts from the corpus and will produce a series of critical 
studies of them. But due to the extensive size of the manuscript, 
in the longer term, this forms part of a planned Digital Orationes 
Project shared by the English Department at Oulu University, 
the Department of Classical Cultures and Languages at Turku 
University, and the Department of Religious Studies within the 
University of Kent at Canterbury. With the aid of Graduate 
research, these institutions will collaborate in the creation of 
a digital archive which will make these exciting Early Modern 
texts available (in parallel translation) to a wider audience. The 
present poster represents our fi rst collaborative endeavour 
within this digital enterprise.
Our aims are the following: a) to digitize this collection, 
providing images of the manuscript and transcriptions of the 
texts within it. We will also b) provide translations in Modern 
English of the full texts in the manuscript; many of the texts 
characteristically shift language from Latin (or Greek) in mid-
fl ow and thus warrant a Modern English translation. We intend 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
250
c) to interlink the manuscript images with the transcriptions 
and translations. Finally, we intend d) to make the corpus 
freely available on line for scholarly purposes. The texts will 
be marked up in XML and we will follow the TEI Guidelines. 
We intend to make this digital archive searchable and hope to 
make use of tools for the analysis of multimodal data created 
at the Faculty of Engineering, University of Oulu. As part of 
the pilot study mentioned above, we will begin by digitising 
those texts that have been transcribed. This will give us a good 
opportunity to produce a model by which to tackle the rest of 
the texts, since we can link the manuscript, the transcription, a 
translation and a critical study of the text. It will also give us an 
opportunity to explore what kinds of functionality we could 
or should include in the manipulation of the texts. We also 
intend to make use of the experiences of the Scots project in 
the use of the database model to handle the creation process 
of the digital text.
We therefore feel that DH2008 would offer us a good 
opportunity to discuss the project and its practices with 
others who have already carried out similar projects. We will 
also examine the best practices available online within related 
areas – as witnessed, for instance, by the Emily Dickinson, 
Lincoln and Perseus projects (http://www.emilydickinson.org/ 
; http://quod.lib.umich.edu/l/lincoln/; http://www.perseus.tufts.
edu/) – in order to fi nesse our search techniques and improve 
access routes to our manuscript materials.
Within a Finnish context the Orationes project aims to 
respond to Jussi Nuorteva (the director of the National 
Archive Service in Finland), who has criticised the Finnish 
Universities for their reticence in producing open access 
digital databases and archives for the use of other researchers. 
Hence, we plan to open up the Digital Orationes Project after 
2009 on the same model as the Diplomatarium Fennicum 
project – http://193.184.161.234/DF/index.htm – (Nuorteva’s 
own), which makes the Finnish medieval sources available for 
researchers in a digitized form. In this way our project will 
also be forward-looking: helping to position the activities of 
Oulu’s English Department and Turku’s Classics Department 
more fi rmly within the new domain of scholarship which has 
been opened up by digital archiving.
 
  
  
 
 
 
JGAAP3.0 -- Authorship 
Attribution for the Rest of Us 
 Patrick Juola
juola@mathcs.duq.edu
Duquesne University, USA
John Noecker
noeckerj@duq.edu
Duquesne University, USA
Mike Ryan
ryanm1299@duq.edu
Duquesne University, USA
Mengjia Zhao 
zhaom@duq.edu
 Duquesne University , USA
Authorship Attribution (Juola, in press) can be defi ned 
as the inference of the author or her characteristics by 
examining documents produced by that person. It is of course 
fundamental to the humanities; the more we know about a 
person’s writings, the more we know about the person and 
vice versa. It is also a very diffi cult task. Recent advances in 
corpus linguistics have shown that it is possible to do this task 
automatically by computational statistics.
Unfortunately, the statistics necessary for performing this task 
can be onerous and mathematically formidable. For example, a 
commonly used analysis method, Principle Component Analysis 
(PCA), requires the calculation of “the eigenvectors of the 
covariance matrix with the largest eigenvalues,” a phrase not 
easily distinguishable from Star Trek technobabble. In previous 
work (Juola, 2004; Juola et al., 2006) we have proposed a model 
and a software system to hide much of the details from the 
non-specialists, while specifi cally being modularly adaptable 
to incorporate new methods and technical improvements. 
This system uses a three-phase framework to canonicize 
documents, create an event set, and then apply inferential 
statistics to determine the most likely author. Because of the 
modular nature of the system, it is relatively easy to add new 
components.
We now report (and demonstrate) the recent improvements. 
Version 3.0 of the JGAAP (Java Graphical Authorship Attribution 
Program) system incorporates over fi fty different methods 
with a GUI allowing easy user selection of the appropriate 
ones to use. Included are some of the more popular and/or 
well-performing methods such as Burrows’ function word 
PCA (Burrows, 1989), Burrows’ Delta (2003; Hoover 2004a, 
2004b)), Juola’s cross-entropy (2004), and Linear Discriminant 
Analysis (Baayen, 2002). The user is also capable of mixing and 
matching components to produce new methods of analysis; for 
example, applying PCA (following Burrows), but to an entirely 
different set of words, such as all the adjectives in a document 
as opposed to all the function words.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
251
With the current user interface, each of these phases is 
independently selectable by the user via a set of tabbed radio 
buttons. The user fi rst defi nes the document set of interest, 
then selects any necessary canonicization and pre-processing, 
such as case neutralization and/or stripping HTML markup 
from the documents. The user then selects a particular event 
set, such as characters, words, character or word N-grams, the 
K most common words/characters in the document, part of 
speech tags, or even simple word/sentence lengths. Finally, the 
user selects an analysis method such as PCA, LDA, histogram 
distance using a variety of metrics, or cross-entropy.
More importantly, the JGAAP framework can hide this 
complexity from the user; users can select “standard” analysis 
methods (such as “PCA on function words”) from a set of 
menus, without needing to concern themselves with the 
operational details and parameters. Most importantly of all, 
the framework remains modular and easily modifi ed; adding 
new modules, event models, and analytic methods can be done 
in minutes by Java programmers of only moderate skill. We will 
demonstrate this by adding new capacity on the fl y.
Perhaps most importantly, we submit that the software 
has achieved a level of functionality and stability suffi cient 
to make it useful to interested non-specialists. Like the 
Delta spreadsheet (Hoover, 2005), JGAAP provides general 
support for authorship attribution. It goes beyond the Delta 
spreadsheet in the variety of methods it provides. It has also 
been tested (using the University of Madison NMI Build-and-
Test suite) and operates successfully on a very wide range 
of platforms. By incorporating many cooperational methods, 
it also encourages the use of multiple methods, a technique 
(often called “mixture of experts”) that has been shown to 
be more accurate than reliance on any single technique (Juola, 
2008).
Of course, the software is not complete and we hope to 
demonstrate some of its weaknesses as well. The user interface 
is not as clear or intuitive as we hope eventually to achieve, 
and we invite suggestions and comments for improvement. As 
the name suggested, the software is written in Java, and while 
Java programs are not as slow as is sometimes believed, the 
program is nevertheless not speed-optimized and can take a 
long time to perform its analysis. Analysis of large documents 
(novels or multiple novels) can exhaust the computer’s 
memory. Finally, no authorship attribution program can be 
a complete survey of the proposed literature, and we invite 
suggestions about additional methods to incorporate.
Despite these weaknesses, we nevertheless feel that the 
new version of JGAAP is a useful and reliable tool, that the 
community at large can benefi t from its use, and that the 
development of this tool can similarly benefi t from community 
feedback.
References
Baayen, Harald et al. (2002). “An experiment in authorship 
attribution.” Proceedings of JADT 2002.
Burrows, John F. (1989). “`An Ocean where each Kind...’ : 
Statistical Analysis and Some Major Determinants of Literary 
Style.” Computers and the Humanities, 23:309-21
Burrows, John F. (2002). “Delta : A Measure of Stylistic 
Difference and a Guide to Likely Authorship.” Literary and 
Linguistic Computing 17:267-87
Hoover, David L. (2004a). “Testing Burrows’s Delta.” Literary 
and Linguistic Computing, 19:453-75.
Hoover, David L. (2004b). “Delta Prime?” Literary and Linguistic 
Computing, 19:477-95.
Hoover, David L. (2005) “The Delta Spreadsheet.” ACH/ALLC 
2005 Conference Abstracts. Victoria: University of Victoria 
Humanities Computing and Media Centre p. 85-86.
Juola, Patrick. (2004). “On Composership Attribution.” ALLC/
ACH 2004 Conference Abstracts. Gothenburg: University of 
Gothenburg.
Juola, Patrick. (2008). “Authorship Attribution : What Mixture-
of-Experts Says We Don’t Yet Know.” Presented at American 
Association of Corpus Linguistics 2008.
Juola, Patrick. (in press). Authorship Attribution. Delft:NOW 
Publishing.
Juola, Patrick, John Sofko, and Patrick Brennan. (2006). “A 
Prototype for Authorship Attribution Studies.” Literary and 
Linguistic Computing 21:169-78
 
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
252
Translation Studies and 
XML: Biblical Translations in 
Byzantine Judaism, a Case 
Study
 Julia G. Krivoruchko
jgk25@cam.ac.uk
Cambridge University, UK
Eleonora Litta Modignani Picozzi
eleonora.litta@kcl.ac.uk
King’s College London, UK
Elena Pierazzo 
elena.pierazzo@kcl.ac.uk
King’s College London, UK
All defi nitions of translation describe this process as involving 
two or more texts running in parallel that are considered to 
be, in a sense, equivalent to each other. When producing a 
translation, a source text is divided into syntactic units and each 
of them is then translated. The translation can be either literal, 
i.e. it mirrors the structure of the original text very closely, or 
free, i.e. it ignores the original structure and translates freely. 
Because languages diverge greatly in their syntax, the structure 
of a language A can not be fully mapped on a language B, since 
the outcome in B may be incomprehensible. Besides, cultures 
differ greatly as to the degree of freedom/literalness tolerated 
in translation.
In dealing with translations compiled in antiquity and the 
Middle Ages, we are in a sense trying to discover how a specifi c 
culture understood the source text, the grammar of the source 
language and the usability of the translation product. 
Even though a number of XML-based projects involving 
translated texts have been to date proposed to the attention 
of the community,1 a model able to describe the precise 
relationship between source and target texts is still required. 
Such issues have been dealt with at the Centre for Computing 
in the Humanities (King’s College, London) in relation to a 
research project involving Biblical translations. The analysis 
process resulted in an encoding model that could solve 
problems specifi cally linked to this project. Yet the model has 
the potential to be generalized and adapted to work in other 
translation-based projects. 
The Biblical translations are by far the most studied in the 
world. The text of the Hebrew Bible used in Hellenistic period 
was written down in a purely consonantal script without 
vowels, which left a large margin for differing interpretations. 
In addition to this, the Hebrew manuscript tradition was far 
from being homogenous. As a result, a number of translations 
of the Hebrew Bible into Greek emerged, some of them 
differing substantially from each other. 
Until recently, the assumption was that Jews abandoned 
their use of Greek Biblical translations, since these were 
adopted by the Church. In particular, they were supposed to 
ignore the Septuagint, which was recognized as a canonical 
and authoritative text by Eastern Churches. However, 
the manuscripts found in Cairo Genizah have shaken this 
assumption. During the 20th century, new Biblical translations 
made by Jews into Greek during Ottoman and Modern period 
were discovered.2
The Greek Bible in Byzantine Judaism (GBBJ) Project3 aims to 
gather textual evidence for the use of Greek Bible translations 
by Jews in the Middle Ages and to produce a corpus of such 
translations. Unfortunately, the majority of GBBJ evidence 
consists not of continuous texts, but of anonymous glossaries 
or single glosses mentioned by medieval commentators.4  The 
functioning of continuous texts at our disposal is also unclear. 
Further challenges arise from the peculiarities of the writing 
system used by the Byzantine translators. Since approximately 
the 7th-8th century AD, Jews stopped using the Greek 
alphabet and switched instead back to the Hebrew one. In 
order to unambiguously represent the Greek phonetics, 
the Hebrew alphabet was often supplied with vowel signs 
and special diacritics. Some manuscripts contain neither or 
use them inconsistently. In order to decode the writing an 
IPA reconstruction is therefore essential. However, Hebrew 
writing occasionally results in better refl ecting the current 
medieval pronunciation of the Greek language. For what 
the linguistic structure is concerned, while in general Greek 
Jewish Biblical translations use the grammar and lexicon of 
the mainstream Greek, in some cases the translators invent 
lexical items and employ unusual forms and constructions, 
trying to calque the maximal number of grammatical features 
from one language into another. Few of the resulting forms are 
diffi cult or even impossible to understand without knowing 
the Hebrew source. To trace the features transferred and the 
consistency of transferring, the tagging of features is necessary. 
Therefore, lemmatization and POS-tagging both of the source 
and the target texts constitute an essential component for the 
research project.
The two main outcomes of the project will be a printed and a 
digital edition; the latter will allow users to query and browse 
the corpus displayed in parallel verses from the source and 
the target texts. The target will be readable in two alphabets: 
Hebrew and transliterated Greek.
In designing the encoding model, we have tried to follow TEI 
standards as much as possible, e.g. elements for the description 
of metadata, editorial features and transcription of primary 
fonts have been devised according to P5 guidelines since the 
beginning. Yet for what the textual architecture is concerned, 
TEI P5 does not include a model that would fi t the project’s 
needs, hence we have chosen to start working on the encoding 
model on the basis of a custom made DTD rather than a TEI 
compliant schema. 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
253
As a result, the tailored encoding model is simpler to apply for 
the encoders. However, all the elements have been mapped on 
TEI P5 for interchange and processing purposes. 
The encoding model works on three different layers:
a) Structural layer. In any translation study, at least two 
texts need to be paired: the source and the target. The 
GBBJ project focuses on the target text, and therefore it 
constitutes our main document. This text was described 
as consisting of segments, within biblical verses, defi ned 
on palaeographical grounds.  Since the Greek texts we are 
dealing with are written in Hebrew characters, they have 
to be transliterated into Greek characters and normalized 
within what we called the <lexicalTranscription>. The 
translational units are then compared to their presumed 
sources (the Masorethic Text, in our case) in <coupledPairs> 
and fi nally matched with the translations from other Biblical 
traditions (Septuagint and Hexaplaric versions), called 
<equivalents>. The latter are further connected to their 
respective apparatuses. There is a possibility to compare 
between the version of Hebrew lexeme as it appears in the 
manuscript and the Masoretic Text or its apparatus.
<GBBJ>
<metadata></metadata>
<text>
  <verse MT=”Koh 2:19” LXX=”Koh 2:19”>
   <segment type=”translation”>
     <transcription language=”Greek” 
script=”Hebrew”>
        <seg>יִינוֹשְׁקִי</seg>
        <IPA>jinoski</IPA>
     </transcription>
     <lexicalTranscription>
        <GreekWord dictionaryForm=”γινώσκω” 
id=”Koh2-19_3”>
            <verb person=”third” 
number=”singular” tense=”present” 
voice=”active” mood=”indica
tive”>γινώσκει</verb>
        </GreekWord>
     </lexicalTranscription>
     <coupledPair>
        <target><ref intRef=”Koh2-
19_3”/></target>
        <source><ref exRef=”MT_
Qoh2-19_3”/></source>
        <equivalents>
           <LXX exRef=”LXX_Eccl2-19_3”></LXX>
        </equivalents>
      </coupledPair>
   </segment>
  </verse>
</text>
</GBBJ>
In order to keep the document as semantically clean and 
coherent as possible, we have devised a way of externalising 
connected information in several “side” fi les. Each of them 
contains a different Biblical text: the source (MT), the 
Septuagint and Hexaplaric variants. The respective apparatuses 
are encoded in separate documents and connected to the main 
GBBJ document through a link in the <equivalents> element 
within the <coupledPairs> section. Establishing a relationship 
between the GBBJ target text and other Greek translations is 
not only important for diachronic linguistic purposes, but also 
for the study of textual history of the GBBJ translations. 
For these external fi les, we have devised a specifi c but simple 
DTD (based on TEI) which allows the parallel connection with 
the main text.
b) Linguistic layer. In translation studies it is important 
to analyse not only semantic relationship between the 
words, but also their morphological correspondence. 
Lemmatisation and POS-tagging have therefore 
been envisaged for both the GBBJ document (within 
<lexicalTranscription>) and the external fi les, connected 
via IDs. Each segment in the source text can be paired 
both semantically and morphologically with any of its 
counterparts, allowing complex queries and the generation 
of correspondence tables.
c) Editorial and codicological layer. The GBBJ text derives 
directly from a primary source, which means that 
information needs to be given on all editorial elements: 
expansions of abbreviations, integrations, corrections, etc. 
The physical structure of the document was also described 
on a very granular level including column breaks, line breaks, 
folio breaks, marginal notes, change of hands, spaces and 
gaps.
The present case study demonstrates the wide range of possible 
applications of an XML framework to translation studies. The 
Greek Bible in Byzantine Judaism Project presents a number 
of problems that are likely to be encountered in other similar 
projects, such as an alphabet not suited to a specifi c language 
and the existence of wide comparable corpora of translational 
traditions. Although some of the solutions found are specifi c 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
254
to the research project, the approach and the conceptual 
model used here may be reused and adapted within the digital 
humanities community.
Notes
1 For example: The Emblem Project Utrecht (http://emblems.let.
uu.nl/index.html, accessed 23/11/07), the English-Norwegian Parallel 
Corpus (http://www.hf.uio.no/ilos/forskning/forskningsprosjekter/
enpc/, accessed 23/11/07).
2 For general discussion see Fernández Marcos (1998). Outline of the 
problems and suggestions for future research: RASHI 1040-1990. 
3 A three year AHRC project based at University of Cambridge 
(Faculty of Divinity) and King’s College, London (Centre for 
Computing in the Humanities); see project website at http://www.
gbbj.org (accessed 7/3/2008).
4 For glossaries see De Lange (1996); Rueger (1959); Tchernetska, 
Olszowy-Schlanger, et al. (2007). For a continuous text see Hesseling, 
(1901).
References
Burnard, Lou and Bauman, Syd (2007), TEI P5: Guidelines for 
Electronic Text Encoding and Interchange, at http://www.tei-
c.org/release/doc/tei-p5-doc/en/html/index.html (accessed 
23/11/07).
De Lange, N. R. M. (1993). The Jews of Byzantium and the 
Greek Bible. Outline of the problems and suggestions for 
future research. RASHI 1040-1990. Hommage à  Ephraïm E. 
Urbach. ed. G. Sed-Rajna. Paris, Éditions du Cerf: 203-10.
ID. ( 1996). Greek Jewish Texts from the Cairo Genizah, Tübingen: 
Mohr-Siebeck.
Fernández Marcos, Natalio. (1998). Introducción a las 
versiones griegas de la Biblia. Madrid: Consejo Superior de 
Investigaciones Científi cas, ch.;
H. P. (1959). “Vier Aquila-Glossen in einem hebraischen 
Proverben-Fragment aus der Kairo-Geniza.” Zeitschrift für die 
Neutestamentlische Wissenschaft 50: 275-277; 
Hesseling, D. S. (1901). “Le livre de Jonas.” Byzantinische 
Zeitschrift 10: 208-217.
RASHI 1040-1990. Hommage à  Ephraïm E. Urbach. ed. G. Sed-
Rajna. Paris, Éditions du Cerf: 203-10. 
Tchernetska, N., Olszowy-Schlanger J., et al. (2007). “An Early 
Hebrew-Greek Biblical Glossary from the Cairo Genizah.” 
Revue des Études Juives 166(1-2): 91-128. 
 
Multi-Dimensional Markup: 
N-way relations as a 
generalisation over possible 
relations between annotation 
layers
 Harald Lüngen
luengen@uni-giessen.de
Justus-Liebig-Universität, Germany
Andreas Witt
andreas.witt@uni-tuebingen.de
 University of Tübingen, Germany 
Text-technological background
Multi-dimensional markup is a topic often discussed. The main 
reasonwhy it is researched is the fact that the most important 
markup languages today make the implicit assumption that for 
a document, only a single hierarchy of markup elements needs 
to be represented. Within the fi eld of Digital Humanities, 
however, more and more analyses of a text are expressed 
by means of annotations, and as a consequence, segments of 
a text are marked up by tags relating to different levels of 
description. Often, a text is explicitly or implicitly marked up 
several times. When using the TEI P5 as an annotation scheme 
one might use markup from different TEI modules concurrently 
as ‘msdescription’ for manuscript description, ‘textcrit’ for 
Text Criticism, and ‘analysis’ for (linguistic) analysis and 
interpretation, because the Guidelines state that “TEI schema 
may be constructed using any combination of modules” (TEI 
P5 Guidelines). 
Abstracting away from limitations of specifi c markup 
languages, textual regions annotated according to different 
levels of descriptions can stand in various relationsships to 
each other. Durusau & O’Donnell (2002) list 13 possible 
relationsships between two elements A and B used to 
concurrently annotate a text span. Their list comprises the 
cases ‘No overlap’ (independence), ‘Element A shares end 
point with start point of element B or the other way round’, 
‘Classic overlap’, ‘Elements share start point’, ‘Elements share 
end point’ and ‘Element share both their start points and end 
points’. The latter case is known under the label ‘Identity’. The 
possible relationships between A and B can also be partitioned 
differently, e.g. into Identity, Region A before region B, or the 
other way round. Witt (2004) has alternatively grouped the 
relations into three ‘meta-relations’ called ‘Identity’, ‘Inclusion’, 
and ‘Overlap’. Meta-relations are generalisations over all the 
13 basic relations inventorised by Durusau & O’Donnell. 
The reason for introducing meta-relations is to reduce the 
number of relations to be analysed to those cases that are 
most typically needed when querying annotations of multiply 
annotated documents. The query tool described in Witt et 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
255
al. (2005) provides 7 two-way query predicates for the 13 
basic relations from Durusau & O’Donnell (where e.g. the 
two relations overlap(A,B) and overlap(B,A) are handled by 
one query predicate) and specialised predicates for the three 
meta-relations.
As argued above, often n-way relationships between elements 
from three or more annotation layers need to be queried. 
When the detailed accounts of cases of relations between 
two elements described above are extended to cases where 
three or more layers are analysed, the number of possible 
relationships is subject to a combinatorial explosion and 
rises into several hundreds and thousands. Only in the case 
of identity(A,B), additional 13 cases of three-way relationships 
can be distinguished; for all remaining cases of two-way 
relationships, considerably more three-way cases need to be 
distinguished. It seems impossible to invent names, let alone to 
formulate and implement queries for each one of them. Still, for 
a user it would be desirable to have a set of query predicates 
for n-way relations available, lest (s)he needs to repeatedly 
combine queries for two-way relationships, which often can 
be done only with the help of a fully-fl edged programming 
language.
Application: Analysing n-way relations 
in text parsing
One text-technological application where relations between 
elements on more than two elements need to be analysed, 
is discourse parsing of argumentative texts. In a bottom-up 
operating discourse parser such as the one developed for 
German research articles in the SemDok project (Lüngen et al. 
2006), it is checked successively whether a discourse relation 
holds between two known adjacent discourse segments such 
that they can be combined to form a larger segment. Often 
this depends on the presence of a lexical discourse marker, 
such as the adverb ‘lediglich’ (‘only’), in the second segment. 
But with ‘lediglich’ as with numerous other markers, there is 
the additional condition that it has to occur in the so-called 
vorfeld (fi rst topological fi eld of a German sentence according 
to the syntax of German, cf. Hinrichs & Kübler 2006), of the fi rst 
sentence of the second discourse segment. Thus, a combination 
of information from at least three different information levels 
(discourse segments, syntax, and discourse markers) needs to 
be checked, i.e. whether the following situation holds:
L1: <ds>..........................
...........................</ds>
L2: <s><vorfeld>.................
....</vorfeld>.............</s>
L3: <dm>lediglich</dm>
This situation corresponds to a meta-relation of three-way 
inclusion: <ds> from Layer 1 must include a <vorfeld> from 
Layer 2, which in turn must include a <dm> from Layer 3.
Querying n-way relations between 
elements of multiple annotations 
We have identifi ed a set of n-way meta-relations that are typically 
needed in text-technological applications for multiply annotated 
documents, namely N-way independence, N-way identity, 
N-way inclusion, and N-way overlap, (where independence, 
identity, and inclusion hold between the elements from all 
n layers, and overlap holds between at least one pair among 
the n elements). The proposed poster presentation illustrates 
further examples from text-technological applications such as 
discourse analysis and corpus linguistic studies, where querying 
n-way relations between elements is required. It explains 
our set of query predicates that have been implemented in 
Prolog for n-way meta-relations, and how they are applied to 
the examples. Furthermore it presents an evaluation of their 
usability and computational performance.
References
Durusau, Patrick and Matthew Brook O’Donnell (2002). 
Concurrent Markup for XML Documents. XML Europe 2002.
Hinrichs, Erhard and Sandra Kübler (2006). What Linguists 
Always Wanted to Know About German and Did not Know 
How to Estimate. In Mickael Suominen, Antti Arppe, Anu 
Airola, Orvokki Heinämäki, Matti Miestamo, Urho Määttä, 
Jussi Niemi, Kari K. Pitkänen and Kaius Sinnemäki (eds.): A 
Man of Measure : Festschrift in Honour of Fred Karlsson on his 
60th Birthday. The Linguistic Association of Finland, Special 
Supplement to SKY Journal of Linguistics 19. Turku, Finland.
Lüngen, Harald, Henning Lobin, Maja Bärenfänger, Mirco 
Hilbert and Csilla Puskas (2006). Text parsing of a complex 
genre. In Bob Martens and Milena Dobreva (eds.): Proceedings 
of the Conference on Electronic Publishing (ELPUB 2006). Bansko, 
Bulgaria.
Witt, Andreas (2004). Multiple hierarchies: New aspects of 
an old solution. In Proceedings of Extreme Markup Languages. 
Montreal, Canada.
Witt, Andreas, Harald Lüngen, Daniela Goecke and Felix 
Sasaki (2005). Unifi cation of XML Documents with 
Concurrent Markup. Literary and Linguistic Computing 20(1), S. 
103-116. Oxford, UK. 
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
256
 Bibliotheca Morimundi. 
Virtual reconstitution of the 
manuscript library of the 
abbey of Morimondo 
 Ernesto Mainoldi
ernesto.mainoldi@libero.it
 University of Salerno, Italy 
Bibliotheca Morimundi has been fi nanced by CNR (Consiglio 
Nazionale delle Ricerche, Rome) in the context of the 
general national project L’identità culturale come fattore 
di integrazione (http://www.cnr.it/sitocnr/IlCNR/Attivita/
PromozioneRicerca/Identitaculturale.html), CNR, Rome, 
2005-2006.
http://www.cnr.it/sitocnr/IlCNR/Attivita/
PromozioneRicerca/IdentitaCulturale_fi le/
IdentitaculturaleGraduatoriaPG.html
Presentation of the plan of search and 
its object
The aim of the project Bibliotheca Morimundi is to recompose 
in a database of digital images the written heritage that 
belonged to the library of the Cistercian abbey of Morimondo 
(Milan), dispersed after the suppression of the monastery in 
1796. The collection was composed by medieval manuscripts, 
renaissance incunabula and diplomas. According to medieval 
catalogues and to ex libris about 80 extant manuscripts 
(dating from the middle of 12th century to the 15th century), 
3 incunabula and hundred of medieval and post-medieval 
diplomas belonged to the abbey of Morimondo. Morimondo 
manuscripts are generally characterised by a remarkable 
thematic range and relevant artistic preciousness: Bibles and 
liturgical books decorated with fi ne miniatures, books with 
musical notation (among which is the most ancient known 
antiphonary of Cistercian order), patristic texts that show an 
interest for authors from the Eastern Christianity, and fi nally 
juridical, scientifi c and theological codices that are witnesses 
of the participation of the Morimondo monastery to the 
European renewal of the twelfth and thirteenth century. Some 
of the diplomas are of high historical importance, as papal and 
imperial documents, and are now conserved at State Archive 
of Milan.
Since manuscripts from Morimondo are now conserved in ca. 
twenty libraries dislocated all around the world, a comparative 
and detailed study of all these sources has not yet been possible: 
the digital resource will allow a global study of this collection, 
based on the comparison of handwritten texts (manuscripts 
and diplomas) produced in the very same scriptorium. On the 
basis of these features the Bibliotheca Morimundi project will 
be a digital library that differentiates itself from other digital 
libraries, which build on an extant collection (see the Sankt 
Gallen or Köln projects: http://www.cesg.unifr.ch/it/index.htm; 
http://www.ceec.uni-koeln.de/). In the case of Morimondo the 
digital library takes the place of the no more extant library.
The virtual recollection of the manuscripts once belonged 
to Morimondo abbey in a digital database should allow a 
systematic study of the history and the culture developed in 
this monastic site, a singular oasis in the Ticino’s valley, at 40 km 
from Milan, where religious life and textual production where 
cultivated together. Throughout the possibility to compare 
these manuscripts, of the majority of which were written in 
Morimondo, scholars would have the opportunity to detect 
palaeographical, codicological, liturgical and textual features of 
the monastic culture in Morimondo.
The realization of the database Bibliotheca Morimundi not only 
will reconstitute the ancient unity of the library of the abbey, 
but is also intended as an interactive instrument of research, 
by providing a wealth of resources including digital images of 
the manuscripts, incunabula and diplomas ,texts transcriptions, 
indices, and bibliography, organized as an hypertext structure 
capable to facilitate the research and to integrate progressing 
results and dissemination of the studies (such as printed papers 
but also notes made by users).
The Database
The database is accessed as a web site structure in html 
language. The consultation of the digitals images, due to 
copyright reasons, is allowed – at least in this fi rst phase of the 
realization – only off line, on local computer terminals hosted 
by the institutions which support the project (Fondazione 
Franceschini of Florence and Fondazione Rusca of Como). 
Catalogues and textual resources will be published on line. 
When a library owner of manuscripts from Morimondo didn’t 
allow a digital reproduction a digitized microfi lm is inserted 
in the database. The pictures, provided as 8 bit colour JPEG 
at the resolution of 300 dpi, are readable in PDF format. The 
metadata will be codifi ed in XML language following the MAG 
standard, http://www.iccu.sbn.it/genera.jsp?id=267, which 
scheme consent to mark either the digital images or the OCR 
fi les of secondary bibliography and other textual resources 
(such transcription, notes progressively added by users etc.).
Status of the work
At present about 30 of 78 mss. have been photographed. 
The most important collection of Morimondo manuscripts 
(summing 18 mss.), owned by the Biblioteca del Seminario 
Vescovile di Como, has been fully photographed in digital 
colour pictures and constituted the current base of the archive 
of digital photos. Others single codices kept in monastic 
library (such as Tamié in France or Horsham in England) have 
been digitally photographed. Digital images amount, up to now, 
at the number of 7000 ca. Others main collections, such as 
those kept in the Bibliothèque Nationale de France in Paris 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
257
or in the British Library in London, have been acquired as b/w 
microfi lms and converted in digital images with a slide scanner. 
In a draft version the database is already consultable at the 
library of the Fondazione Rusca of Como, where also the mss. 
of the Biblioteca del Seminario are conserved. Some scholars 
have already begun their studies by using the data (images and 
texts) and metadata (codicological data about the manuscript 
source, digital image informations) already inserted in the 
database.
Developments of the search
A fundamental direction of development of the «Bibliotheca 
Morimundi» project will the multidisciplinary integration 
of differentiated scientifi c competences in the study of the 
sources included in the database. Already a team of scholars 
with academic roles has been contacted or involved in 
the project. The integration of digitals images and textual 
instruments that will progressively facilitates the study of 
the Morimondo primary sources will also constitute a fi eld 
of application for developers of computational resources in 
the humanities. Another development will be the incentive 
toward the interrelation of several type of cultural institutional 
needs, such the needs of research and the needs of historical 
preservation and divulgation. The project has already made 
progress on the basis of such a cooperation between an 
institution involved in active scientifi c research (Fondazione 
Franceschini – SISMEL, one of the most active institution, 
at international level, in the study of the Middle Ages) and a 
library (Biblioteca del Seminario vescovile di Como, keeper 
of 18 manuscripts from Morimondo). Other conventions for 
scientifi c cooperation between institutions are currently in 
progress.
Bibliography
Correlated Projects
Codices Electronici Ecclesiae Coloniensis (CEEC) 
http://www.ceec.uni-koeln.de/
Codices Electronici Sangallenses (CESG) – Virtual Library 
http://www.cesg.unifr.ch/it/index.htm
Digital Image Archive of Medieval Music 
http://www.diamm.ac.uk/
Manuscriptorium 
http://www.manuscriptorium.com/Site/ENG/default_eng.asp
Monasterium Projekt (MOM)
www.monasterium.net
Papers
Michele Ansani, Edizione digitale di fonti diplomatiche: 
esperienze, modelli testuali, priorità, «Reti medievali VII- 2006 
/ 2 - luglio-dicembre», http://www.dssg.unifi .it/_RM/rivista/
forum/Ansani.htm
Arianna Ciula, L’applicazione del software SPI ai codici senesi 
in Poesía medieval (Historia literaria y transmisión de textos) 
cur. Vitalino Valcárcel Martínez - Carlos Pérez González, 
Burgos, Fundación Instituto Castellano y Leonés de la Lengua 
2005 pp. 483 (Beltenebros 12), pp.?305-25
Emiliano Degl’Innocenti, Il Progetto di digitalizzazione 
dei Plutei della Biblioteca Medicea Laurenziana di Firenze 
«DigItalia». Rivista del digitale nei beni culturali, Roma = 
DigItalia 1 (2007), pp. 103-14
Digitalisierte Vergangenheit. Datenbanken und Multimedia 
von der Antike bis zur frühen Neuzeit, cur. Christoph Schäfer 
- Florian Krüpe, Wiesbaden, Harrassowitz 2005 pp. XI-147 
tavv. (Philippika. Marburger altertumskundliche Abhandlungen 
5) 
  
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
258
Investigating word co-
occurrence selection with 
extracted sub-networks of 
the Gospels
 Maki Miyake 
mmiyake@lang.osaka-u.ac.jp
 Osaka University, Japan
 Graph representation and the techniques of graph theory 
and network analysis offer very effective ways of detecting 
and investigating the intricate patterns of connectivity that 
exist within large-scale linguistic knowledge resources.  In this 
presentation, we apply a graph-clustering technique for data 
processing that utilizes a clustering-coeffi cient threshold in 
creating sub-networks for the Gospels of the New Testament. 
Specially, this study discusses some graph clustering results 
from the perspectives of optimal clustering and data sizes 
with a view to constructing an optimal semantic network 
by employing the hierarchical graph clustering algorithm of 
Recurrent Markov Clustering (RMCL).  The corpus used in 
this study is a Greek version of the Gospels (Nestle-Aland, 
1979) for which two semantic networks are created based 
on network features.  The network structures are investigated 
from the perspective of constructing appropriate semantic 
networks that capture the relationships between words and 
concepts.
In the natural language processing of texts, word pairs are 
usually computed by the windowing method (Takayama, 
Flournoy, Kaufmann, & Peters, 1998).  The windowing method 
provides a relatively simple representation of similarity levels 
that is suitable for clustering.  The technique involves moving 
a certain sized window over a text to extract all fi xed-
sized word grams (Vechthomova, Roberston, & Jones, 2003). 
Word pairings are then made by combining all extracted 
words. In the present study, co-occurrence data is computed 
with two window sizes that refl ect syntactic and semantic 
considerations. The fi rst size (WS1) is set at 1 for the nearest 
co-occurrence, while the second size (WS10) is set to 10 to 
collect words and no stemming process was taken. Accordingly, 
8,361 word occurrences were identifi ed.  An adjacency matrix 
for the graphs was created from the word co-occurrence 
data for a particular range of the texts.  Using the clustering 
coeffi cient as a threshold, the network was reduced into 18 
sub-networks which were created at 0.1 increments of the 
clustering coeffi cient value (from 0.1 to 0.9).
In Figure 1, the degree distributions for the two networks for 
all occurrence nodes are plotted along log-log coordinates. 
The window size of 1 shows the best fi t to a power law 
distribution (r=1.7).  The average degree value of 15.5 (2%) for 
the complete semantic network of 8,361 nodes clearly indicates 
that the network exhibits a pattern of sparse connectivity; in 
other words, that it possesses the characteristics of a scale-
free network according to Barabasi and Albert (1999).  In 
contrast, the window size of 10 has a bell-shaped distribution 
and its average degree value of 106.4 (13%) indicates that 
the network exhibits a much greater level of connectivity 
compared to the window size of 1.
 
Figure.1 Degree distributions
The second network feature is the clustering coeffi cient which 
represents the inter-connective strength between neighboring 
nodes in a graph.  Following Watts and Strogatz (1998), the 
clustering coeffi cient of the node (n) can be defi ned as: 
(number of links among n’s neighbors)/(N(n)*(N)-1)/2), where 
N(n) denotes the set of n’s neighbors. The coeffi cient assumes 
values between 0 and 1.
Moreover, Ravasz and Barabasi (2003) advocate a similar notion 
of clustering coeffi cient dependence on node degree, based on 
a hierarchical model of scaling laws.  The results of scaling C(k) 
with k for the two networks are presented in Figure 2.  As the 
two networks conform well to a power law, we can conclude 
that they both possess intrinsic hierarchies.
Figure.2 Clustering coeffi cient distributions
The graph clustering method applied in this study is Recurrent 
Markov Clustering (RMCL), recently proposed by Jung, Miyake, 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
259
and Akama (2006), as an improvement to Van Dongen’s (2000) 
Markov Clustering (MCL) which is widely recognized as an 
effective method for the detection of patterns and clusters 
within large and sparsely connected data structures (Dorow, 
Widdows, Ling, Eckmann, Sergi & Moses (2005), Steyvers & 
Tenenbaum (2005)).  The fi rst step in the MCL consists 
of sustaining a random walk across a graph.  The recurrent 
process is essentially achieved by incorporating feedback data 
about the states of overlapping clusters prior to the fi nal MCL 
output stage.  The reverse tracing procedure is a key feature of 
RMCL making it possible to generate a virtual adjacency matrix 
for non-overlapping clusters based on the convergent state 
yielded by the MCL process.  The resultant condensed matrix 
represents a simpler graph that can highlight the conceptual 
structures that underlie similar words.
Figure 3 presents cluster sizes as a function of the clustering 
coeffi cient for the two window sizes of 1 and 10. The terms 
for the input data (WS1-data, WS10-data) refer to the two 
initial adjacency matrices.  Focusing on the trend in cluster 
sizes for the window size of 10 (WS10), the fact that cluster 
sizes remain relatively constant regardless of size of the data 
indicates that as the window size increases, clusters are less 
dependent on the clustering coeffi cient.
Figure.3 MCL results
In order to optimize the co-occurrence data, we employ 
Newman and Girvan’s (2004) notion of modularity, which is 
particularly useful in assessing the quality of divisions within 
a network.  Modularity Q indicates differences in edge 
distributions between a graph of meaningful partitions and a 
random graph under the same vertices conditions (numbers 
and sum of their degrees).  Accordingly, Miyake and Joyce 
(2007) have proposed the combination of the RMCL clustering 
algorithm and this modularity index in order to optimize the 
infl ation parameter within the clustering stages of the RMCL 
process.
Moreover, to consider the recall rates of nodes, the F measure 
is also employed to optimize the selection of the most 
appropriate co-occurrence data, because the precision rate, P, 
always depends on a trade-off relationship between modularity 
Q and the recall rate R.
Figure 4 plots modularity Q and the F measure as a function 
of clustering coeffi cients for the two window sizes of 1 and 
10.  The results indicate that there are no peaks in the plot of 
Q values.  This fi nding suggests that as the value of r decreases, 
the value of Q increases.  In the case of WS1, there are 
differences in the peaks for the two measures of Q and F, for 
while Q peaks at 0.6, the value of F peaks at 0.4. In this study, 
we regard the peak in the F measure as a recall rate according 
to the size of the data.  As the results for WS10 show no peaks 
in the Q value, we also take the peak value for F which is 0.7. 
In this way, the number of node is about 5,000 for the two 
selected sub-networks, which are almost the same size as the 
networks.
Figure 4.
In conclusion, two network representations of the Gospels 
were created for different window sizes of text, and the 
networks were analyzed for the basic features of degree and 
clustering coeffi cient distributions in order to investigate the 
presence of scale-free patterns of connectivity and hierarchical 
structures.  This study also reported on the application of 
a hierarchical graph clustering technique to sub-networks 
created with different clustering-coeffi cient thresholds.  In 
terms of constructing an optimal semantic network, the 
combination of the modularity measurement and the F 
measure is demonstrated to be effective in controlling the 
sizes of clusters.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
260
Bibliography
Barabási, A.L., and R. Albert (1999), Emergence of Scaling in 
Random Networks. Science, 86, pp.509-512. 
Dorow, B., D. Widdows, K. Ling, J. Eckmann, D. Sergi, and 
E. Moses (2005), Using Curvature and Markov Clustering 
in Graphs for Lexical Acquisition and Word Sense 
Discrimination. MEANING-2005, 2nd Workshop organized by 
the MEANING Project, February 3rd-4th 2005, Trento, Italy. 
Trento, Italy. 
Jung, J., Miyake, M., Akama, H.(2006), Recurrent Markov 
Cluster (RMCL) Algorithm for the Refi nement of the 
Semantic Network, In Proceedings of the 5th International 
Conference on Language Resources and Evaluation (LREC2006), 
pp.1428-1432.
Nestle-Aland (1987), Novum Testmentum Graece. 26th 
edition. Stuttgart: German Bible Society, 1987.
Miyake, M., Joyce, T (2007), Mapping out a Semantic Network 
of Japanese Word Associations through a Combination of 
Recurrent Markov Clustering and Modularity. In Proceedings 
of the 3rd Language & Technology Conference (L&TC’07), 
pp.114-118.
Steyvers, M., and J. B. Tenenbaum (2005), The Large-Scale 
Structure of Semantic Networks: Statistical Analyses and a 
Model of Semantic Growth. Cognitive Science 29.1, pp.41-78.
Takayama, Y., R. Flournoy, S. Kaufmann, and S. Peters. 
Information Mapping: Concept-based Information Retrieval 
Based on Word Associations. http://www-csli.stanford.edu/
semlab-hold/infomap/theory.html, 1998.
van Dongen, Stijn Marinus (2000), “Graph Clustering by Flow 
Simulation”. PhD Thesis. University of Utrecht. http://igitur-
archive.library.uu.nl/dissertations/1895620/inhoud.htm
Vechthomova, O., S. Robertson, and S. Jones (2003), Query 
Expansion With Long-Span Collocate. Information Retrieval 6, 
pp.251-273.
Watts, D., and S. Strogatz (1998), Collective Dynamics of 
‘Small-World’ Networks. Nature 393, pp.440-442.
 
 
 
 
  
 
Literary Space: A New 
Integrated Database System 
for Humanities Research 
 Kiyoko Myojo
kiyokomyojo@gmail.com
Saitama University, Japan
Shin’ichiro Sugo 
shinsugouakita@mac.com
 Independent System Engineer, Japan, 
We describe here a basic design of a multi-purpose database 
system for Humanities research that we are developing. The 
system named “LiterarySpace” provides a new integrated 
electronic workbench for carrying out various kinds of 
processing acts on textual study: documentation, transcription, 
interpretation, representation, publication and so on 
(Shillingsburg 2006). In other words, it has a polar range of 
possibilities of functions from a private tool to a publishing 
medium: image data of any materials such as inscribed stones, 
wooden tablets, papyrus rolls, paper codex etc. can be stored 
for private and/or for public use, and recorded text data 
such as the results of electronically scholarly editing and/or 
outcomes of hermeneutic inquiry with/without relation to the 
graphics, and then represent all or part of the data archives to 
yourself in a stand-alone computer and/or to anyone via the 
internet.
The essential architecture of this relational database is simple: 
It is constructed from three database fi les: fi le-1, graphic data 
fi le = GDF; fi le-2, text data fi le = TDF; fi le-3, control data fi le = 
CDF. GDF has two data fi elds in each of its records: the one for 
storing image data and the other for its unique serial numbers 
the system automatically issues. We named this identifi cation 
number IDGraphic. A record in TDF consists of three data 
fi elds which keep three kinds of data: 1) text data, 2) its unique 
identifi cation integers assigned by the system which we named 
IDText, 3) IDGraphic as a relational key to GDF. In terms of 
the relation between GDF and TDF, one distinguishing feature 
is that one GDF record allows an overlapped link to plural 
TDF records; namely the correspondence of records in GDF 
to ones in TDF is one-to-many and not vice versa. A CDF 
record contains the following two fi elds: 1) IDSet fi eld in 
which IDGraphic or IDText is registered as a relational key, 
and 2)Index fi eld which keeps numbers that have been input 
in order to make the sequence of the records called out by 
the IDSet. As for the identifi cation of the type of IDSet, with 
just the two alternatives, IDGraphic or IDText, the name of 
the CDF serves as a mark. If the fi lename given by the user 
contains the word “GDF” or “gdf” , the fi le is interpreted as a 
set of records from GDF. Otherwise, it is being from TDF.
The powerfulness of “LiterarySpace” is caused from this 
simple structure which corresponds to the theoretical model 
we made up based on our clear understanding about space 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
261
and time in writing (Myojo & Uchiki 2006). Concerning space, 
written existence demands two different spaces: physical 
space which every written text occupies, and conceptual 
space which the text generates in the minds of writers and 
readers at the same time (Bolter 1991). On the other hand, 
time is time; time is never physical but ever visionary and 
such time in a written text can be controlled by the writer 
and even the reader. The three database fi les of our system 
coincide with these epistemological dimensions: GDF with 
the physical space, TDF with the conceptual space, and CDF 
with the variable time, or a little more precisely, fi ctional 
spaces in which each conducted time goes (Stanzel 1989). 
Within the one space of the one “LiterarySpace” there can 
exist only one GDF, one TDF, and plural CDF(s). The space of 
the single GDF is constituted of image data as non-character-
coded documentation of extant materials. And the time of 
its space is represented by the sequential order indicated 
by integers assigned mechanically for each additional record. 
Needless to say, the order corresponds to the succession in 
which the records are input. The one-time-only character of 
the inputting acts in reality is the reason why the integer, i.e. 
IDGraphic is unique. So you might call the time generated by 
the unique numbers the “real” one. The single TDF is made 
up from text data as character-coded documentation of any 
writing, that is to say, the products of conceptual work; and the 
space has also the single “real” time represented in the order 
designated by the IDText integers, which naturally corresponds 
to the succession in which the data are put. The philosophical 
substance of CDF(s) is, however, different; its space is a meta-
space which consists of data records chosen from GDF or 
TDF. As the space is, so to speak, fi ctional, it is possible to be 
set in imaginary time. The meta-time can be produced by the 
sequence of the Index-numbers issued not by machine but by 
the user. Because of the fi ctionality, one can make more than 
one sequence if necessary. So, unlike other types of fi eld, the 
Index-fi eld can be increased. To be sure, there also exists “real” 
time in accordance with the succession of the records input 
into the CDF and if no sequence is created, the records would 
be represented in the input order.
The fi rst part of our presentation will illustrate this system 
design more in detail, explaining signifi cant features including 
e.g. the reason of the above mentioned one-to-many 
correspondence of GDF to TDF in connection with the 
theoretical modelling (McCarty 2005). 
In the second part we will demonstrate a prototype of the 
system which is built using the database application “FileMaker 
Pro” (After the specifi cation of the software has been 
completed, its program will be rewritten in Common Lisp). 
The current main database in the prototype is Franz Kafka’s 
database constituted of information particularly from his 
“Oktavhefte”. One of us, Myojo is originally a Kafka researcher 
and the primal need of this system arose in the context of 
her literary research project (Myojo 2002). Myojo’s study 
following her own methodology as a combination between 
“Editionswissenschaft” and “critique géntique” has always 
demanded a cross-referenced complicated work handling 
three editions simultaneously: the practical edition by Max 
Brod, the critical edition “Kritische Kafka-Ausgabe”, and the 
facsimile edition “Franz Kafka-Ausgabe”. So we should admit 
that the fi rst attempt of our modelling was not theoretical 
but practical (Myojo 2004). It is, however, because our project 
started to grasp the actual needs of the one scholar in the 
Humanities that this system has grown up as a real useful 
entity. Using the Kafka database as an example, we would like 
to demonstrate not only how the system operates but also 
how effi ciently the performance assists the investigation for 
work in the humanities.
The third part will be dedicated to showing the other feasibilities 
of this system. One of the many defi ning characteristics of 
this system is that it allows us to contain several databases 
at the same time. For instance in addition to Kafka’s database 
one can also construct Shakespeare’s as well as the work 
of some Japanese authors’ etc. in the same system. Also, a 
more signifi cant point is that one can input as much data as 
necessary without worrying about grouping or sequences. 
Simply put, one can make a record of Kafka’s information just 
after inputting that of Shakespeare’s. This unrestrictedness is 
enabled by the aforementioned epistemological data structure 
because the grouping and the sequence can be controlled 
afterwards by means of creating a meta-area, i.e. a CDF. 
Presently in our prototype two more databases of Japanese 
authors are under construction: Kenji Miyazawa (宮沢賢治)’s 
and Soseki Natsume (夏目漱石)’s. The strong merits of this 
capability will be presented as an exhibition of research results 
in the fi eld of comparative literature (Myojo 2003). 
One more important point we have to mention: as we noted 
at the beginning of this text, the system serves as a private tool 
and/or a publishing medium. The signifi cance of this feature 
would be recognized well if you imagined the case of dealing 
with works protected by copyright. Of course the dual aspects 
of privateness and publicness correspond to the original 
distinguishing character of the computer itself. From not only 
this but the all above points of view it might be suggested that 
this system could become a powerful electronic substitute 
for a physical notebook. In the last part of the presentation 
we would like to discuss the multifaceted possibilities, i.e. the 
universality of the system.
Acknowledgments
This work was funded in part by The Japan Society for the 
Promotion of Science (JSPS) under Grant-in-Aid for Scientifi c 
Research (C) [18529001]. We would also like to thank 
Christian Wittern (Kyoto University), Tetsuya Uchiki (Saitama 
University) and other members of The Japanese Association 
for Scholarly Editing in the Digital Age (JASEDA) for their help 
in establishing regular and fruitful discussions.             
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
262
Bibliography
Bolter, David J., Writing Space, Hillsdale (Lawrence Erlbaum 
Associates) 1991. 
McCarty, Willard, Humanities Computing, Houndmills (Palgrave 
McMillan), 2005.
Myojo, Kiyoko, Kafka Revisited, Tokyo (Keio University Press), 
2002 [in Japanese].              
Myojo, Kiyoko, “Kafka und sein japanischer Zeitgenosse 
Kenji”, Saitama University Review 39.2 (2003): 215-225.  
Myojo, Kiyoko, “What is the Future of Computer-Assisted 
Scholarly Editing?” IPSJ SIG Technical Reports CH-62 (2004): 
37-44 [in Japanese].
Myojo, Kiyoko & Tetsuya Uchiki: “A Theoretical Study on 
Digital-Archiving of Literary Resources”, IPSJ Symposium 
Series 17 (2006): 153-160 [in Japanese]. 
Shillingsburg, Peter L., From Gutenberg to Google, Cambridge 
(Cambridge University Press) 2006.
Stanzel, Franz K., Theorie des Erzählens, Göttingen 
(Vandenhoec & Ruprecht), 1989 [1979].
 
  
  
  
 
 
A Collaboration System for 
the Philology of the Buddhist 
Study 
 Kiyonori Nagasaki
nagasaki@ypu.jp
 Yamaguchi Prefectural University, Japan 
In the fi eld of the Buddhist study, especially, the philosophy of 
the Buddhism, there are many texts which have been passed 
down throughout history since around 5th century BCE. In 
spite of the long history, philological study started around the 
18th century CE and has not yet been considered adequate. 
The original texts which were written in India do not remain, 
but many manuscripts copied by Buddhist monk scribes 
remain in Indic languages such as Sanskrit or Pāli or translated 
into Tibetan or classical Chinese. Those translated texts 
alone consist of huge number of pages. Some of them have 
been published in the scholarly editions, but many texts are 
not published, or are not verifi ed as reliable texts. Moreover, 
sometimes old Sanskrit manuscripts may be newly discovered 
in Nepal, Tibet or others. Under these conditions, even if a 
new scholarly edition is published, it may have to be edited 
again with the discovery of a newly-found Indian manuscript. 
Therefore, in the fi eld, a collaboration system for the sake of 
editing of the texts on the Internet would be effective so that 
reliable texts could be edited anytime, sentence-by-sentence, 
word-by-word, or even letter-by-letter on the basis of those 
witnesses. The collaboration system must be able to:
 (1) represent and edit the texts and the facsimile images in 
a multilingual environment such as Sanskrit, Tibetan and CJK 
characters.
 (2) store the information of the relationship between each 
of the content objects which are  included in the each of 
the witnesses as text data and facsimile image.
 (3) add the physical location of those objects to the 
information.
 (4) record the name of the contributors of each piece of 
information.
It is not diffi cult to represent and edit the texts and the facsimile 
images in a multilingual environment by the popularization of 
UTF-8. However, regarding CJK characters that are either not 
yet in Unicode or belong to the CJK-Extension B area, the 
collaboration system adopts a method called “Mojiyaki” to 
represent the glyphs of the characters as the image fi les on 
Web browsers and a character database called “CHISE” based 
on the IDS ( Ideographic Description Sequence ). Tibetan 
scripts are included in Unicode and supported in Windows 
Vista. But the font of the scripts is not included in the older 
versions of Microsoft Windows. In the fi eld, most researchers 
use the Tibetan texts on the computers by means of their 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
263
transliteration into ASCII characters. Thus, according to these 
conventions, the system must support at least ASCII characters 
for the Tibetan characters. Indic characters are the same as 
the Tibetan characters.
The content objects which are included in each of the witnesses 
such as the paragraphs, the sentences, the words or the letters 
in the text data or the fragments of the facsimile image have 
relationships to others in the context of the history of thought 
in Buddhism. The collaboration system provides a function 
that will describe such information about the relationships 
separately from the literal digital copies of the witnesses. The 
information at least consists of the location of the content 
objects in the witness such as the paragraph, the sentence 
or the word and the attributes of the relationship such as 
variants, quotations, translations, and so on. Because some 
relationships have a kind of a hierarchy, it must be refl ected in 
the collaboration system. However, it is important to keep the 
fl exibility in the methods of the description of the hierarchy 
because the hierarchy is often different in each tradition of the 
texts. One more important thing is that copyright problems 
might be solved by describing the information separately from 
the digital copies of the witnesses.
It is important to describe the physical location of the content 
objects by the means of the traditional methods such as page 
and line number so that the information of the relationship 
can maintain interchangeability with the traditional methods 
which refer to their physical location in the witness. 
The collaboration system must record the name of the 
contributors of the information so that responsibility for the 
information can be shown explicitly and users can fi lter the 
unnecessary information.
The prototype of a collaboration system which implements 
the above functions is already completed as a Web 
application using the “Madhyamaka Kārikā” which is a famous 
philosophical Buddhist text that has been quoted or referred 
in other texts since about the 3rd Century. It refl ects opinions 
of some Buddhist researchers. It is working on GNU/Linux 
using Apache HTTPD server and PostgreSQL and coded by 
PHP and AJAX so that users can do all of the works on their 
Web browsers. All of them consist of free software. It can be 
demonstrated on the Digital Humanities 2008. Moreover, At 
present, I am attempting to describe the relationships by use 
of RDF, OWL and the elements defi ned by the TEI. The method 
of the description will be also shown at the conference.
Bibliography
Caton, Paul, “Distributed Multivalent Encoding”, Digital 
Humanities 2007 Conference Abstracts, pp. 33-34. (2007).
DeRose, Steven, “Overlap: A Review and a Horse”, 
Extreme Markup Languages 2004: Proceedings, http://www.
mulberrytech.com/Extreme/Proceedings/html/2004/
DeRose01/EML2004DeRose01.html. (2004).
MORIOKA, Tomohiko, “Character processing based on 
character ontology”, IPSJ SIG Technical Report, 2006-CH-072, 
pp. 25-32. (2006).
Nagasaki, Kiyonori, “Digital Archives of Indian Buddhist 
Philosophy Based on the Relationship between Content 
Objects”, IPSJ SIG Technical Report, 2007-CH-75, pp. 31-38. 
(2007).
Nagasaki, Kiyonori, Makoto MINEGISHI and Izumi HOSHI, 
“Displaying multi-script data on the Web”, Proceedings of the 
Glyph and Typesetting Workshop, 21st Century COE Program 
“East Asian Center for Informatics in Humanities - Toward an 
Overall Inheritance and Development of Kanji Culture - “ Kyoto 
University, pp. 44-51. (2004).
Renear, Allen H., “Text Encoding”, A Companion to Digital 
Humanities, Blackwell Publishing, 2004, pp. 218-239. (2004).
Steinkellner, Ernst, “Methodological Remarks On The 
Constituion Of Sanskrit Texts From The Buddhist Pramāṇa-
Tradition”, Wiener Zeitschrift für die Kunde Südasiens, Band 
XXXII, pp. 103-129. (1998).
 
  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
264
 Information Technology 
and the Humanities: The 
Experience of the Irish in 
Europe Project 
 Thomas O’Connor
thomas.oconnor@nuim.ie
National University of Ireland, Maynooth, Ireland
Mary Ann Lyons
marian.lyons@spd.dcu.ie
St. Patrick’s College, Ireland
John G. Keating
john.keating@nuim.ie
 National University of Ireland, Maynooth , Ireland
 Irish in Europe Project
As a result of its leadership role within the fi eld of early 
modern migration studies, the Irish in Europe Project, founded 
in NUI Maynooth in 1997, is now the custodian of a rich and 
constantly expanding corpus of biographical data on Irish 
migrants in early modern Europe. The most substantial data 
sets relate to Irish students in the universities of Paris and 
Toulouse in the seventeenth and eighteenth century, Irish 
patients in Parisian hospitals in the early eighteenth-century, 
Irish students in Leuven University; Irish students in France’s 
leading medical faculties, and Irish regiments in eighteenth-
century France and Spain.
In line with the Project’s mission to make this data accessible 
to researchers, teachers and students and to ensure that 
the material is rendered responsive to the full range of 
users needs, the Irish in Europe Project undertook the 
development of an electronic research environment for the 
collection, management and querying of biographical data 
on Irish migrants in early modern Europe. This was facilitate 
by the award of a Government of Ireland Research Project 
Grant in the Humanities and Social Sciences in 2006, which 
accelerated progress in reaching four target objectives, namely 
to encode these databases in extensible Mark up language 
(XML), to provide internet based access to the databases, to 
establish a Virtual Research Environment (VRE) to facilitate 
research and teaching, and lastly, to manage and promote the 
adoption of best practice for biographical databases. In this 
encoded format, the data is rendered susceptible to a broad 
range of queries and manipulations, permitting, for instance, 
the automatic generation of graphics, maps, tables etc. This 
project will be rolled out in 2007-9. In July-November 2007 
important progress was made on the project in the context 
of the preparing a database Kiosk for the National Library of 
Ireland exhibition ‘Strangers to Citizens: the Irish in Europe, 
1600-1800’ which opened in December 2007.
Kiosk Development
The Kiosk is essentially user-friendly software providing access 
to research derived from four different biographical studies of 
migration in the Early Modern period, i.e. student migration to 
France (Brockliss and Ferté, 1987; Brockliss and Ferté, 2004), 
student migration to Louvain (Nilis, 2006), military migration 
to France (Ó Conaill, 2005) and military migration to Spain 
(Morales, 2002; Morales, 2003; Morales, 2007). Data from 
the associated databases were provided in Microsoft Excel 
format or extracted using custom developed programs, from 
Microsoft Word versions and Portable Document Format 
(PDF) versions of the research papers. XML data models, 
similar to those previously described by Keating et al. (2004) 
were used to encode the four data sets. Each dataset contained 
information pertinent to the profession under study, and 
there was some overlap, particularly related to personal data. 
In general, student data were associated with migration from 
dioceses whereas military migration data were associated with 
counties. Central to the software requirements of this project 
was the specifi cation that common software tools should 
be used to visualize differing data sets whenever possible 
for (i) usability issues and (ii) providing comparative analyses 
functionality which has never been available to Early Modern 
migration researchers, for example, see Figure 1.
The project team designed and developed two distinct 
and fundamental “database inspectors” essential for data 
visualization: (i) an interactive vector-based “heat-map” of 
Ireland which provides scaled diocese or county density 
migration patterns for a variety of inputs selected by the 
user, and (ii) an interactive record inspector which provides 
key record information based on exact, partial or phonetic 
searching of individual surnames and other personal features. 
The database inspectors were developed using Macromedia 
Flex and have been tested in a variety of modern browsers. 
Converted datasets reside on the Irish in Europe web 
server (http://www.irishineurope.ie) and are accessed using 
optimized searching software implemented as web services. 
Database inspectors communicate with the web services 
using CGI (outward); the latter return XML which describe 
how the maps or record inspectors should be drawn – the 
drawing functions are all implemented using Macromedia Flex 
and Flash, as shown in Figure 2.
The Kiosk, essentially hosting a virtual exhibition, was 
custom developed (using Objective C) within the project 
using Internet browser tools provided as part of the XCode 
and Safari development suite. The current version provides 
accessibility options including user selected zoom and pan, 
and an audio soundtrack for each virtual exhibition page. The 
complete system is available for operation in Kiosk mode or 
can be accessed online. Overall, this project was completed 
in 28 person months and required a wide range of software 
development and information architecture skills not possessed 
by a single individual. We propose to present the lessons 
learned from the management of the project and development 
process, in addition to those described below.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
265
Lessons learned from Collaboration
The collaborative exercise involved in the production of NLI/
Irish in Europe Biographical Databases Kiosk yielded three 
critical results: 
Firstly, the exercise revealed that the software architecture 
of the Kiosk had to refl ect both the nature and form of the 
historical data used and the specifi cations of the clients. It 
was the practical interaction between the data collectors/
interpreters and the information technology experts that 
shaped the architecture of the Kiosk. These functions can not 
and ought not be compartmentalized and, in project of this 
nature, should not be hermetically sealed off from each other.
Secondly, the solution of technical and software engineering 
diffi culties and challenges in the project involved not only 
increasingly defi ned specifi cations from the data collectors 
and the NLI clients but also critical input from the software 
engineering and technical teams. For instance, important 
inconsistencies in the raw information came to light thanks to 
the interrogations of the software and technical teams, who 
had tested the data. This led to a feedback, which permitted 
the creation of a practical working relationship between 
the humanities team, the software and technical teams and 
the clients. It is highly suggestive for the development of 
architectures for humanities-software-client-user relations in 
the future.
Thirdly, the expansion in the range of functions provided by the 
site was driven by the dynamic interface between the software/
technical teams and the information. While it is usual for the 
architecture of similar sites to be dictated by purely technical 
criteria or costing issues, this project revealed the possibility 
for creative input from the technical-raw data interface. It is 
rare for technical teams to be afforded this level of access to 
the data collection and interpretation/presentation.
References
Brockliss, L.W.B. and Patrick Ferté, P. (1987) Irish clerics 
in France in the seventeenth and eighteenth centuries: a 
statistical survey. Proceedings of the Royal Irish academy , 87 C, 
pp. 527-72.
Brockliss, L.W.B. and Patrick Ferté, P. (2004). Prosopography 
of Irish clerics in the universities of Paris and Toulouse, 1573-
1792. Archivium Hibernicum, lviii, pp. 7-166.
Hernan, E. G. and Morales, O. R. (2007). Essays on the Irish 
military presence in early modern Spain, 1580-1818, Eds. Madrid, 
Ministerio de Defensa.
Keating, J. G., Clancy, D., O’Connor, T. and Lyons, M. A. (2004) 
Problems with databases and the XML solution. Archvium 
Hibernicum, lviii, pp. 268-75.
Morales, O. R. (2002) El socorro de Irlanda en 1601, Madrid.
Morales, O. R. (2003) España y la pérdida del Ulster, Madrid.
Morales, O. R. (2007). The Irish military presence in the Spanish 
Armies, 1580-1818. Madrid, Ministerio de Defensa.
Nilis, J. (2006). Irish students at Leuven University, 1548-1797’. 
Archivium Hibernicum, lx, pp. 1-304.
Ó Conaill, C. (2005) ‘“Ruddy cheeks and strapping thighs”: 
an analysis of the ordinary soldiers in the ranks of the Irish 
regiments of eighteenth-century France’ in The Irish Sword, 
xxiv (2005/5), pp 411-27. 
 
Figure 1: Extract from “Strangers to Citizens” 
Kiosk’s Database Landing page
Figure 2: Extract from “Strangers to Citizens” Kiosk’s 
Student Migration Comparison page showing two 
Database Inspectors (Diocese Density Maps)
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
266
Shakespeare on the tree
 Giuliano Pascucci 
giuliano@aconet.it
University of Rome “La Sapienza” , Italy
This work illustrates some computer based tools and 
procedures which have been applied to the complete corpus 
of Shakespeare’s works in order to create a phylogenetic tree1 
of his works and to discriminate between Shakespeare’s and 
Fletcher’s authorship in the writing of All is True, which is known 
to be the result of a collaboration of both authors.
The general procedure applied to this study was devised in 2001 
by a group of University of Rome La Sapienza2 scholars who 
asked themselves whether starting from a graphic character 
string it was possible to extract from it information such as 
the language in which the string itself was written, whether or 
not it belonged to a wider context (e.g. it is a part of a literary 
work), its author and its fi liation from other texts.
Their method was based on the idea of linguistic entropy as it 
has been defi ned by Chaitin-Kolmogorov3 and was applied to 
the translations into 55 different languages of a single text: the 
Universal Declaration of Human Rights.
Following Kolmogorov’s theories and considering each of 
the 55 texts as a mere string of characters they decided to 
study the linguistic entropy of each text, that is to say the 
relationship between the information contained in a string 
of characters and the ultimate compressibility limit of the 
string itself. Because said limit, i.e. the complete removal of 
all redundancies, is also the ideal limit that a zipping software 
should reach, the authors used a compression algorithm to 
fi nd out what kind of information could be extracted from a 
given text starting from what they could get to know about its 
linguistic entropy.
The algorithm they used was an appropriately modifi ed 
version of LZ774, which was able to index the length and 
position of redundant strings within a text, and also, among 
other things, to extrapolate and collect them. Such modifi ed 
version of LZ77 was called BCL1, thus named after the initials 
of the authors’ names. This algorithm was sided by two more 
software programs (FITCH and CONSENSE)5 created by 
Joe Felsenstein for inferences about phylogenies and made 
available online by the University of Washington. After 
processing the texts using both BCL1 and FITCH, Benedetto, 
Caglioti and Loreto obtained a graph in which the 55 languages 
were grouped in a way that matches to a surprising extent 
philological classifi cations of the same languages.
Although the authors had no specifi c interest in literature, at 
the end of their paper they expressed the hope that their 
method may be applied to a wider range of areas such as DNA 
and protein sequences, time sequences and literature.
The most relevant difference between the present case study 
and previous stylometric studies is that the latter have only 
dealt with the analysis of single words or sentences. More 
precisely, the studies which investigated single words especially 
focussed on features such as length, occurrence and frequency, 
whereas the works that dealt with phrase or sentence analysis 
especially studied features such as the avarage number of 
words in a sentence, the avarage length of sentences, etc.
These procedures have been followed in many different seminal 
studies which gave birth to modern stylometry: Ellegård’s 
study about the Junius Letters6, Mosteller’s investigation of the 
Federalist Papers7, Marriott’s analysis of the Historia Augusta8, last 
but not least Mendenhall’s scrutiny of Shakespeare’s works9. 
During the last decades of the last century, some new studies 
have been carried out using computer based tools. However, 
such studies do not differ from those dating back to 19th 
and 18th century, in that they use computers as fast helpers, 
instead of bringing about new hypotheses based on specifi c 
characteristics and potential of the computer. This is the case, 
for example, of Thomas Horton’s10 study about function words 
in Shakespeare’s All is True and Two Noble Kinsmen.
On the contrary, the present study doesn’t analyse function 
words or a particular class of words, nor does it simply deal 
with phrase or sentence analysis. The investigation here is 
based on the ratio of equal character strings shared by two or 
more texts. Moreover a character string cannot be identifi ed 
with a single word or phrase in that it may contain diacritical 
signs, punctuation, blanks and even word or phrase chunks.
A few years ago Susan Hockey clearly stated that11, if deeply 
investigated, a text must show on some level its author’s 
DNA or fi ngerprint. It goes without saying that the greater 
the number of DNA sequences common to two biological 
entities, the greater their phenotypical resemblance is. As a 
consequence it is also very likely that the two entities are in 
someway related.
Based on this conviction, which has nowadays become self-
evident both in Biology and Genetics, the present study 
analyses shakespearian texts as though they were mere DNA 
strings. Then the texts are automatically grouped into families 
and placed on a phylogenetic tree, so as to account both for 
their evolution and for their deepest similarities.
Results are surprisingly precise and the families thus created 
confi rm the groupings which textual critics and phylologists 
have agreed on over the last few centuries. Other interesting 
similarities have also been brought to light. The algorithms, for 
instance, have been perfectly able to recognise and group on a 
single branch of the phylogenetic tree Shakespeare’s so called 
Roman Plays, which share the same setting, some themes and 
a number of characters. The system also grouped together 
the Historical plays, works whose similarities have also been 
acknowledged by textual and literary criticism. Furthermore 
the experiment has pointed out a liguistic similarity between 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
267
the tragedy of Romeo and Juliet and some of Shakespeare’s 
comedies. Although such similarity has never been studied in 
depth and certainly deserves further investigation, it would 
not come across as unlikely or bizarre to the shakespearian 
scholar.
Experiments have also been carried out to test the validity of 
the algorithms on shorter texts. In this second phase of the 
study, the complete corpus of Shakespeare’s works was split 
into 16 KiB text chunks. This time the system was able to 
create 37 subsets of chunks, each of which coincided with a 
play and appropriately placed each subset on the phylogenetic 
tree.
In a fi nal phase of the experiment the effectiveness of the 
algorithms on authorship was tested against a corpus of 1200 
modern English texts by known authors with positive results 
and it was then applied to the play All is True to discriminate 
between Shakespere’s and Fletcher’s authorship vis-à-vis single 
parts of the play.
Whereas the results achieved during the testing phase were 
completely successful (100% of correct attributions) when 
dealing with Shakespeare’s All is True they were a little less 
satisfactory (about 90%). Two factors may account for this: 
on the one hand this may be due to the fl uidity of English 
morphology in that period; on the other hand this specifi c text 
may have suffered the intervention of other authors as a few 
critics have suggested during the last century.
Experiments are still being carried out to refi ne the procedure 
and make the algorithms produce better performances.
 Notes
1 A phylogenetic tree is a graph used in biology and genetics to 
represent the profound  relationship (e.g. in the DNA), between two 
phenotypically similar entities which belong to the same species or 
group.
2 D. Benedetto, E. Caglioti (department of Mathematics), V. Loreto 
(Department of Physics)
3 A.N. Kolmogorov, Probl. Inf. Transm. 1, 1(1965) and G.J. Chaitin, 
Information Randomness and Incompleteness (WorldScientific, 
Singapore, 1990), 2nd ed.
4 LZ77 is a lossless data compression algorithm published in a paper 
by Abraham Lempel and Jacob Ziv in 1977. 
5 Both programs are based on algorithms used to build phylogenetic 
trees and are contained in a software package called PHYLIPS.
6 A. Ellegård, A Statistical Method for determining Autorship: The Junius 
Letters 1769-1772, Gothenburg, Gothenburg University, 1962
7 F. Mosteller, D. Wallace, Inference and Disputed Authorship: The 
Federalist, Reading (Mass.),  Addison-Wesley, 1964
8 I. Marriot, “The Authorship of the Historia Augusta: Two Computer 
Studies”, Journal of Roman Studies, 69, pagg. 65-77 
9 T. C. Mendenhall, “A Mechanical Solution of a Literary Problem” , 
The Popular Science Monthly, 60, pagg. 97-105
10 The Effectiveness of the Stylometry of Function Words in Discriminating 
Between Shakespeare and Fletcher,  Edinburgh, Department of Computer 
Science, 1987. This text can be found online at:  http://www.shu.ac.uk/
emls/iemls/shaksper/fi les/STYLOMET%20FLETCHER.txt
11 Hockey S., Electronic Texts in the Humanities, New York, Oxford 
University Press, 2000
  
  
 
  
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
268
XSLT (2.0) handbook 
for multiple hierarchies 
processing 
 Elena Pierazzo
elena.pierazzo@kcl.ac.uk
King’s College London, UK
Raffaele Viglianti 
raffaele.viglianti@kcl.ac.uk
King’s College London , UK
In transcribing and encoding texts in XML, it is often (if not 
always) the case that structures and features do not nest within 
each other but overlap. Paraphrasing a notorious quotation 
from Paul Maas on the most unsolvable problem in editing, one 
may say that “there is no remedy for multiple hierarchies”1. 
Every year a considerable number of new papers and posters 
about how to deal with multiple hierarchies and overlapping 
structures are presented to conferences, and the new version 
of the TEI includes a chapter on similar matters2. And yet, as 
the liveliness of debate shows, a convenient, standard and 
sharable solution is still to be found. 
Any kind of text potentially (and actually) contains multiple 
hierarchies, such as verses and syntax, or speeches and verses. 
Perhaps the most extreme form of this problem arises when 
transcribing a manuscript, assuming that the transcriber wants 
to describe both the content and also the physical structure 
and characteristics at the same time. Pages, columns and line-
breaks confl ict with paragraphs and other structural and non 
structural divisions such as quotation and reported speeches, 
as well as deletions, additions, and changes in scribal hand.
At present three different approaches to this problem have 
been proposed:
1. non-XML notation (for instance LMNL in Cowan, 
Tennison and Piez 2006) to be processed by specifi c tools 
which must be developed in-house;
2. pseudo-XML, such as colored XML or concurrent XML;3
3. full XML approaches such as milestones, stand-off 
markup4, or fragmentation.
All of these approaches depend on post-processing tools. 
Some of these tools have been developed using scripting or 
other languages such as perl, Python, or Java,  and others use 
XSLT-based approaches. 
The milestone approach is the one chosen by the TEI, and, 
consequently, by us. Nevertheless, milestones introduce 
greater levels of complexity when building (X)HTML output 
and for this reason they might not be used in practice.
As XSLT is the most common technology used to output XML 
encoded texts, at CCH we experimented with different ways 
to deal with milestones and have developed a handful of XSLT 
techniques (as opposed to tools) that can be easily adapted to 
different circumstances. 
Clashing of two hierarchies
Let us consider, for example, the TEI P5 empty element 
<handShift/> that delimits a change of scribal hand in a 
manuscript. Yet problems arise if one needs to visualize both the 
paragraphs and the different hands with different formatting. 
In case of XHTML visualization, one would almost certainly 
want to transform the <handShift> from an empty element 
to a container, but this container could well then overlap with 
existing block or inline elements such as <p>. 
The easiest way to deal with this is to use the “disable-output-
escaping” technique by outputting HTML elements as text; for 
instance:
<xsl:template match=”tei:handShift”>
    <xsl:text disable-output-
escaping=”yes”>&lt;span style=”color:
red;”&gt;</xsl:text>
</xsl:template>
<xsl:template match=”tei:anchor[@
type=’end-handShift’]”>
    <xsl:text disable-output-
escaping=”yes”>&lt;/span&gt;</xsl:text>
</xsl:template>
However, this solution presents the obvious disadvantage that 
the output will not be well structured (X)HTML, and although 
browsers are often forgiving and therefore may cope with this 
in practice, this forgiveness cannot be relied on and so this 
process cannot be recommended. 
A better option is to transform the area delimited by two 
<handShift>s in a container but fragmenting it to avoid 
overlapping.
One possible XSLT algorithm to expand and to fragment an 
empty element could be:
• Loop on the <handShift>s
• Determine the ancestor <p>
<xsl:variable name=”cur-p” 
select=”generate-id(ancestor::p)”/>
• Determine the next <handShift>
<xsl:variable name=”next-
hs” select=”generate-
id(following::handShift)”/>
• Create a new non-empty element <handShift>
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
269
• Loop on all nodes after <handShift/>, up to but not 
including either the next <handShift/> or the end of the 
current <p>. This can be achieved using an XPath expression 
similar to the following:
following::*[ancestor::
p[generate-id()=$cur-p]]
    [not(preceding::handShift[generate-
id()=$next-hs])] 
    | 
following::text()[ancestor::
p[generate-id()=$cur-p]] 
    [not(preceding::handShift[generate-
id()=$next-hs])]
That will return: 
<p> … <handShift> … </handShift></p>
<p><handShift> … </handShift> … </p>
This resulting intermediate XML could then be used to 
produce XHTML that would fulfi l the visualization required. 
However this would involve another XSL transformation and 
the intermediate fi le would not be valid against the schema, 
unless an ad hoc customized schema is generated for that 
purpose.
Nevertheless, thanks to XSLT 2 it is possible to produce a 
single process outputting a fi rst transformation into a variable 
and then apply other templates on the variable itself using 
the mode attribute, thus dividing the process into steps and 
avoiding both external non-valid fi les and also modifi cations 
to the schema.
This is an example using the mode attribute.
• Declaration of variables 
<xsl:variable name=”step1”>
        <xsl:call-template name=”one”/>
</xsl:variable>
<xsl:variable name=”step2”>
        <xsl:apply-templates 
select=”$step1” mode=”step2”/>
</xsl:variable> 
• XML to XML transformation (Step 1)
Copying the whole XML text: 
<xsl:template match=”*” mode=”step1”>
 <xsl:copy>...</xsl:copy>
</xsl:template>
Other templates (as the ones described above) to transform 
<handShift/>:
<xsl:template match=”handShift” 
mode=”step1”>
[...]
</xsl:template>
Saving the elaborated fi le into the declared variable:
<xsl:template name=”one” mode=”step1”>
        <xsl:apply-templates 
select=”TEI” mode=”step1”/>
</xsl:template> 
•  XHTML transformation (Step 2)
<xsl:template match=”/” mode=”step2”>
 <html>...</html>
</xsl:template>
Other templates to transform <handShift> and <p> in XHTML 
elements:
<xsl:template match=”handShift” 
mode=”step2”>
 <span class=”hand”>...</span>
</xsl:template>
• Output
<xsl:template match=”/”>
        <xsl:copy-of select=”$step2”/>
</xsl:template>
The poster will include a comparison of the performances 
of the XSLT 2.0 algorithm with a sequence of XSLT 1.0 
transformations.
Clashing of more than two hierarchies
It is not improbable that in complex texts such as manuscripts 
more than two hierarchies clash. Consequently, the diffi culties 
of visualization in XHTML can become more complex.
During the analysis for a CCH project devoted to the digital 
edition of Jane Austen’s manuscripts of fi ctional texts, the 
need emerged to mark up lines as block elements in order to 
manage them via a CSS stylesheet.
In TEI P5 lines are marked by the <lb/> empty element, and so 
it was necessary to transform these into containers. Therefore 
at least three hierarchies were clashing: <handShift/>, <lb/> 
and <p>. 
A good way to handle the confl ict could be looping on text 
nodes between milestones. In the following example, all the 
text nodes between <handShift>s are expanded into container 
elements and then transformed into <span> elements carrying 
a class attribute. Moreover all the lines are transformed into 
further <span>s using the algorithm mentioned before in 
order to manage them as block elements.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
270
The following templates show a possible implementation of 
this method.
Step 1 XML to XML:
<xsl:template match=”text()
[not(ancestor::teiHeader)]” 
mode=”step1”>                
<handShift>
<xsl:copy-of select=”preceding::
handShift[1]/@new”/>
<xsl:value-of select=”.”/>
</handShift>
</xsl:template>
Step 2 XML to XHTML:
<xsl:template match=”handShift”>
 <span class=”@new”>
<xsl:apply-templates mode=”step2”/>
</span>
</xsl:template>
Such a solution is also applicable with more than two clashing 
hierarchies. 
Even though this approach can be applied generically, a deep 
analysis of the needs for representation and visualization is 
required in order to develop more customized features. For 
instance, the need to show lines as block elements has caused 
other hierarchical clashes that have been resolved using 
customized applications of the algorithms explained above. 
According to project requirements, in fact, if the apparently 
innocuous element <lb/> is used to produce non-empty 
elements in the output, any TEI element at a phrasal level is 
potentially overlapping and requires a special treatment.
The poster may be seen as providing an XSLT Cookbook for 
multiple hierarchies (the XSLT code  will be available as just 
such a cookbook from the TEI wiki.) In our opinion simple 
recipes are better for encoding multiple hierarchies than a tool 
is, even a customizable one. The fl exibility and the extensibility 
of the TEI encoding schema allows for an almost infi nite 
combination of elements, attributes and values according to 
the different needs of each text. Since the fi rst release of the 
TEI Guidelines, the Digital Humanities community has learnt 
to enjoy the fl exibility of SGML/XML based text encoding, but 
such advantages come with a price, such as the diffi culty of 
creating generic tools able to accommodate the specifi c needs 
of every single project. 
Furthermore, even assuming that a fi nite combination of 
elements, attributes and values could be predicted at input 
(considerably limiting the possibilities offered by the TEI 
schema), the potential outputs are still infi nite. This is why the 
most successful technology for processing text encoded in 
XML is either an equally fl exible language – XSLT – or tools 
that are based on such a language but that still require a high 
degree of customization.
Therefore, sharing methodologies and approaches within 
the community, though disappointing for those looking for 
out-of-the-box solutions, is perhaps the most fruitful line of 
development in the fi eld of multiple hierarchies.
Notes
1 “Gegen Kontamination ist kein Kraut gewachsen”, in Maas 1927.
2 “Non-hierarchical structures” in Burnard and Bauman 2007 at 
http://www.tei-c.org/release/doc/tei-p5-doc/en/html/NH.html. 
Something of the kind for SGML was also in TEI P3.
3 See Sperberg McQueen 2007 for an overview.
4 Burnard and Bauman 2008, at http://www.tei-c.org/release/doc/tei-
p5-doc/en/html/SA.html#SASO
References
Lou Burnard and Syd Bauman (2007) TEI P5: Guidelines for 
Electronic Text Encoding and Interchange, available at http://
www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html 
(11/12/07).
Paul Maas (1927), Textkritik, Leipzig.
Alexander Czmiel (2004) XML for Overlapping Structures 
(XfOS) using a non XML Data Model, available at http://www.
hum.gu.se/allcach2004/AP/html/prop104.html#en (11/16/07)
John Cowan, Jeni Tennison and Wendell Piez (2006), LMNL 
Update. In “Extreme Markup 2006” (slides available at  http://
www.idealliance.org/papers/extreme/proceedings/html/2006/
Cowan01/EML2006Cowan01.html or at http://lmnl.net) 
(11/16/07)
Patrick Durusau and Matthew Brook O’Donnell (2004) 
Tabling the Overlap Discussion, available at http://www.
idealliance.org/papers/extreme/proceedings/html/2004/
Durusau01/EML2004Durusau01.html (11/16/07)
Michael Sperberg McQueen (2007), Representation of 
Overlapping Structures. In Proceedings of Extreme Markup 
2007 (available at http://www.idealliance.org/papers/
extreme/proceedings/xslfo-pdf/2007/SperbergMcQueen01/
EML2007SperbergMcQueen01.pdf) (15/03/2008)
 
  
  
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
271
 The Russian Folk Religious 
Imagination
Jeanmarie Rouhier-Willoughby
j.rouhier@uky.edu
University of Kentucky, USA
Mark Lauersdorf
mrlaue2@email.uky.edu
University of Kentucky, USA
Dorothy Porter 
dporter@uky.edu
University of Kentucky, USA
The “folk” and intellectual views of folk material play a 
profound role in Russian cultural history. The German 
Romantics argued that the folk conveyed the true essence 
of a nation’s identity, a stance adopted in nineteenth century 
Russia. Beginning in the late 1930s, the Soviets held up the folk 
as heroes who conveyed the ideals of the socialist state. Both 
of these conceptions persist until the present day and present 
an attitude that differs signifi cantly from most contemporary 
civil societies, where the folk are typically viewed as poor, 
backward people that need to be enlightened. Throughout 
19th- and 20th-centuries, folk culture and “high” culture have 
come together in Russia in a way unknown in most European 
societies. Thus, it can be argued that in order to understand 
Russia, one must study the conception of the “folk” and their 
belief systems. Russian religion is no exception in this regard; 
to study Russian religious belief without a consideration of the 
folk conceptions is to overlook one of the most important 
sources for Russian religious ideas.
Russian Orthodoxy has been the source of a great deal of 
speculation about the extent of dvoeverie (dual faith). Soviet 
scholars argued that the Russian Orthodox religion had a strong 
pre-Christian base, which allowed the Soviet government to 
assert that religious folk tradition was actually not “religious” 
at all and thereby should be preserved despite the atheist 
policies of the state. Since the fall of the Soviet Union, scholars 
have undertaken the study of folk religion in earnest, but there 
is as yet no comprehensive study of the interrelations between 
various folk genres in relation to religious belief. Typically 
folklorists study either oral literature (e.g., legends and songs), 
or folk ritual and iconography. This separation of genres inhibits 
a full understanding of the complexity of the complete religious 
belief system. Our multimedia web-based critical edition, the 
Russian Folk Religious Imagination (RFRI), will feature an 
innovative cross-disciplinary approach combining the study of 
legends on saints and biblical fi gures, songs and religious rituals, 
and folk iconography into a single, comprehensive research 
project that will be published in a new digital framework 
designed to integrate text and multimedia into a coherent 
whole (http://www.rch.uky.edu/RFRI/). We are using the AXE 
annotation tool (created by Doug Reside at the Maryland 
Institute for Technology in the Humanities: http://www.mith2.
umd.edu/) for encoding commentary on audio, video, images 
and textual materials (for an example of video annotation, see 
here: http://www.rch.uky.edu/RFRI/AXE-example.html). This 
far-reaching project will be of use to specialists in a wide range 
of disciplines including historians, folklorists, anthropologists, 
linguists, and scholars of literature and of religion, as well as 
to amateur historians and to the general public. Our poster 
presentation will showcase the achievements of the project 
thus far and provide demonstrations of the variety of material 
and techniques we are using in the development of this 
project.
Despite the Soviet government’s tolerance of scholarly 
fi eldwork gathering folk religious traditions, there is a paucity of 
in-depth research and publication in this area, due to the offi cial 
policy of atheism in the Soviet Union. Folklorists collecting 
data on this topic often could not publish their fi ndings, and 
the material has languished in archives and private collections. 
Even when published editions did exist, they quickly went out 
of print, so that specialists elsewhere were also unable to do 
extensive research on Russian folk religion. Our edition will 
provide unprecedented access for scholars and students of 
folklore and of Russia to materials they could not previously 
obtain without extensive archival work. Scholars will be able to 
read currently inaccessible texts, access audio and video fi les 
of song and legend texts and rituals, and consult still images 
of folk iconography and rituals. Users will be able to search, 
compare, and study in full context the entire range of text, 
image, audio and video materials in a way that would not be 
possible in a print edition. This approach not only provides a 
greater breadth of materials (typically only available in Russian 
archives) for both scholars and students, but also brings to the 
fore the advantages of using digital resources in the humanities. 
In addition, the RFRI project will serve as broad an audience as 
possible by providing Russian originals and English translations 
of both the original texts and the scholarly commentary and 
textual analyses. The project resulting from this expertise will 
signifi cantly increase the knowledge of and scholarly interest 
in Russian folk belief and religion. A digital multimedia critical 
edition, as we have conceived it, will not only make use of the 
latest in digital technology, but will also feature a combination 
of technologies that is truly cutting-edge. Our methods 
and design will be a model for other scholars to follow in 
developing fully integrated multimedia research projects using 
open-source software and internationally accepted standards.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
272
Using and Extending FRBR 
for the Digital Library for the 
Enlightenment and the Romantic 
Period – The Spanish Novel (DLER-
SN)
Ana Rueda
rueda@email.uky.edu
University of Kentucky, USA
Mark Richard Lauersdorf
mrlaue2@email.uky.edu
University of Kentucky, USA
Dorothy Carr Porter
dporter@uky.edu
University of Kentucky, USA
Spanish literature has been largely overlooked in the 
development of the canon of European fi ction of the 
Enlightenment, and literary criticism has traditionally treated 
the eighteenth-century and early nineteenth-century Spanish 
novel with indifference, when not with hostility.  While 
France, England, and Germany have produced print catalogs 
of their novelistic production for this period, Spain still lacks 
one.  Manuals of literature constantly remind us of the void 
in Peninsular Spanish literature in this genre, a sentiment 
which has become a commonplace in literary scholarship. 
Montesinos, one of the better informed scholars in the fi eld, 
sees nothing but sterility in the Spanish eighteenth-century 
novel of this “lamentable período” (Introducción a una historia de 
la novela en España, en el siglo XIX, 40), and Chandler, like many 
literary investigators of this period, concludes that literary 
efforts were reduced to servile imitations of the masters of a 
preceding age (A New History of Spanish Literature, 497).  Such 
perceptions are changing as the Spanish Enlightenment is now 
becoming an important site of critical inquiry with a growing 
body of scholarship.  Spain produced a national body of fi ction 
that was far superior in volume to the few acclaimed fi ctional 
accounts and enjoyed great popularity among the general 
readership of its day.
Extra-literary factors hindered but also shaped the Spanish 
novel between 1700 and 1850.  Given the extraordinary 
popularity of novels through this period over the whole of 
Europe and the fact that Spain closely followed French fashions 
after Felipe V became the fi rst king of the Bourbon dynasty, it is 
not surprising that Spain translated and adapted foreign novels 
to its own geography, customs, and culture.  The production of 
Spanish novels picked up considerably after 1785, propelled by 
the adaptation of European models published in translation.  But 
the country’s novelistic output suffered from a major set-back: 
religious and government censorship, which was preoccupied 
with the moral aspect of novel writing.  Although translations 
of foreign novels were widely read in Spain, the erudite accused 
them of contributing to the corruption of the Spanish language. 
Further, the novel as a genre disquieted censors and moralists 
of the age.  At best, the novel was condemned as frivolous 
entertainment, devoid of “useful” purpose, to the extent that 
Charles IV issued a decree forbidding the publication of novels 
in 1799.  The decree, however, was not consistently enforced, 
but it did affect literary production. Given the dual censorship 
– religious and civil – existing in Spain at the time, many novels 
remained in manuscript form.  Spain’s publishing and reading 
practices remained largely anchored in the ancien régime until 
the 1830’s, a decade that witnessed social transformations and 
innovations in printing technology which dramatically altered 
the modes of production and consumption in the literary 
marketplace.
The Digital Library for the Enlightenment and the Romantic Period 
– The Spanish Novel (DLER-SN), an international collaborative 
project of scholars at the University of Kentucky and the 
Consejo Superior de Investigaciones Científi cas in Madrid, 
Spain, is creating an online resource center for the study of 
the Spanish novel of the Enlightenment that extends into the 
Romantic Period (1700-1850).  This scholarly collection will 
reconstruct the canon for the eighteenth- and nineteenth-
century novel in Spain and, by joining bibliographic and 
literary materials with an extensive critical apparatus, it will 
constitute a quality reference work and research tool in the 
fi eld of Eighteenth- and Nineteenth-Century Studies.  The 
overall objective is to foster new research on the signifi cance 
of these Spanish novels by providing online open access to a 
collection of descriptive bibliographic materials, critical studies, 
and searchable full-text editions.  The proposed poster will 
outline the DLER-SN project and showcase the fi rst portion 
of it: the development and implementation of the bibliographic 
database that will serve as the backbone for the complete 
digital collection. 
To organize the DLER-SN Database, we turn to the Functional 
Requirements for Bibliographic Records (FRBR), an entity-
relationship model for describing the bibliographic universe, 
developed by the International Federation of Library 
Associations and Institutions (IFLA). FRBR allows us not only 
to describe the individual novelistic works, but also to place 
them within the context of the collection (i.e., within their full 
eighteenth and nineteenth-century literary context). In this 
way it is different from every other bibliographic resource of 
the period of which we are aware.
A single novel may exist in one manuscript, written in Spanish; 
a second manuscript that is a translation of the Spanish into 
French; two separate printed editions of the Spanish; one 
printed edition of the French; and several individual copies of 
each edition, found in different libraries in the US and Europe.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
273
Entity: Work
In the FRBR model, the text of the original novel would be 
identifi ed as the work. The FRBR guidelines are clear that the 
concept of work is in fact separate from any physical object. 
Although in our example the original novel is synonymous 
with the Spanish manuscript, it is the story/ideas/text written 
in the manuscript and not the manuscript itself that is the 
work.
Entity: Expression
The manuscripts, both Spanish and French, are two expressions 
of the same work. These expressions have relationships among 
each other and with the work. In our example, both expressions 
represent the same work, and they also relate to each other: 
the French is a translation of the Spanish.
Entity: Manifestation
The editions – the actual print runs – are manifestations of the 
expressions on which they are based. In the case of manuscripts, 
the manifestation is the manuscript itself.
Entity: Item
The individual copies of the editions, as found in libraries and 
private collections, are items which exemplify the manifestations. 
Variations may occur from one item to another, even when the 
items exemplify the same manifestation, where those variations 
are the result of actions external to the intent of the producer 
of the manifestation (e.g., marginal notation, damage occurring 
after the item was produced, [re]binding performed by a 
library, etc.). In the case of manuscripts, the item is the same as 
the manifestation (i.e., the item is the manuscript itself).
 
The application of the FRBR model to the DLER-SN Database 
is fairly straightforward, however there are three issues that 
are specifi c to the Spanish novel that involve extensions to 
the model.
New Work vs. Expression
In the FRBR model there is no clear dividing line between 
assigning a modifi ed text as a new work or as an expression of 
an existing work. To illustrate this problem, let’s add an English 
version of our novel to the mix, since multiple versions of 
stories written in different languages are common occurrences 
during our period of investigation of the Spanish novel. Our 
hypothetical English version clearly tells the same story as 
both the Spanish and French expressions, but it is clearly not 
a translation of either of them. Perhaps some characters are 
left out and others are added, scenes are rearranged, etc. The 
English version could be considered a new work or another 
expression of the original work depending on the contextual 
relationship we wish to emphasize. In order to emphasize 
that there is a clear thematic relationship between the English 
version and the Spanish and French versions (i.e., it tells much 
of the same story), it could be reasonable for us to say that, 
in this case and for our purposes, the English version is an 
expression of the same work rather than a new work. However, 
if in assembling our FRBR-oriented database we were to 
choose to call the English version a new work, we would then, 
perhaps, wish to experiment with the creation of a more 
abstract category above work to illustrate that there is some 
relationship (in character archetypes, leitmotifs, etc.) between 
the different works.
“Masked” Works and Expressions
In the DLER-SN collection we have identifi ed two types of 
“masked” works and expressions. 
1. An author claims his product is a translation of another 
text, but it is in fact an original product (the author is 
claiming an expression of an existing work, but it is actually a 
new work) – this is a “masked” work.
2. An author claims his product is an original when it is in 
fact a translation of an existing text (the author is claiming a 
new work, but it is actually an expression of an existing work) 
– this is a “masked” expression. 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
274
We will need to make decisions about how to identify these 
products within our FRBR-based system. It may be most 
reasonable to identify such texts as both work and expression 
(there is nothing in the guidelines that disallows such multiple 
designations); however, if so, we will need to identify them 
appropriately as “masked”.
Censorship & Circumvention (Its 
Effect on Work, Expression, Manifestation, 
and Item)
In some cases censorship will be an issue – i.e., changes 
made to texts to satisfy the requirements of the government 
and the church can potentially have an effect on any level of 
the hierarchy. If this is the case, we will need to determine 
a method for identifying these changes and describing them, 
perhaps identifying “types” of modifi cations:
• title changes
• prologues and prologue variations
• publication outside Spain
• personal (unoffi cial) reproduction and distribution
We also anticipate that there may have been illegal print runs 
made of uncensored texts. If so, we will want to identify these 
uncensored runs with a special designation in the system. An 
especially interesting case would be a text that appeared in 
both censored and uncensored printings.
The database will be an important educational tool as well 
because it discloses a body of fi ction severely understudied (and 
undertaught) due to lack of documentation and inaccessibility. 
The DLER-SN Database will help scholars reassess the need for 
modern editions and for new studies of these forgotten texts 
which, nonetheless, constituted the canon of popular culture 
of their time.  It is important to note that only a dozen of these 
novels enjoy modern editions.  Once completed, our project 
will give scholars in multiple disciplines the tools necessary to 
begin serious work in this dynamic but underworked fi eld.
Bibliography
Reginald F. Brown. La novela española, 1700-1850 (1953).
Richard E. Chandler and Kessel Schwartz, A New History of 
Spanish Literature. Revised Edition. Baton Rouge: Louisiana 
State University Press (1991)
José Ignacio Ferreras. Catálogo de novelas y novelistas españoles 
del siglo XIX (1979).
Functional Requirements for Bibliographic Records Final 
Report, IFLA Study Group on the FRBR. UBCIM Publications 
– New Series Vol 19.  (http://www.ifl a.org/VII/s13/frbr/frbr.
pdf)
Dionisio Hidalgo. Diccionario general de bibliografía española 
(1867).
José Montesinos. Introducción a una historia de la novela en 
España en el siglo XVIII. Seguida de un esbozo de una bibliografía 
española de traducciones de novelas 1800-1850 (1982).
Antonio Palau y Dulcet. Manual del librero hispano-americano 
(1923-1927).
Ángel González Palencia. Estudio histórico sobre la censura 
gubernativa en España, 1800-1833 (1970, 1934)
Francisco Aguilar Piñal. Bibliografía de autores españoles del siglo 
XVIII (1981-1991).
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
275
How to convert paper 
archives into a digital data 
base? Problems and solutions 
in the case of the Morphology 
Archives of Finnish Dialects
Mari Siiroinen
mari.siiroinen@helsinki.fi 
University of Helsinki, Finland
Mikko Virtanen
mikko.virtanen@helsinki.fi 
University of Helsinki, Finland
Tatiana Stepanova 
tatjana.stepanova@helsinki.fi 
University of Helsinki, Finland 
This poster presentation and computer demonstration deals 
with some of the problems encountered and solutions found 
when converting paper archives containing linguistic data into 
a digital database.
The Morphology Archives of Finnish 
Dialects
The paper archives referred to are the Morphology Archives of 
Finnish Dialects, which contain about 500,000 paper fi le cards 
classifi ed and arranged on the basis of almost a thousand 
linguistically based catalogue codes. The data the fi le cards 
contain are derived from spontaneous dialectal speech by 
linguistically trained fi eld-workers. The data has been analysed 
and encoded to determine, for example, types of infl ection and 
word formation and their use in the sentence, sound changes 
in word stems, and particles and their related uses.
The data gathering was accomplished during a 30-year period 
(from the 1960s to the 1990s).
The paper archives are located in the Department of Finnish 
Language and Literature in the University of Helsinki. There are six 
copies of the data in the archives in six differ-ent universities 
and research institutions in Finland, Sweden and Norway. The 
Morphology Archives of Finnish Dialects are closely related to two 
other archives of Finnish dialects, namely the Lexical Archive of 
Finnish Dialects (Research Institute for the Languages of Finland) 
and Syntax Archives of Finnish Dialects (University of Turku).
The purpose of the Morphology Archives of Finnish Dialects has 
been to facilitate research on the rich morphology of Finnish 
and to provide researchers with well-organised data on the 
dialects of different parishes. The Archives cover all the Finnish 
dialects quite well since it consists of 159 parish collections 
equally distributed among Finnish dialects. The archive 
collections have served as data sources for over 300 printed 
publications or theses.
For additional information about the Archives, see www.
helsinki.fi /hum/skl/english/research/ma.htm.
The Digital Morphology Archives of 
Finnish Dialects
Plans to digitize the data in the Archives were fi rst made in the 
1990s. The digitization project fi nally got the funding in 2001. A 
project to create a digital database of Finnish dialects (Digital 
Morphology Archives, DMA) was then launched. The project was 
funded by the Academy of Finland, and it ended in 2005.
The digitization was not implemented simply by copying 
the paper archives, but the objective has been to create an 
independent digital archive, which also contains data not 
included in the paper archives, in particular to ensure suffi cient 
regional representation.
The Digital Morphology Archives currently contain 138,000 
clauses in context (around one million words) from 145 parish 
dialects of Finnish. So far a total of 497,000 morphological 
codes have been added to the dialectal clauses (approx. 4 
codes for each clause). In the parish collections, which are 
coded thoroughly, each example has been assigned from 5 to 
10 codes. This increase in the number of codes will improve 
the possibilities of using the DMA for research purposes. The 
Digital Morphology Archives are unique in that all the data is 
derived from spoken language.
The database was implemented using MySQL, while the search 
system is built on HTML. The data are stored in the Finnish 
IT Centre for Science (CSC) (http://www.csc.fi /) and has been 
accessible in current form via the internet to licensed users 
since 2005. Licences are granted to students and researchers 
upon request.
An internet search facility developed jointly with the Language 
Bank of Finland (CSC) allows quick and straightforward searches 
both from the entire material and from individual parishes or 
dialect areas. Searches can also be made directly from the 
dialect texts. These word and phrase searches can also be 
targeted at dialect texts without diacritic marks. Searches can 
also be refi ned by limiting them to certain linguistic categories 
according to a morphological classifi cation containing 897 
codes.
For additional information about the Digital Morphology 
Archives, see www.csc.fi /english/research/software/dma
Licence application form:
www.csc.fi /english/customers/university/useraccounts/
languagebank/?searchterm=language%20bank
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
276
Problems and Solutions
One of the problems encountered during the process has been 
that digitizing the data manually is very slow. In fact, the data 
in the digital data base still only cover about 5.5% of the paper 
archives. Scanning of the paper fi le cards has been proposed as 
a solution. The new challenge then would be to fi nd a powerful 
enough OCR program, as the paper cards have mostly been 
written by hand.
Another problem has been the presentation of Finno-Ugric 
phonetic transcription, which includes symbols and diacritics 
that are not part of the ISO-Latin-1 character-set. As Unicode 
is not yet supported by all programs, characters in the ISO-
Latin-1 character set were chosen to replace some of the 
Finno-Ugric phonetic symbols.
The second phase of digitization was launched in September 
2007, with new funding. By the end of 2009, it is estimated that 
at least 250,000 new dialectal clauses with linguistic coding will 
have been added to the digital archive.
A Bibliographic Utility for 
Digital Humanities Projects
James Stout
James_Stout@brown.edu
Brown University, USA
Clifford Wulfman
Clifford_Wulfman@brown.edu
Brown University, USA
Elli Mylonas 
Elli_Mylonas@brown.edu
Brown University, USA
Introduction
Many, if not most, digital humanities projects have a 
bibliographical component. For some projects the collection, 
annotation and dissemination of bibliographical information 
on a specifi c topic is the primary focus. The majority, however, 
include bibliography as a resource. Over the years, our group has 
worked on several projects that have included a bibliography. In 
each case, we were given bibliographical information to include 
at the outset, which the scholar working on the project would 
edit and amplify over time.
In the past we wrote several one-off bibliographic management 
systems, each for a particular project. Each tool was tailored 
to and also limited by the needs of its project. Over time we 
ended up with several bibliographic components, each slightly 
different in the way it was implemented and the way it handled 
data formats. We decided to settle upon a general purpose tool, 
in order to avoid writing any further single use applications, 
which were diffi cult and time consuming to support.
We were primarily interested in a tool that could handle many 
forms of bibliographic entity, from conventional scholarly 
materials, serials and manuscripts to multimedia and websites. 
It had to allow scholars to interact with it easily using a web 
interface for single record input and editing, and also be capable 
of loading large numbers of prepared records at once. We were 
less concerned with output and text-formatting capabilities, 
given the ability to produce structured XML output. We 
expected to allow the individual projects to query the system 
and format the output from the bibliographic tool.
Bibliographical management systems, although tedious to 
implement, are a well-understood problem, and we originally 
hoped to use a pre-existing tool. Upon surveying the fi eld, 
we realized that the available tools were primarily single-user 
citation managers, like EndNote, Bibtex, and Zotero. Our users 
were compiling and manipulating scholarly bibliographies, 
so we wanted something that was familiar to academics. 
However, we also wanted the fl exibility and overall standards 
compatibility of the library tools.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
277
A great deal of the diffi culty of bibliographic management 
arises out of the complexity of bibliographic references 
and the problem of representing them. Many libraries use a 
form of MARC to identify the different pieces of information 
about a book. MARC records are not ideal for storing and 
displaying scholarly bibliography; they aren’t meant to handle 
parts of publications, and they don’t differentiate explicitly 
among genres of publication. Many personal bibliographical 
software applications store their information using the RIS 
format, which provides appropriate categories of information 
for most bibliographic genres (e.g. book, article, manuscript, 
etc.). Hoenicka points out the fl aws in the underlying 
representation of RIS, but agrees that it does a good job of 
collecting appropriate citation information [Hoenicka2007a, 
Hoenicka2007b]. We wanted to be able to use the RIS 
categories, but not the RIS structures. We also hoped that we 
would be able to store our bibliographic information in XML, 
as we felt that it was a more versatile and appropriate format 
for loosely structured, repeating, ordered data.
Personal bibliography tools were not appropriate for our 
purposes because they were desktop applications, intended 
to be used by a single person, and were optimized to produce 
print output. At the time we were researching other tools, 
RefDB had a very powerful engine, but no interface to speak 
of. WIKINDX was not easily confi gurable and was not easy to 
embed into other projects. Both RefDB and WIKINDX have 
developed features over the last year, but we feel that the 
system we developed is different enough in ways that render 
it more versatile and easier to integrate into our projects. 
Although RefDB is the system that is closest to Biblio, it uses 
own SQL-based system for storing information about records, 
whereas we were looking for an XML based system. RefDB 
also allows the user to modify records in order to handle 
new genres of material. However, the modifi cations take place 
at the level of the user interface. As discussed below, Biblio 
provides full control over the way that records are being 
stored in the database. 
Based our survey of existing tools and the needs we had 
identifi ed for our projects, we decided to implement our own 
bibliographic utility. We made sure to have a fl exible internal 
representation, and a usable interface for data entry and 
editing. We left all but the most basic display up to the project 
that would be using the bibliography.
Implementation
Biblio is an XForms- and XQuery- based system for managing 
bibliographic records using the MODS XML format. The pages 
in the system are served by a combination of Orbeon, an 
XForms implementation, and eXist, a native XML database that 
supports XQuery. Using a simple interface served by eXist, 
the user can create, edit, import, export, and delete MODS 
records from a central collection in the database. When the 
user chooses to edit a record, they are sent to an Orbeon 
XForms page that allows them to modify the record and save 
it back to the database.
By basing the backend of our system on XML, we can take 
advantage of its unique features. Among the most useful is the 
ability to easily validate records at each point changes may 
have occurred. Validation begins when a document is fi rst 
imported or created, to ensure that no invalid document is 
allowed to enter the database. The document is validated 
against the standard MODS schema, which allows us to easily 
keep our system up to date with changes in the MODS format. 
Once a record is in the system, we must ensure that future 
editing maintains its integrity. Instead of simply validating upon 
saving a document, we use on-the-fl y XForms validation to 
inform the user immediately of any mistakes. The notifi cation 
is an unobtrusive red exclamation mark that appears next to 
the fi eld containing the error.
Although validation helps prevent mistakes, it does little to 
protect the user from being exposed to the complexity and 
generality of the MODS format. Because the MODS format is 
designed to handle any type of bibliographic record, any single 
record only needs a small subset of all the available elements. 
Fortunately, most records can be categorized into “genres”, 
such as “book” or “journal article”. Furthermore, each of these 
genres will have several constant elements, such as their title.
To maximize workfl ow effi ciency and easy of use, we have 
designed a highly extensible genre system that only exposes 
users to the options that are relevant for the genre of the record 
they are currently editing. The genre governs what forms the 
user sees on the edit page, and hence what elements they are 
able to insert into their MODS record. The genre defi nition 
can also set default values for elements, and can allow the user 
to duplicate certain elements or groups of elements (such as 
a <name> element holding an author). When a record is saved 
to the database, any unused elements are stripped away.
The genre system is also compatible with MODS records 
that are imported from outside the system. We simply allow 
the user to choose what genre best describes the imported 
record, which allows the system to treat the record as if it 
had been created by our system. The user can also select 
“Previously Exported” if they are importing a document that 
was created by our system, which will cause the appropriate 
genre to be automatically selected.
We have designed a simple XML format for creating genre 
defi nitions that makes it easy to add, remove, or change 
what genres are available for the user. The format allows 
the administrator to specify which elements are in a genre, 
what type of fi eld should be used (including auto-complete, 
selection, and hidden), what the default value of a fi eld should 
be, and whether the user should be allowed to duplicate a 
fi eld. All of our predefi ned genres also use this format, so it is 
easy to tweak the system to fi t the needs of any organization.
Finally, once users have accumulated a large set of records of 
various genres, we use the power of XML again to enable the 
user to search and sort their records on a wide and extensible 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
278
range of criteria. The administrator can easily specify new 
search and sort criteria for any element in the MODS schema, 
using the powerful XPath language.
Results
The bibliographic tool that we built provides an intelligent 
fi lter between the academic’s concept of bibliography and 
the cataloger’s concept of bibliographic record. We adopted 
the fundamental representation of a bibliographic entry, the 
MODS structure, from the library community because we 
want to use their tools and their modes of access. At the same 
time, we don’t expect scholars and their students to interact 
directly either with MODS as an XML structure or MODS as 
a cataloging system, so we mediate it through a RIS-inspired 
set of categories that inform the user interface. The choice of 
an XML structure also benefi ts the programmer, as this makes 
it very easy to integrate Biblio into an existing web application 
or framework. The resulting bibliographic utility has an easily 
confi gurable, easy to use data entry front end, and provides 
a generic, standards-based, re-confi gurable data store for 
providing bibliographic information to digital projects.
Bibliography
[BibTeX] BibTeX. http://www.bibtex.org/
[Endnote] Endnote. http://www.endnote.com
[Hoenicka2007a] Marcus Hoenicka. “Deconstructing RIS 
(part I)” (Blog entry). http://www.mhoenicka.de/system-cgi/
blog/index.php?itemid=515. Mar. 12, 2007
[Hoenicka2007b] Marcus Hoenicka. “Deconstructing RIS 
(part II)” (Blog entry). http://www.mhoenicka.de/system-cgi/
blog/index.php?itemid=567. Apr. 23, 2007.
[MODS] MODS. http://www.loc.gov/standards/mods/
[Orbeon] Orbeon. http://www.orbeon.com/
[RefDB] RefDB. http://refdb.sourceforge.net
[RIS] RIS. http://www.refman.com/support/risformat_intro.
asp
[Wikindx] WIKINDX. http://wikindx.sourceforge.net
[Zotero] Zotero http://www.zotero.org
A Digital Chronology of the 
Fashion, Dress and Behavior 
from Meiji to early Showa 
periods(1868-1945) in Japan
Haruko Takahashi
RXF13365@nifty.com
Osaka-Shoin Women’s University, Japan
Introduction
This paper describes a digital chronology with images of 
fashion, dress and behavior (Hereafter called FDB) from 1868 
to 1945 in Japan. This period when kimono and western style 
of dress contended with each other, was a very important 
time for Japanese clothing culture. Nevertheless there have 
been few timelines published of high credibility due to a lack 
of supporting evidence.   This chronology consists of 4000 
images and documents related to 8000 events of this period. It 
will be available on the Internet in the near future.
How this digital chronology came 
about
The National Museum of Ethnology Japan, the project of 
making a database of clothing culture in the world, which was 
named MCD ( Minpaku Costume Database ) was started in 
1984. Now it is available to the public through the museum’s 
website under the heading “CostumeDatabase” (http://www.
minpaku.ac.jp). 
This database consists of six sub-databases. Of these, fi ve are 
reference database and one is an integrated image database 
of the clothes and accessories collection of The National 
Museum of Ethnology. The number of items is 208,000 in the 
database.
To make this Costume Database, many reference data have 
been collected and analyzed. A lot of newspaper archives 
from 1868 to 1945 were in this collected reference data and 
I thought that a digital chronology could be created by using 
these archives. The articles and images came from 20 different 
newspapers such as The Yomiuri, Asahi, Mainichi, Tokyo-
nichinichi, Miyako, Kokumin, and Jiji.
The features of this digital chronology 
The features of this digital chronology are summarized as 
following four points (fi g.1). 
1) The events were chosen out of newspaper archives on 
the basis of 140 themes. These 140 themes, detailing the 
acculturation of FDB for about 80 years after the Meiji 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
279
Restorationwere set up by analyzing books and papers as 
well as newspaper archives. They include “Society’s Opinions 
of FDB”,  “Views on Health”,  “Views on the Body”,  “Dress 
Reform Problem and the Blending of Japanese and Western 
Styles”,  “Formal Dress”,  “Kimono”,“Makeup and Hairstyle”, 
etc., and “Road and Lighting” (Explaining their effect on 
FDB).  -_The book, Acculturation of fashion culture: behavior 
and dress from Meiji to early Showa periods_(Takahashi, pub.
Sangensha 2005) describes the argument for selecting these 
140 themes. 
2) Sources are described in all respects. As a result the 
timeline also functions as an index tool, and most events 
have supporting sources attached.
3) The chronology is divided into two columns entitled 
“Events” and “Contemporary Conditions”. These remove 
ambiguity in the descriptions of FDB from the general 
comprehensive timeline. The former, have precise dates 
given and therefore function in the same way as thegeneral 
timeline.   On the other hand, “Contemporary Conditions” 
in the latter show clothingculture more generally and 
include fads which cannot be connected to exact dates.
4) Alongside the description of each year of “Contemporary 
Conditions”, you can see images of “Scenes of 
Contemporary Life” “Men”, “Women”, “Children”, and 
“Beautiful Women”which are typical of the time.
These images came from the illustrations of newspapers which 
had served the same purpose as photographs in the second 
half of the 19th century.   These pictures were drawn to 
illustrate serial novels published in the papers.   They portray 
the joy, anger, humor and pathos of the people of those days. 
They also represented the culture of each class, such as the life 
of the empoverished which photographers of those days did 
not capture. These illustrations were drawn to help readers 
better understand the text.  However, if they didn’t accurately 
portray something, then eager readers would write to the 
newspapers to complain. This led to a high level of accurary 
on the part of the artists.
The credibility of these pictures as source is discussed in the 
paper Newspaper serial novel illustrations as a source of data of 
the Costume Image Database in the middle of the Meiji era. The 
Bulletin of Japan Art Documentation Society. (Takahashi,. 2005. 
p.1-15).
The system of this digital chronology
Application Capabilities 
This system uses Horiuchi Color’s iPallet/Kumu. This 
application’s capability is as follows.
1) It has the function which allows the user to smoothly 
increase the size of a detailed picture. The user can also 
examine fi ne details in the magnifi ed picture with a drug-
and-zoom function (Fig.2).
2) It supports Firefox 2 for Japanese on the Windows XP, 
Windows Vista, and MacOSX.
3) Users can search by title, year, and keyword.
4) The use of standard plug-ins (ex. Flash) is possible.
Display of Dates in <Contemporary 
Conditions>
The dates in grey on the left handside in <contemporary 
conditions> are approximate dates only. However, this is not 
the best way of representing approximate dates. Is it possible 
to visually express the rise and fall of fads?  That’s the big 
challenge for this system..
Conclusion
This digital chronology concretely shows the changes in FDB 
for about 80 years from 1868, and also faithfully details the 
clothing culture of those days. It would be helpful in themaking 
the movies and stage productions, and thus is much more than 
just a scientifi c reference text. 
This research is supported by Grants-in- Aid for Scientifi c 
Research (C) from the Japan Society for the Promotion of 
Science (JSPS) from 2006 to 2008.
Fig.1 Example of display
    Fig.2 Example of image data
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
280
Knight’s Quest: A Video 
Game to Explore Gender and 
Culture in Chaucer’s England
Mary L. Tripp
mtripp@mail.ucf.edu
University of Central Florida, USA 
Thomas Rudy McDaniel 
rudy@mail.ucf.edu
University of Central Florida, USA
Natalie Underberg 
nunderbe@mail.ucf.edu
University of Central Florida, USA
Karla Kitalong 
kitalong@mail.ucf.edu
University of Central Florida, USA
Steve Fiore
sfi ore@ist.ucf.edu
University of Central Florida, USA
A cross-disciplinary team of researchers from the College of 
Arts and Humanities (English, Philosophy, and Digital Media) 
at the University of Central Florida are working on a virtual 
medieval world based on Chaucer’s Canterbury Tales. The 
objectives are (1) that students experience a realistic virtual 
environment, with historical detail (buildings, music, artwork), 
and (2) that students view fourteenth century England from 
various perspectives, both as a master and the mastered, from 
the sacred to the profane. These objectives are accomplished 
through a knight’s quest game based on the Wife of Bath’s Tale. 
The Wife’s tale emphasizes the battle between men’s perceived 
authority and women’s struggles for power. In her tale, a knight 
must go on a quest to fi nd out what women most want. The 
Wife’s tale leaves a narrative gap in the knight’s quest, which is 
where our adaptation of Chaucer’s work is launched.
The knight interacts with Chaucerian characters on his journey-
-he meets everyday people like a cook, a reeve, a miller. In order 
to understand the perspectives of master and mastered, sacred 
and profane, the knight will dress as a woman to disguise his 
identity; enter a cathedral to converse with a priest; disguise 
himself as a peasant; and, later disguise as a monk to escape 
danger. Other characters are scripted to react differently 
to the noble knight when he is in disguise. This fetishism of 
clothing is reminiscent of several of Shakespeare’s comedies 
and tragedies, notably Merchant of Venice, Midsummer Night’s 
Dream, and King Lear. There are also two narrators, the knight 
operating from his perspective (male and noble) and the Wife 
(female and a widow), interrupting and offering her opinion on 
the action of the quest.
The initial phase, a Game Design Document, was completed 
in May 2007. A complete dialogue script for the storyline is 
scheduled for December 2007, and initial programming and 
modifi cation of the project, using the Elder Scrolls IV: Oblivion 
game engine to create this virtual world have also taken place 
through 2007. A paper prototype usability study is complete 
during November 2007 and a prototype visualization will be 
available by June 2008.
As the Canterbury Tales is traditionally taught at the high 
school level (Florida’s curriculum includes this under 12th 
grade English Literature), the target audience for this prototype 
is 14 to 16 year old players. Eight episodes of gameplay will run 
an average player about two hours to complete. This format 
has proven suitable for classroom use, where the teacher can 
use the game during a computer lab session, with independent 
student interaction time, or as a whole-class teacher-directed 
activity to be completed over 3 to 4 days. The objective here 
is not to teach the plot of Canterbury Tales—this information 
is readily available from a number of different sources. Our 
primary focus is to immerse the user in Chaucer’s historical 
world while educating the user on the historical facts behind 
the medieval era. These facts have been diluted over the years 
by popular fi ction. An additional mission of this game is to 
provide the player a variety of cultural perspectives through 
the use of disguise and interaction with members of the Three 
Estates of clergy, nobility, and peasantry and the feminine 
estates of virgin, wife and widow. This game is not developed 
to “teach” students the narrative structure of the Tales. It is 
designed with the literary theories of New Historicism and 
Cultural Studies as a basis for understanding the Tales as 
situated in a particular history and culture.
Current research in the fi elds of human learning and cognitive 
science speak to the exceptional ability of computer games to 
explore identity, problem solving skills, verbal and non verbal 
learning, and the transfer of learned ability from one task to 
another (Gee, 2003; Berman & Bruckman, 2001; Cassell, 1998; 
Fiore, Metcalf, & McDaniel, 2007; McDaniel, 2006; Jenkins, 2006; 
Ryan, 2001a, 2001b; Squire, 2002). In fact, learning games in the 
humanities have been used for hundreds of years--Bach’s Well 
Tempered Clavier and The Art of the Fugue are his “learning 
games,” simple to complex musical exercises that build skill 
(Prensky, 2000). Virtual worlds offer a unique and interesting 
way in which to observe critical relationships involving race, 
gender, identity, community, and history. Such relationships 
are clearly within the territory of humanities scholarship, and 
we believe video games will excite and motivate students to 
understand and engage with these complex topics in a fashion 
that is intuitive and exciting for them.
In terms of humanities-specifi c objectives, games can provide 
entry points for discussions and refl ections of all types of 
cultural and critical issues. The discussion of interactivity and 
the semantic quality of narrative and its application to digital 
media is an important aspect of computer game development. 
Ryan, in her article, “Beyond Myth and Metaphor—The Case 
of Narrative in Digital Media” concludes that computer 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
281
games offer the most suitable interface for digital narrative 
is compelling, especially in light of current applications in the 
fi eld (2001a).
Serious games can also foster new understanding among their 
players. In philosophy of the mind, Gadamer describes a fusion 
of horizons as the way humans understand. We, as human 
beings, are historical and we each possess a unique horizon. 
Each person has his own prejudice, history, and tradition, all 
within the context of his language. If we approach knowledge 
acquisition from Gadamer’s perspective, serious games are 
one of the best ways for students to more fully understand 
the humanities. Although Gadamer understands these new 
experiences as material interactions with the world, we 
propose that new understanding can take place through 
interactions with the virtual world as well.
Lorraine Code echoes some of the historical and cultural 
defi nitions of hermeneutical understanding that Gadamer 
proposes, she fi lters these ideas through the lens of feminism and 
her explanation of a mitigated defi nition of cultural relativism. 
In her essay “How to Think Globally: Stretching the Limits 
of Imagination,” she uses Shrage’s idea that relativism works 
mainly with ‘crosscultural comparisons’ to endorse her view 
that global understanding happens within a local context (Code, 
1998). Our team proposes that serious games allow the player 
to construct new perceptions of global understanding from a 
variety of social, gendered and cultural perspectives (Berman 
& Bruckman, 2001; Squire, 2002). Also, the production of video 
games offers an important learning opportunity for students 
involved in the multimodal production and representation of 
source-specifi c content, narrative, and gameplay experiences.
We believe that this project, the development of a virtual 
medieval world of Chaucer, can challenge learners to use 
this path toward creating a new kind of knowledge of the 
humanities. This new kind of knowledge in the humanities is 
not the traditional memorized litany of Western icons, nor 
a structure of literary genres and plots, but intensive and 
extensive meaningful interaction with the cultural and artistic 
achievements of humankind. This project is really what game 
based learning can be at its best, meeting the needs of a 
changing modern paradigm of understanding and basing its 
development in good theoretical research.
Representative Bibliography
Berman, J. & Bruckman, A. (2001). “The Turing Game: 
Exploring Identity in an Online Environment.” Convergence, 
7(3), 83-102.
Cassell, Justine. (1998). “Storytelling as a Nexus of Change 
in the Relationship between Gender and Technology: a 
Feminist Approach to Software Design.” From Barbie to Mortal 
Kombat: Gender and Computer Games. Cassell and Jenkins, eds. 
Cambridge: MIT Press.
Code, L. (1998). “How to Think Globally: Stretching the Limits 
of Imagination.” Hypatia, 13(2), 73.
Fiore, S. M., Metcalf, D., & McDaniel, R. (2007). “Theoretical 
Foundations of Experiential Learning.” In M. Silberman (Ed.), 
The Experiential Learning Handbook (pp. 33-58): John Wiley & 
Sons.
Gadamer H. G. (1991). Truth and Method. (2nd revised 
edition) (J. Weinsheimer & D. Marshall, Trans.). New York: 
Crossroad. (Original work published 1960).
Garris, R., R. Ahlers, et al. (2002). “Games, Motivation, and 
Learning: A Research and Practice Model.” Simulation Gaming 
33(4), 441-467.
Gee, J. P. (2003). What Video Games Have to Teach Us About 
Learning and Literacy. New York: Palgrave Macmillan.
Jenkins, H. (2006). Convergence Culture. New York: New York 
University Press.
McDaniel, R. (2006). Video Games as Text and Technology: 
Teaching with Multimodal Narrative. Paper presented at the 
9th Annual Conference of the Association for Teachers of 
Technical Writing in Chicago, IL. March 22, 2006.
Prensky, M. (2001). Digital Game-Based Learning. New York: 
McGraw-Hill.
Ricci, K. S., Eduardo; Cannon-Bowers, Janis (1996). “Do 
Computer-Based Games Facilitate Knowledge Acquisition 
and Retention?” Military Psychology 8(4), 295-307.
Ryan, Marie-Laure. (2001a) “Beyond Myth and Metaphor—
The Case of Narrative in Digital Media.” Game Studies 1(1). 
26 October 2007. <http://www.gamestudies.org/0101/ryan/>.
Ryan, Marie-Laure. (2001b) Narrative as Virtual Reality: 
Immersion and Interactivity in Literature and Electronic Media. 
Baltimore: Johns Hopkins University Press.
Squire, K. (2002). “Cultural Framing of Computer/Video 
Games.” Game Studies, 2(1).
Tripp, M. (2007). Avatar: From Other to Self-Transcendence 
and Transformation. Paper presented at the Philosophy Club 
of Winter Park, FL, 7 August.
Underberg, N (2006). “From Cinderella to Computer Game: 
Traditional Narrative Meets Digital Media in the Classroom.” 
Milwaukee, WI: American Folklore Society: 2006.
Unsworth, John. (2006). “Digital Humanities: Beyond 
Representation.” Lecture. University of Central Florida. 
Orlando, FL. 13 November.  
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
282
TEI by Example: Pedagogical 
Approaches Used in the 
Construction of Online 
Digital Humanities Tutorials
Ron Van den Branden
ron.vandenbranden@kantl.be
Centre for Scholarly Editing and Document Studies (KANTL), 
Belgium
Melissa Terras
m.terras@ucl.ac.uk
University College London, UK
Edward Vanhoutte
edward.vanhoutte@kantl.be
Centre for Scholarly Editing and Document Studies (KANTL), 
Belgium
Over the past 20 years, the TEI (Text Encoding Initiative) 
has developed comprehensive guidelines for scholarly text 
encoding (TEI, 2007). In order to expand the user base of TEI, 
it is important that tutorial materials are made available to 
scholars new to textual encoding. However, there is a paucity 
of stand-alone teaching materials available which support 
beginner’s level learning of TEI. Materials which are available 
are not in formats which would enable tutorials to be provided 
in blended learning environments (Allan, 2007) such as 
classroom settings, for instance, as part of a University course, 
or allow individuals to work through graded examples in their 
own time: the common way of learning new computational 
techniques through self-directed learning.
As a result, there is an urgent need for a suite of TEI tutorials 
for the self directed learner. The “TEI by Example” project is 
currently developing a range of freely available online tutorials 
which will walk individuals through the different stages in 
marking up a document in TEI. In addition to this, the tutorials 
provide annotated examples of a range of texts, indicating 
the editorial choices that are necessary to undertake when 
marking up a text in TEI. Linking to real examples from projects 
which utilise the TEI reaffi rms the advice given to learners.
The aims and focus of the project were documented in Van 
den Branden et al. (2007), whereas the aim of this poster is 
to detail the editorial, technological, and pedagogical choices 
the authors had to make when constructing the tutorials, to 
prepare stand- alone tutorials of use to the Digital Humanities 
audience, and beyond.
TEI by Example is effectively an implementation of problem 
based learning, an effi cient and useful approach to teaching 
skills to individuals in order for them to undertake similar 
tasks themselves, successfully. The literature on this is wide 
and varied (for seminal literature regarding the effectiveness 
of this pedagogic approach see Norman and Schmidt (1992); 
Garrison (1997); and Savin-Baden and Wilkie (2006)). There 
has been particular consideration as to the effectiveness of 
example and problem based learning when learning computer 
programming (for example, see Mayer (1981), Mayer (1988), 
Kelleher and Pausch (2005)). Additionally, another wide 
area of academic research is how to develop online tutorial 
materials successfully (Stephenson, 2001; Jochems et al., 2003). 
Understanding the nature of online tutorials, and grappling 
with the pedagogical issues these technologies offer us, was a 
core issue when beginning to implement the TEI by Example 
materials.
In order to develop the TEI by Example tutorials, the team 
had to understand the technical possibilities and limitations 
afforded by the online environment, and decide how best to 
integrate these into the tutorial materials. By juxtaposing static 
(pages, articles) and dynamic (quizzes, validation) functionality, 
the project aims to provide a holistic learning environment 
for those new to the TEI. Further linking to other examples 
provided by the community extends the remits of the project 
into another, alternative viewpoint by which to start learning 
the TEI, aside from the TEI guidelines themselves (TEI, 2007). 
Additionally, the role of user testing will be explored to feature 
feedback and comments from the TEI user community, to aid 
in the development of intuitive tutorial materials.
This poster will report on progress, problems, and potential 
solutions in developing teaching materials for the TEI, 
demonstrate the tutorials developed, and highlight areas in 
which the TEI and Digital Humanities communities can aid in 
the design, and implementation, of materials for students and 
teachers.
References
Allan, Barbara (2007). Blended Learning. Tools for teaching and 
training. London: Facet Publishing.
Garrison, D. R. (1997). “Self Directed Learning, Towards a 
Comprehensive Model”. Adult Education Quarterly, 48(1): 18-
33.
Jochems, W., van Merrienboer, J. and, Koper, R. (eds.) (2003). 
Integrated E-Learning: Implications for Pedagogy, Technology and 
Organization (Open & Flexible Learning). London: Routledge 
Farmer.
Kelleher, C. and Pausch, R. (2005). “Lowering the Barriers to 
Programming: A Taxonomy of Programming Environments 
and Languages for Novice Programmers”, ACM Computing 
Surveys, June 2005, 37(2): 83-137.
Mayer, R. (1981). “The Psychology of How Novices Learn 
Computer Programming” ACM Computing Surveys (CSUR), 
13(1): 121 - 141.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
283
Mayer, R. (1988). Teaching and Learning Computer Programming: 
Multiple Research Perspectives. Hillsdale, N.J: Lawrence Erlbaum 
Associates Inc.
Norman, G. R., and Schmidt, H. G. (1992). “The psychological 
basis of problem-based learning: a review of the evidence”. 
Acad Med, 67(9): 557-565
Savin-Baden, M. and Wilkie, K. (2006). Problem based learning 
online. Maidenhead: Open University Press.
Stephenson, J. (ed.) (2001). Teaching and Learning Online: 
Pedagogies for New Technologies. Abingdon: Routledge.
TEI (2007). Burnard, L. and Bauman, S. (eds). “TEI P5 
Guidelines for Electronic Text Encoding and Interchange” 
http://www.tei-c.org.uk/P5/Guidelines/index.html
Van den Branden, R., Vanhoutte, E., Terras, M. (2007). 
“TEI by Example”. Digital Humanities 2007, University of 
Illinois at Urbana-Champaign, USA, June 2007. http://www.
digitalhumanities.org/dh2007/abstracts/xhtml.xq?id=221
CLARIN: Common Language 
Resources and Technology 
Infrastructure
Martin Wynne 
martin.wynne@oucs.ox.ac.uk
Oxford University
Tamás Váradi 
varadi@nytud.hu
Hungarian Academy of Sciences
Peter Wittenburg 
Peter.Wittenburg@mpi.nl
Max Planck Institute for Psycholinguistics 
Steven Krauwer 
steven.krauwer@let.uu.nl
Utrecht University
Kimmo Koskenniemi 
kimmo.koskenniemi@helsinki.fi 
University of Helsinki
This paper proposes the need for an infrastructure to make 
language resources and technology (LRT) available and readily 
usable to scholars of all disciplines, in particular the humanities 
and social sciences (HSS), and gives an overview of how the 
CLARIN project aims to build such an infrastructure.
Why we need a research 
infrastructure for language resources
Problems of standards for textual representation, interoperability 
of tools, and problems with licensing, access and sustainability 
have dogged the Humanities since the invention of the digital 
computer. Language resources such as text corpora exhibit a 
variety of forms of textual representation, metadata, annotation, 
and access arrangements. Tools are usually developed for ad 
hoc use within a particular project, or for use by one group of 
researchers, or for use with only one text or set of data, and 
are not developed suffi ciently to be deployed as widely-used 
and sustainable services. As a result, a large amount of effort 
has been wasted over many years developing applications with 
similar functionality. Veterans of ACH and ALLC will know 
that the problems which are addressed by this paper are not 
new ones. What a persistent and sustainable infrastructure, 
as part of the e-Science and Cyberinfrastructure agenda, can 
offer is perhaps the fi rst realistic opportunity to address these 
problems in a systematic, sustainable and global fashion.
The Summit on Digital Tools in the Humanities in 
Charlottesville, Virginia in 2006 estimated that only 6% of 
scholars in the Humanities go beyond general purpose 
information technologies (email, web browsing, word 
processing, spreadsheets and presentation slide software), and 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
284
suggested that revolutionary change in humanistic research 
is possible thanks to computational methods, but that this 
revolution has not yet occurred. This is an exciting time in 
humanities research, as the introduction of new instruments 
makes possible new types of research, but it is clear that new 
institutional structures are needed for the potential to be 
realised.
CLARIN is committed to boost humanities research in a 
multicultural and multilingual Europe, by allowing easy access 
and use of language resources and technology to researchers 
and scholars across a wide spectrum of domains in the 
Humanities and Social Sciences. To reach this goal, CLARIN is 
dedicated to establishing an active interaction with the research 
communities in the Humanities and Social Sciences (HSS) and 
to contribute to overcoming the traditional gap between the 
Humanities and the Language Technology communities.
The CLARIN proposal
The proposed CLARIN infrastructure is based on the belief 
that the days of pencil-and-paper research are numbered, 
even in the humanities. Computer-aided language processing 
is already used by a wide variety of sub-disciplines in the 
humanities and social sciences, addressing one or more of the 
multiple roles language plays, as carrier of cultural content 
and knowledge, instrument of communication, component of 
identity and object of study. Current methods and objectives 
in these disparate fi elds have a lot in common with each other. 
However it is evident that to reach the higher levels of analysis 
of texts that non-linguist scholars are typically interested in, 
such as their semantic and pragmatic dimensions, requires an 
effort of a scale that no single scholar could, or indeed, should 
afford.
The cost of collecting, digitising and annotating large text or 
speech corpora, dictionaries or language descriptions is huge 
in terms of time and money, and the creation of tools to 
manipulate these language data is very demanding in terms 
of skills and expertise, especially if one wants to make them 
accessible to professionals who are not experts in linguistics 
or language technology. The benefi ts of computer enhanced 
language processing become available only when a critical 
mass of coordinated effort is invested in building an enabling 
infrastructure, which can then provide services in the form 
of provision of all the existing tools and resources as well 
as training and advice across a wide span of domains. Making 
resources and tools easily accessible and usable is the mission 
of the CLARIN infrastructure initiative.
The purpose of the infrastructure is to offer persistent services 
that are secure and provide easy access to language processing 
resources. Our vision is to make available in usable formats 
both the resources for processing language and the data to 
be processed, in such a way that the tasks can be run over 
a distributed network from the user’s desktop. The CLARIN 
objective is to make this vision a reality: repositories of data 
with standardized descriptions, language processing tools 
which can operate on standardized data, with a framework for 
th resolution of legal and access issues, and all of this available 
on the internet using Grid architecture.
The nature of the project is therefore primarily to turn 
existing, fragmented technology and resources into accessible 
and stable services that any user can share or customize for 
their own applications. This will be a new underpinning for 
advanced research in the humanities and social sciences - a 
research infrastructure.
Objectives of the current phase
CLARIN is currently in the preparatory phase, which 
has the aim of bringing the project to the level of legal, 
organisational and fi nancial maturity required to implement 
the infrastructure. As the ultimate goal is the construction 
and operation of a shared distributed infrastructure to make 
language resources and technology available to the humanities 
and social sciences research communities at large, an approach 
along various dimensions is required in order to pave the way 
for implementation. The fi ve main dimensions along which 
CLARIN will progress are the following:
• Funding and governance, bringing together the funding 
agencies in all participating countries and to work out 
a ready to sign draft agreement between them about 
governance, fi nancing, construction and operation of the 
infrastructure.
• Technical infrastructure, defi ning the novel concept of a 
language resources and technology infrastructure, based 
on existing and emerging technologies (Grid, web services), 
to provide a detailed specifi cation of the infrastructure, 
agreement on data and interoperability standards to be 
adopted, as well as a validated running prototype based on 
these specifi cations.
 • Languages and multilinguality, populating the prototype 
with a selection of language resources and technologies for 
all participating languages, via the adaptation and integration 
of existing resources to the CLARIN requirements, and in a 
number of cases the creation of specifi c essential resources.
• Legal and ethical issues relating to language resources will 
have to be examined and thoroughly understood, and the 
necessary legal and administrative agreements proposed to 
overcome the barriers to full exploitation of the resources.
• Focus on users, the intended users being the humanities 
and social sciences research communities.
This fi nal dimension is in many ways the most important, and 
will be explored in the most detail in this paper. In order to 
fully exploit the potential of what language technology has to 
offer, a number of actions have to be undertaken: (i) an analysis 
of current practice in the use of language technology in the 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
285
humanities will help to ensure that the specifi cations take into 
account the needs of the humanities, (ii) the execution of a 
number of exemplary humanities projects will help to validate 
the prototype and its specifi cations, (iii) the humanities and 
social sciences communities have to be made aware of the 
potential of the use of language resources and technology to 
enable innovation in their research, and (iv) the humanities and 
language technology communities have to be brought together 
in networks in order to ensure lasting collaborations between 
the communities. The objective of this cluster of activities is 
to ensure that the infrastructure has been demonstrated to 
serve the humanities and social sciences users, and that we 
create an informed community which is capable of exploiting 
and further developing the infrastructure.
Concluding remarks
CLARIN still has a long way to go, but it offers an exciting 
opportunity to exploit the achievements of language and 
speech technology over the last decades to the benefi t 
of communities that traditionally do not maintain a close 
relationship with the latest technologies. In contrast to many 
European programmes, the main benefi ciaries of this project 
are not expected to be the big ICT-oriented industries or the 
bigger language communities in Europe. CLARIN addresses 
the whole humanities and social sciences research community, 
and it very explicitly addresses all the languages of the EU 
and associated states, both majority and minority languages, 
including languages spoken and languages studied in the 
participating countries. 
 
Fortune Hunting: Art, 
Archive, Appropriation
 Lisa Young
Lisa_Young@brown.edu
Brown University, USA
James Stout
James_Stout@brown.edu
Brown University, USA
Elli Mylonas 
Elli_Mylonas@brown.edu
 Brown University, USA 
Introduction
Although digital humanities projects have a variety of forms 
and emphases, a familiar type is the dataset that is prepared - 
encoded, structured, and classifi ed - to allow scholars to engage 
with a research question. Such projects base the rational for 
their encodings and classifi cation on earlier research, on 
disciplinary knowledge from the subject area and on encoding 
practice. The choices are then documented so researchers are 
aware of the assumptions underlying their dataset. The user 
interface that provides access to the data is also designed to 
privilege particular audiences and modes of interaction.
Most projects our group undertakes follow this model, and 
we are familiar with the process of elucidating the information 
necessary in order to implement them. Fortune Hunting, 
however, an art project, appropriates the forms of the archive 
and the methodologies of literary analysis to express a 
personal vision. The artist developed her representations of 
the fortune cookie texts without any knowledge of the work 
in digital humanities; she took her initial inspiration from the 
library catalog and simple desktop databases. As we proceeded 
with the web implementation of this art project, we became 
increasingly aware of the similarity of the artist’s interactions 
to the encoding and classifi cation tasks we used in other 
projects, and we drew on that paradigm. As she was introduced 
to these methodologies, classifi cations and analytical tools, the 
artist also discovered new ways of working with her texts.
Artist’s Statement
The seemingly personalized fortune-cookie fortune is in fact 
a mass-produced item distributed to restaurants in batches. 
This random delivery system means that the fortune a given 
diner receives is mostly a function of chance. Even with this 
knowledge, many diners read their fortunes hoping to fi nd 
an answer to a pressing question or recognize some part of 
themselves.
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
286
Over a period of years, I methodically saved the fortune-cookie 
fortunes I received while dining out. I curated my collection 
intuitively, keeping fortunes that presented an “accurate” 
description of myself (“you are contemplative and analytical 
by nature”), offered a sense of hope (“you will be fortunate in 
everything you put your hands to”), or evoked a rueful sense 
of misidentifi cation (“you have great physical powers and an 
iron constitution”).
I wanted to examine my desire to fi nd structured meaning 
in what was really a haphazard or random occurrence. After 
some thought, I decided a digital archive that could sort the 
fortunes in a systematic way would be the perfect lens through 
which to read them. My work with the Brown University 
Scholarly Technology Group started with this germ of an idea. 
The result is the mini corpus “Fortune Hunting.”
“Fortune Hunting” allows users to create an endlessly evolving 
series of narratives that explore both self-defi nition and chance, 
the incomplete and transitory nature of being and desired 
states of being, and the shifting nature of the personal pronoun 
“you.” The anticipation, unexpected surprise (or possibly even 
disappointment) users experience as they construct searches 
continually enacts the same desiring mechanism (the need to 
construct meaning) that is as at the very root of the project.
As I began to work I realized that most fortunes were directed 
toward the diner through the use of the word “you.” I sorted 
the fortunes into three categories: “subject” you, “object” you 
and “without” you. Within these categories, sorts were further 
specifi ed by personal pronoun phrases. For example, all the 
fortunes that contain “you are” (subject you) can be seen 
together. Similarly, all the fortunes that contain “about you” 
(object you) can be seen together. A third method of searching 
(“without” you) captured all fortunes not containing the word 
“you” (example: art is the accomplice of love.)
At the same time, I became aware of the linguistic connections 
the search engine could manifest (for example: capturing all 
the fortunes that contained the word “love”). As a result, we 
created an individual word sort function. Words could be 
selected using the “exact word” search (for example: every) or 
a “similar word” search (which would yield every, everybody, 
everyone, and everything). The word sort function allowed 
fortunes to be sorted across multiple “you” categories.
As the database evolved, I became interested in ways that I 
could step outside its grammatical and linguistic parameters. 
At that point we developed a feature that allowed me to create 
“collections” of fortunes centered on my own subjective 
interpretations. These collections were divided into three 
categories: subjects (collections of fortunes on a particular 
topic such as travel, love or luck), portraits (clusters of fortunes 
that described types of individuals: writer, paranoid, neurotic 
overachiever), and narratives (groups of fortunes that created 
short stories: searching for happiness, reconciliations, revenge). 
Through analysis and intuition, my collection sorts created 
connections (both thematic and linguistic) that would not 
necessarily occur using either “you” or keyword searches.
Another important aspect of the database was incorporating 
the actual images of the fortunes. From the start, I was drawn 
to the ephemeral appearance of the fortunes: messages on 
paper scraps marked by stains and creases. With this in mind, 
I chose to scan the fortunes and have them appear on screen 
as images Instead of creating a strictly text-based interface. 
After users have completed their searches, they can view their 
results on a “picture page” which displays the images of the 
fortune cookie fortunes, or an “archive page” which displays 
the metadata attached to each fortune (allowing viewers 
another way to trace interconnections between fortunes). 
Lastly, viewers can move to a printer friendly page and print 
out their results, leaving each visitor with a visual record of 
their travels through the database.
“Fortune Hunting” is like going through a trunk in an attic, 
sorting through a collection of someone else’s things and 
making your own connections and re-readings. Because all the 
searches are Boolean “or” searches users can cast a wide net 
as they comb the database. “Fortune Hunting” is constantly 
evolving, and each visit to the site can involve a different 
interpretation of the material. New fortunes and subsequent 
subject, portrait and narrative searches continue to be added.
Digital Humanist’s Statement
Fortune Hunting is an art project; the artist was inspired by 
her reactions to fortune cookie texts to create a piece that 
would engender a similar experience in the viewer. The form 
she chose to exhibit her work, even before she began to work 
on its digital incarnation, highlighted her efforts to classify and 
inter-relate the fortunes. She created a large wall display, on 
which the fortunes were listed in columns that represented 
categories, with lines linking fortunes from different groups. 
Young’s desire to explore the discovery of meaning from 
the randomness of the fortunes led her to quantitative tools 
and methods that are  usually applied to carefully assembled 
and encoded archives. The “Fortune Hunting” database and 
website embody two competing modes of interaction: on the 
one hand, tool based interactions allow users to determine 
their own browsing strategy, using searches, keywords and 
a visualization, mimicking the artist’s own experience of the 
materials. On the other hand, users can view the results of the 
artist’s interpretations, a re-organization of the archive out of 
which she creates new meaning in the form of narratives and 
impressions.
Young developed tagging and classifi cation as a way to interpret 
her fortune archive independently of her collaboration with a 
digital humanities group, but her use of these linguistic and 
analytical tools is essentially a misappropriation. The archive has 
been carefully compiled, but is based on personal evaluation, 
and oriented to creative ends rather than scholarship. The 
classifi cations that are applied to the texts are also subjectively 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
287
based. The formats and activities resemble conventional 
digital humanities research, but for a different purpose - the 
pleasurable discovery of meaning in the juxtaposition of 
essentially random artifacts.  The whole analytical structure Is 
at the same time misused and exceedingly productive. 
Like Ramsay’s explorations of literary analysis [Ramsay2003], 
and the playful distortions of interpretation that we fi nd in 
the Ivanhoe Game and Juxta [ARPProjects], this project has a 
liberating effect on the digital humanist. Just as Young plays with 
the fortune texts in order to understand how they “work,” we 
are also playing with the subjective and exploratory application 
of our methods. The creative use of these tools allows us to 
focus on different parts of the discovery process, and we 
derive pleasure from the functioning of the tools themselves. 
In its similarity and great difference from conventional digital 
humanities projects, “Fortune Hunting” makes us self-conscious 
about our own practice.
Bibliography
[ARP] Applied Research in Patacriticism. http://www.
patacriticism.org/
[ARPProjects] Applied Research in Patacriticism. Projects. 
http://www.patacriticism.org/projects.html
[FortuneHunting] Fortune Hunting Web Site. http://www.
fortunehunting.org (site publication date: Dec.14, 2007)
[Ramsay2003] Ramsay, Stephen. “Toward an Algorithmic 
Criticism,” Literary and Linguistic Computing 18.2 (2003)
[TAPoRTools] Tapor Tools. http://portal.tapor.ca/portal/
coplets/myprojects/taporTools/
[VisualCol] Visual Collocator. http://tada.mcmaster.ca/Main/
TAPoRwareVisualCollocator
 
 
 
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
288
Index of authors
Allison, Sarah ...........................................................................13, 15
Anderson, Deborah Winthrop .................................................... 41
Andreev, Vadim ............................................................................... 42
Antoniuk, Jeffery ............................................................................ 70
Armstrong, Karin ...............................................................145, 241
Arnaout, Georges .......................................................................... 44
Audenaert, Neal .....................................................................47, 50
Balazs, Sharon ............................................................................... 70
Baumann, Ryan ........................................................................... 5, 8
Beavan, David ................................................................................ 53
Benavides, Jakeline ....................................................................... 55
Biber, Hanno .................................................................................. 57
Birkenhake, Benjamin ............................................................... 233
Blackwell, Christopher .............................................................5, 10
Blake, Analisa .............................................................................. 145
Blanke, Tobias ................................................................................. 60
Boot, Peter ................................................................................30, 62
Bradley, John ...............................................................................1, 65
Breiteneder, Evelyn ....................................................................... 57
Brey, Gerhard ................................................................................. 68
Brocca, Nicoletta ........................................................................ 187
Brown, Susan ...........................................................................35, 70
Buchmüller, Sandra ...................................................................... 72
Buzzetti, Dino .........................................................................29, 78
Carlin, Claire ................................................................................ 124
Chow, Rosan ................................................................................... 72
Christodoulakis, Manolis ............................................................. 68
Ciula, Arianna ................................................................ 30, 81, 235
Clements, Patricia ......................................................................... 70
Coartney, Jama .............................................................................. 84
Connors, Louisa ............................................................................. 86
Cools, Valérie ................................................................................ 109
Cooney, Charles ........................................... 89, 91, 93, 179, 237
Csernoch, Maria ............................................................................ 95
Cummings, James ...........................................................30, 97, 99
Czmiel, Alexander ...................................................................... 101
Daelemans, Walter .................................................................... 146
Dalmau, Michelle ....................................................................... 216
David, Stefano............................................................................. 103
Dik, Helma .........................................................................105, 107
Downie, Stephen ........................................................................ 239
Dubin, David .................................................................................. 24
Dué, Casey .................................................................................... 5, 6
Dunn, Stuart ............................................................................29, 60
Dyens, Ollivier ............................................................................. 109
Ebbott, Mary ................................................................................ 5, 6
Eder, Maciej ................................................................................. 112
Ehmann, Andreas ....................................................................... 239
Eide, Øyvind .......................................................................... 22, 115
Elkink, Michael ........................................................................... 241
Evmenov, Dmitri ......................................................................... 243
Fachry, Khairun Nisa ................................................................. 226
Fahmi, Ismail ............................................................................... 244
Ferragina, Paolo .......................................................................... 246
Fiore, Stephen ....................................................................154, 280
Fiormonte, Domenico ......................................................... 12, 187
Fisher, Claire ................................................................................ 218
Forest, Dominic ...........................................................34, 109, 163
Fragkouli, Elpiniki .............................................................................1
Fraistat, Neil ................................................................................... 12
Furuta, Richard ................................................... 47, 50, 157, 160
Goldfi eld, Joel .............................................................................. 117
Groß, Nathalie ............................................................................ 120
Grundy, Isobel ................................................................................ 70
Gärtner, Kurt ........................................................................ 30, 122
Hanlon, Ann ................................................................................. 189
Harkness, Darren James.......................................................... 204
Hedges, Mark ................................................................................ 60
Hinrichs, Erhard ............................................................................ 27
Holmen, Jon .................................................................................... 22
Holmes, Martin ..........................................................30, 124, 127
Honkapohja, Alpo ...................................................................... 132
Hoover, David ..................................................... 31, 34, 117, 134
Horton, Russell ............................................ 89, 91, 93, 179, 237
Hughes, Lorna ........................................................................31, 60
Hunyadi, Laszlo ............................................................................. 29
Hyman, Malcolm ....................................................................... 136
Isolani, Alida ................................................................................. 246
Ivanovs, Aleksandrs .................................................................... 210
Jannidis, Fotis ............................................................................... 138
Jockers, Matthew ...................................................................13, 34
Johnson, Ian .......................................................................... 12, 139
Johnson, Anthony ........................................................................ 249
Johnston, David ........................................................................... 109
Joost, Gesche .................................................................................. 72
Juola, Patrick ................................................................34, 229, 250
Kaislaniemi, Samuli .................................................................... 132
Kamps, Jaap ................................................................................ 226
Karsikas, Mari ............................................................................ 169
Keating, John ......................................................................141, 264
Kelly, Jennifer ............................................................................... 141
King, Katie ....................................................................................... 35
Kitalong, Karla ...................................................................154, 280
Koskenniemi, Kimmo ................................................................. 283
Krauwer, Steven .......................................................................... 283
Krefting, Rebecca .......................................................................... 35
Kretzschmar, William ................................................................ 143
Krivoruchko, Julia ....................................................................... 252
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
289
Lauersdorf, Mark ..............................................................271, 272
Lee, Jin Ha ................................................................................... 239
Leitch, Caroline ........................................................................... 145
Liedtke, Christian........................................................................ 120
Lindemann, Marilee ..................................................................... 35
Litta Modignani Picozzi, Eleonora ......................................... 252
Lombardini, Dianella ................................................................. 246
Lopez, Tamara ...................................................................... 81, 235
Lucchese, George .......................................................................... 50
Luyckx, Kim ................................................................................. 146
Lüngen, Harald ........................................................................... 254
Lyons, Mary Ann ......................................................................... 264
Mahony, Simon ........................................................................... 149
Mainoldi, Ernesto ....................................................................... 256
Mallen, Enrique .................................................................157, 160
Maly, Kurt ....................................................................................... 44
Mandell, Laura ............................................................................... 35
Marttila, Ville ............................................................................... 132
McCarty, Willard......................................................................... 173
McDaniel, Rudy .................................................................154, 280
Meister, Jan-Christoph ................................................................. 12
Mektesheva, Milena ..................................................................... 44
Meneses, Luis .....................................................................157, 160
Meunier, Jean-Guy .............................................................. 34, 163
Miyake, Maki .............................................................................. 258
Mondou, Patric ........................................................................... 109
Monroy, Carlos ............................................................................ 157
Morrissey, Robert ....................................................................... 179
Moshell, Michael ........................................................................ 154
Musolino, Giulia .......................................................................... 223
Mylonas, Elli .......................................................................276, 285
Myojo, Kiyoko .............................................................................. 260
Mäkinen, Martti ......................................................................... 152
Mörth, Karlheinz ........................................................................... 57
Nagasaki, Kiyonori ..................................................................... 262
Newton, Greg .............................................................................. 127
Noecker, John .............................................................................. 250
Norrish, Jamie ............................................................................. 166
Nucci, Michele ............................................................................ 103
O’Connor, Thomas ...................................................................... 264
Olsen, Mark .................................................. 89, 91, 93, 179, 237
Opas-Hänninen, Lisa Lena .............................................169, 249
Ore, Christian-Emil ............................................................. 22, 115
Palmer, Carole ................................................................................ 24
Pantou-Kikkou, Eleni .................................................................. 173
Pascucci, Giuliano ....................................................................... 266
Piazza, Francesco ....................................................................... 103
Pierazzo, Elena ..................................................................252, 268
Pizziconi, Sergio .......................................................................... 223
Porter, Dorothy.............................................................. 5, 271, 272
Pytlik-Zillig, Brian ........................................................................ 171
Radzikowska, Milena ................................................................... 16
Ramsay, Stephen .......................................................... 16, 20, 171
Rehbein, Malte .............................................................................. 78
Rehm, Georg ...........................................................................21, 27
Reis, Marga .................................................................................... 27
Renear, Allen ...............................................................................1, 24
Robey, David............................................................................29, 31
Rockwell, Geoffrey ..................................................................... 173
Rodríguez, Nuria ........................................................................ 176
Roe, Glenn .................................................... 89, 91, 93, 179, 237
Rouhier-Willoughby, Jeanmarie .............................................. 271
Rudman, Joseph ......................................................................... 181
Ruecker, Stan .................................................... 16, 18, 32, 33, 70
Rueda, Ana ................................................................................... 272
Ryan, Mike ................................................................................... 250
Rybicki, Jan................................................................................... 184
Sarlo, Bruno ................................................................................. 221
Scaife, Ross ......................................................................... 5, 8, 190
Schiavinotto, Tommaso .............................................................. 246
Schlitz, Stephanie ....................................................................... 185
Schmidt, Desmond .................................................................... 187
Scholing, Peter ............................................................................. 244
Schraefel, Monica ............................................................................1
Schreibman, Susan ...................................................... 30, 34, 189
Seales, Brent ....................................................................... 5, 8, 190
Seppänen,Tapio ......................................................................1, 169
Shapiro, Joe ..............................................................................13, 14
Sherrick, Grant............................................................................... 50
Short, Harold .................................................................................. 31
Siemens, Lynne ........................................................................... 193
Siemens, Ray ................................................ 30, 32, 34, 145, 241
Siiroinen, Mari ............................................................................. 275
Silvi, Daniele ................................................................................ 223
Sinclair, Stéfan .................................................................16, 18, 34
Smith, Neel .................................................................................5, 10
Smith, Martha Nell ...................................................................... 35
Soms, Henrihs............................................................................. 210
Spiro, Lisa ..................................................................................... 194
Stepanova, Tatiana ..................................................................... 275
Stevenson, Alison ........................................................................ 166
Stout, James ........................................................................276, 285
Sugo, Shin’ichiro .......................................................................... 260
Suzuki, Takafumi ........................................................................ 196
Tabata, Tomoji ............................................................................. 199
Takahashi, Haruko .................................................................... 278
Terras, Melissa ...................................................................218, 282
Tiinanen, Suvi .............................................................................. 169
Tripp, Mary ...............................................................154, 202, 280
Underberg, Natalie ...........................................................154, 280
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
290
Unsworth, John ................................................................12, 16, 31
Urban, Richard............................................................................... 24
Uszkalo, Kirsten ........................................................... 32, 33, 204
Vaahtera, Jyri ............................................................................... 249
Walker, Brian .............................................................................. 212
Walsh, John ..................................................................30, 214, 216
Van den Branden, Ron .....................................................206, 282
van den Heuvel, Charles ............................................................. 55
Vanhoutte, Edward ...........................................................208, 282
Váradi, Tamás .............................................................................. 283
Varfolomeyev, Aleksey................................................................ 210
Warwick, Claire ..................................................................32 , 218
Whaling, Richard ...............................................................105, 107
Wickett, Karen ............................................................................... 24
Wiersma, Wybo .......................................................................... 221
Wiesner, Susan .............................................................................. 84
Viglianti, Raffaele........................................................................ 268
Willinsky, John ............................................................................. 145
Virtanen, Mikko .......................................................................... 275
Vitt, Thorsten ............................................................................... 138
Witt, Andreas ...................................................... 21, 27, 233, 254
Wittenburg, Peter ....................................................................... 283
Wong, Amelia ................................................................................. 35
Worthey, Glen ................................................................................ 13
Voyer, Robert ................................................ 89. 91. 93. 179. 237
Wu, Harris ...................................................................................... 44
Wulfman, Clifford ....................................................................... 276
Wynne, Martin ........................................................................... 283
Young, Lisa ................................................................................... 285
Zanasi, Marco ............................................................................. 223
Zhang, Junte .......................................................................226, 244
Zhao, Mengjia ....................................................................229, 250
Zubair, Mohammad ..................................................................... 44
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
291
Digital Humanities 2008_____________________________________________________________________________
_____________________________________________________________________________
CCXCII