The	Artificial	Intelligence	(AI)	
Hermeneutic Network:  Toward 
an Approach to Analysis and 
Design of Intentional Systems 
Jichen Zhu 
Georgia Institute of Technology 
jichen.zhu@lcc.gatech.edu
 
D. Fox Harrell, Ph.D.  
Georgia Institute of Technology  
fox.harrell@lcc.gatech.edu	
‘I felt that I should be able to get the computer to sound 
good more or less on its own, so that someone listening 
to	it	says,	“Who	is	that	playing?”	But	if	you	get	“What’s	
that?”	instead,	you	have	to	go	back	to	the	drawing	board.’	
(Lewis, 2000) 
Abstract 
Digital information technologies are increasingly being adopted in the humanities as both research 
tools and supports for new forms of cultural expression. 
Some	of	these	digital	technologies,	in	particular	artificial	
intelligence (AI) programs, exhibit complex behaviors 
usually seen as the territory of intentional human phe-
nomena, such as creativity, planning and learning. This 
paper	identifies	a	prototypical	subset	of	these	programs,	
which we name intentional systems, and argues that their 
seemingly intentional behaviors are not the sole effect of 
underlying algorithmic complexity and knowledge engi-
neering practices from computer science. In contrast, we 
argue	(paralleling	the	field	of	software	studies)	that	in-
tentional systems, and digital systems at large, need to be 
analyzed as a contemporary form of historically, cultur-
ally, socially, and technically situated texts. Perception 
of system intentionality arises from a network of con-
tinuous	meaning	exchange	between	system	authors’	nar-
ration	and	users’	interpretation	processes	embedded	in	a	
broader social context. The central contribution of this 
paper is a new interdisciplinary analytical framework 
called the AI hermeneutic network that is informed by 
traditions of hermeneutic analysis, actor-network theory, 
cognitive semantics theory, and philosophy of mind. To 
illustrate the design implication of the AI hermeneutic 
network, we present our recent work Memory, Reverie 
Machine, an expressive intentional system that generates 
interactive narratives rich with daydreaming sequences. 
Intentional Systems 
Trombonist	 and	 composer	 George	 Lewis’s	 above	 de-
DIGITAL HUMANITIES 2009
Page 302
scription of his interactive musical system Voyager ex-
emplifies	a	growing	number	of	digital	systems,	such	as	
the autonomous painting program AARON (Cohen 2002) 
and recent computational narrative works (Harrell 2006; 
Mateas	&	Stern	2002;	PŽrez	y	PŽrez	&	Aliseda	2006),	
that utilize AI techniques in pursuit of cultural expres-
sion. Decades after heated debates about the feasibility 
of AI, the question of whether computers may one day 
possess human-level intelligence no longer spurs soci-
ety’s	fear	and	curiosity.	Instead,	systems	are	designed	to	
encourage users to make sense of them as intentional and 
independent entities. Compared to instrumental, produc-
tion-oriented programs such as the PhotoShop, these 
systems display intentional behaviors related to human 
mental phenomena such as planning, learning, narrating, 
and creating, as if their actions were about something in 
the world (Searle 1983) rather than mere execution of 
algorithmic rules. Lewis, for instance, insists that Voy-
ager ‘not [be] treated as a musical instrument, but as an 
independent	 improviser.’	 He	 deliberately	 designed	 the	
system to display independent behaviors arising from 
its own internal processes that even its designer cannot 
fully anticipate. The improvisational dialogue between 
Voyager and the musicians, Lewis emphasizes, is ‘bi-di-
rectional	transfer	of	intentionality	through	sound.’	com-
putational complexity, 2) process opacity, 3) human-like 
coherent behaviors, and 4) execution of authorial inten-
tion. The term encompasses not only AI systems but also 
AI-like systems that exist either outside of computer sci-
ence communities or are not described by their authors 
as AI systems for ideological or other reasons. Critical 
analysis and design of intentional systems, like informa-
tion technologies at large in the digital humanities, calls 
for the recognition of these systems as important forms 
of cultural production, beyond their traditionally instru-
mentalized, productivity oriented roles. 
Intentional Systems as Texts 
Although generally used to describe written forms of dis-
course, the term text as the object of literary theory and 
modern	hermeneutics	 is	not	confined	 to	only	 linguistic	
forms. In his essay on the literary text, German philoso-
pher Manfred Frank (Frank 1989) criticizes the notion 
that meanings that authors encode within texts can be 
objectively retrieved without distortion by readers given 
appropriate methods of interpretation (Hirsch 1967). In-
stead, Frank proposes a complex communication process 
in which both author and reader actively create, shape, 
and reconstruct meanings. This echoes the even broader 
notion of dialogic meaning posited by the Russian phi-
losopher and critic Mikhail Bakhtin in which language 
is understood as dynamic, contextual, intertextual, and 
relational (Holquist 1990). Acknowledging the textuality 
of intentional systems opens up understanding of system 
intentionality to a range of socially situated methods. 
Intentional systems are not simply the result of clever 
algorithmic and data structural innovations. The AI 
practitioner and theorist Philip Agre cogently points out 
that the ‘the purpose of AI is to build computer systems 
whose operation can be narrated using intentional vocab-
ulary.’	(Agre	1997)	Michael	Mateas,	co-developer	of	Fa-
çade, further deconstructs the codes invoked in AI prac-
tice	by	computation,	and	definitions	of	system	progress)	
and	 the	co-existing	‘code	machine’	(including	physical	
processes, computational processes, and complex causal 
flow),	in	order	to	pin	down	the	long-neglected	social	and	
discursive aspect of AI systems (Mateas 2002). In ad-
dition to considering actual computer programs, analy-
sis	 of	 intentional	 systems	 should	not	omit	 the	 authors’	
publications, presentations, and interpersonal communi-
cation about the system. Such narrative outputs situate 
the	system	in	AI	research	communities	and	frame	users’	
interpretation, and therefore must be considered as part 
of the intentional system. 
The AI Hermeneutic Network 
The central contribution of this paper is the AI herme-
neutic network model, enabled by theorizing intentional 
systems as texts. The interdisciplinary framework ana-
lyzes system intentionality as a result of a hermeneutic 
communication	process	that	involves	both	authors’	nar-
rations	 and	 users’	 interpretations	 through	 interaction	
with	both	actual	systems	and	authors’	narrative	output.	
In addition, this paper recognizes that intentional sys-
tems exist in broader social contexts that involve more 
than just authors and users. Animate and inanimate ac-
tors,	 called	 ‘actants’	 in	 actor-network	 theory	 (Callon	
1986; Latour 1996), participate in the network through 
multi-directional communication. Government and mili-
tary funding, for instance, often plays a prominent role in 
determining direction and validity of different approach-
es of AI research. 
Historically, hermeneutic studies developed interpreta-
tive theories and methods in order to recover the mean-
ings of sacred texts intended by the (divine) author(s). 
Modern	 hermeneutics,	 influenced	 by	 Schleiermacher,	
recognizes that everything calls for the work of inter-
pretation and broadens itself to the philosophical inter-
rogation of interpretation (Virkler 1981). This paper 
highlights	 discursive	 ‘elasticity’	 of	 the	AI	 key	 words,	
such as planning (Agre 1997). He observes that these 
key terminologies are simultaneously precise (formal) 
and vague (vernacular), which allows AI practitioners 
to seamlessly integrate their everyday experience as em-
bodied intentional being in the algorithmic research, and 
to narrate computation with popularly accessible ver-
DIGITAL HUMANITIES 2009
Page  303
nacular vocabulary.  
One relatively unexplored aspect of this continuous ne-
gotiation of values and meanings between both human 
and	computational	 actors	 (Latour	1996)	 is	users’	 read-
ings and interpretations of intentionality from systems 
that are clearly inanimate. For example, human co-
performers	 and	 their	 audiences’	 interpretations	of	Voy-
ager’s	behaviors	as	intentional	are	central	to	construe	the	
systems’	status	as	an	independent	performer	in	its	own	
right, as intended by its designer. Frank argues that ‘[i]
n the understanding of its readers the text … acquires a 
meaning	which	exceeds	the	memory	of	its	origin.’(Frank	
1989) Any analysis of system intentionality then is not 
adequate without considering participation of users and 
audiences.  
This paper emphasizes the discursive strategy and se-
mantic interpretation from a cognitive linguistics per-
spective. Conceptual blending theory (Fauconnier 2001; 
Fauconnier & Turner 2002; Turner 1996) offers a cogni-
tive foundation for understanding system intentionality 
as actively (re)constructed by users via integrating con-
cepts of intentionality based on encounters with animate 
agents, and conceptualization of algorithmic operation 
of inanimate computer systems. Thus, users compress 
the behavior of unfamiliar computational systems to hu-
man scale by constructing conceptual blends of systems 
with human-like intentionality, through semantic hooks 
that facilitate such blends in the various discourses sur-
rounding the systems.  
Conclusion:  Design Implications of the AI 
Hermeneutic Network
The novel framework of the hermeneutic network sug-
gests new design approaches for intentional systems in 
digital humanities. Our current interactive narrative work 
Memory, Reverie Machine generates stories in which the 
main character varies dynamically along a scale between 
a user-controlled avatar with low intentionality and an 
autonomous non-player character with high intentional-
ity. By algorithmically controlling the semantic hooks 
for interpreting system behavior as intentional in the nar-
rative discourse (Zhu & Harrell 2008), the authors turn 
system intentionality into a scalable expressive dimen-
sion in interactive storytelling (Harrell & Zhu 2009). 
In conclusion, this paper proposes a new interdisciplin-
ary framework to analyze intentional systems as social 
and cultural productions, as opposed to construing them 
as the domain of purely technical practices. It underlines 
authors’	narrative	and	users’	interpretative	strategies,	in	a	
socially situated network of meaning exchange. Finally, 
through our own computational work we suggest new 
design implications for intentional systems, such as the 
scale of intentionality (Zhu & Harrell 2008) that poten-
tially can add new forms of expressivity to intentional 
systems in digital humanities. 
References
Agre, P. E. (1997). ‘Toward a Critical Technical Prac-
tice:	Lessons	Learned	 in	Trying	 to	Reform	AI’,	 in	So-
cial Science, Technical Systems, and Cooperative Work: 
Beyond the Great Divide, eds G. C. Bowker, S. L. Star, 
W. Turner, L. Gasser & G. Bowker, Lawrence Erlbaum 
Associates, pp. 131-58. 
Callon, M. (1986). ‘Some Elements of a Sociology of 
Translation: Domestication of the Scallops and the Fish-
ermen	of	St	Brieuc	Bay’,	in	Power, Action and Belief: A 
New Sociology of Knowledge, ed. J. Law, Routledge & 
Kegan Paul, London. 
Cohen, H.	 (2002).	 ‘A	 Self-Defining	 Game	 for	 One	
Player: On the Nature of Creativity and the Possibility 
of	Creative	Computer	Programs’,	Leonardo, vol. 35, no. 
1, pp. 59-64. 
Fauconnier, G. (2001). ‘Conceptual Blending and Anal-
ogy’,	 in	The Analogical Mind: Perspectives from Cog-
nitive Science, eds D. Gentner, K. J. Holyoak & B. N. 
Kokino, MIT Press, Cambridge, MA. 
Frank, M. (1989). The Subject and the Text: Essays on 
literary theory and philosophy, Cambridge University 
Press, Cambridge. 
Harrell, D. F. (2006). Walking Blues Changes Under-
sea: Imaginative Narrative in Interactive Poetry Genera-
tion with the GRIOT System, paper presented to AAAI 
2006	Workshop	 in	Computational	Aesthetics:	Artificial	
Intelligence Approaches to Happiness and Beauty, Bos-
ton, MA, AAAI Press. 
Hirsch, E. D. (1967). Validity in Interpretation Yale 
University Press, New Haven and London. 
Holquist, M. (1990). Dialogism: Bakhtin and His 
World, 2 edn. 
Latour, B. (1996). Aramis, or the Love of Technology, 
Harvard University Press, Cambridge. 
Mateas, M.	 (2002).	Interactive	Drama,	Art,	and	Artifi-
cial Intelligence, Carnegie Mellon University. 
Searle, J. (1983). Intentionality: An Essay in the Philos-
DIGITAL HUMANITIES 2009
Page 304
ophy of Mind, Cambridge University Press, Cambridge. 
Turner, M. (1996). The Literary Mind: The Origins of 
Thought and Language, Oxford UP, New York; Oxford. 
Virkler, H. A. (1981). Hermeneutics: Principles and 
Processes of Biblical Interpretation, Baker Book House, 
Grand Rapids, MI. 
Fauconnier, G. & Turner, M. (2002). The Way We 
Think: Conceptual Blending and the Mind’s Hidden 
Complexities, Basic Books. 
Harrell, D. F. & Zhu, J. (2009). Agency Play: Dimen-
sions of Agency for Interactive Narrative Design, paper 
presented to to appear in the Proceeding of AAAI Spring 
2009 Symposium on Interactive Narrative Technologies 
II. 
Mateas, M. & Stern, A. (2002). ‘A Behavior Language 
for	 Story-Based	 Believable	 Agents’,	 IEEE Intelligent 
Systems, vol. 17, no. 4, pp. 39-47. 
Pérez y Pérez, R. & Aliseda, A. (2006). The Role of 
Abduction in Automatic Storytelling, paper presented to 
Proceedings of the AAAI workshop in Computational 
Aesthetics: AI Approaches to Beauty & Happiness, Bos-
ton, MA, AAAI Press, pp. 53-60. 
Zhu, J. & Harrell, D. F. (2008). Daydreaming with In-
tention: Scalable Blending-Based Imagining and Agency 
in Generative Interactive Narrative, paper presented to 
AAAI 2008 Spring Symposium on Creative Intelligent 
Systems, Stanford, CA, AAAI Press, pp. 156-62. 
DIGITAL HUMANITIES 2009
Page  305
Posters
DIGITAL HUMANITIES 2009
Page 306
SEASR integrates with Zotero to 
Provide Analytical Environment 
for Mashing up Other Analytical 
Tools
Loretta Auvil 
University of Illinois at Urbana-Champaign
lauvil@ncsa.uiuc.edu	
Boris Capitanu 
University of Illinois at Urbana-Champaign
capitanu@ncsa.uiuc.edu
Xavier Llorà 
University of Illinois at Urbana-Champaign
xllora@illigal.ge.uiuc.edu
Michael Welge
University of Illinois at Urbana-Champaign
welge@ncsa.uiuc.edu	
Bernie Ács
University of Illinois at Urbana-Champaign
bernie@ncsa.uiuc.edu
This paper describes a development effort to link two humanities cyberscholarship infrastructure projects 
supported by The Andrew W. Mellon Foundation. We 
have created an extension to Zotero [1] that acts as a 
bridge between the data stored by Zotero, and the suite 
of analytic tools provided by SEASR [2]. This extension 
provides users with the ability to apply a variety of data 
analysis algorithms to their Zotero constructed collec-
tions, and visualize the results directly in the browser. 
This is accomplished by directly accessing the data mod-
el provided by Zotero, and converting that data model 
into RDF, which allows the ability to exploit the analyti-
cal capabilities of SEASR.
The SEASR environment provides a framework to in-
tegrate data, analytics, and tool constructs, so that data 
from one component can be passed to another. One of the 
unique capabilities of SEASR is the facility to provide a 
tool for mashups. That is, the ability to allow users to 
combine	tools	in	efficient	and	effective	ways.	This	paper	
describes the coupling of two relevant environments for 
humanist,	Zotero	and	SEASR	-	Zotero’s	data	asset	cre-
ation with the analytical capabilities of SEASR. Through 
the	use	of	Zotero’s	plugin	environment,	we	can	execute	
the analysis capabilities of the SEASR environment.  
The following sections provide a description of the two 
major pieces of this effort—Zotero and SEASR. Also 
provided is a description of the major functions per-
formed by the combination of the two. These include: 
data gathering, data analytics, and data visualization. We 
end with a summary of the integration of the two efforts 
and a view to our future work.
1. Background
1.1 Zotero  
Zotero was selected because of its popularity with schol-
ars	to	record,	catalog	and	find	resources	collected	from	
the Internet. Zotero was developed at the Center for His-
tory and New Media, George Mason University, and is 
a	tool	aimed	at	facilitating	a	user’s	research	process	by	
providing mechanisms for collecting, managing, and cit-
ing internet resources. Zotero functions as an extension 
of the popular open-source browser, Firefox, which al-
lows it to provide its services in the same environment 
where the research is usually performed. One of the key 
features provided by Zotero is the ability to automati-
cally extract metadata from online resources as part of 
the resource collection process, and store it conveniently 
on	the	user’s	computer,	allowing	for	offline	retrieval	of	
this data on demand. Zotero also provides advanced tag-
ging and searching functionality, allowing the user to or-
ganize,	find,	and	visualize	the	collected	resources.
Zotero includes a powerful metadata editor, allowing the 
user to make additions/corrections to the automatically 
extracted	information.	Users	can	add	new	fields,	attach	
screenshots and documents, create notes, and even create 
relationships between the various resources collected. 
Overall, with such a vast and diverse amount of infor-
mation,	a	mechanism	for	finding	patterns	or	interesting	
relationship between these resources would go a step 
further in helping researchers discover and extract more 
information from their collections. Enter SEASR.
1.2 SEASR  
(Software Environment for the  
Advancement of Scholarly Research) 
SEASR	analytics	enhances	scholars’	use	of	digital	ma-
terials by helping them uncover hidden information and 
connections, supporting the study of assets from small 
patterns drawn from a single text or chunk of text to 
broader entity categories and relations across a million 
words or a million books. SEASR is designed to enable 
digital humanities scholars to rapidly design, build, and 
DIGITAL HUMANITIES 2009
Page  307
share software applications that support research and 
collaboration.
The SEASR team developed Meandre [3], which is the 
machinery	 for	 assembling	 and	 executing	 data	 flows—
software applications consisting of software compo-
nents that process data (such as by accessing a data store, 
transforming the data from that store and analyzing or 
visualizing the transformed results).
SEASR is extensible allowing for new analytics to be 
added, such as support for linguistic analysis for dif-
ferent time periods or languages, to readjusting entire 
steps in the work process so that researchers can validate 
results from their queries. Components can be created 
from other programming projects. The SEASR environ-
ment is data driven and includes a workbench to orches-
trate	the	flow	of	data	through	the	different	components.	
All SEASR analytics are enabled as web service calls.
SEASR	also	provides	publishing	capabilities	 for	flows	
and components, enabling users to assemble a repository 
of components for reuse and sharing. This allows users 
to leverage other research and development efforts by 
querying and integrating component descriptions that 
have been published previously at other shareable re-
pository locations.
2. Data Gathering
Zotero’s	data	model	 is	very	flexible,	 allowing	 the	user	
to	 add	 new	 fields,	 create	 notes,	 attach	 documents	 and	
screenshots, and establish relationships between re-
sources. At a minimum, Zotero adds the following in-
formation for each resource that is added: title of the 
resource, originating URL, and the dates when the re-
source	was	 created,	modified,	 and	accessed.	For	many	
major research and library sites Zotero can automatically 
extract the full reference information, which includes au-
thorship data, abstracts, page references, locations, etc. 
This provides a wealth of information that can then be 
submitted for analysis to SEASR.
Once the data are converted into RDF (a process which 
is transparent to the user), it can be sent for processing, at 
user’s	request,	to	any	of	a	number	of	available	data	anal-
ysis algorithms. When such a data processing request 
is received, the extension establishes a communication 
channel with the web service associated with the pro-
cessing	flow,	through	which	the	RDF	data	are	submitted.	
After processing completes, the results are retrieved via 
the same communication channel and displayed in a new 
browser window. Depending on the complexity of the 
type	of	processing	requested,	there	may	be	a	significant	
delay until the results are retrieved.
Figure 1. Plugin for Zotero that requests SEASR analytics.
The	 extension	 provides	 a	 flexible	 mechanism	 through	
which	the	user	can	specify	which	data	processing	flows	
they	want	to	have	access	to,	by	configuring	a	list	of	SEA-
SR	servers	where	these	flows	are	hosted.	This	way,	the	
user can include any number of Zotero-compatible data 
processing	flows	hosted	by	3rd	party	organizations.
3. Data Analytics 
The SEASR team has been integrating a variety of tools 
as well as developing our own analytics. Currently we 
have integrated natural language processing tools (NLP) 
and current research algorithms from our data mining 
collaborators as well as transformation components to 
allow for data movement between the different compo-
nents. 
We have enabled some very simple and straightforward 
requests, like word counts, information regarding part of 
speech, and entity extraction capabilities. We also have 
additional machine learning approaches that can be lev-
eraged, like clustering, frequent pattern analysis, predic-
tive modeling, graph mining, and sequence analysis. We 
have currently integrated D2K (Data to Knowledge) [4] 
and T2K (Text to Knowledge) analysis, OpenNLP [5], 
and GATE (General Architecture for Text Engineering) 
[6]. This means that from your Zotero collection, you 
can ask for a social network analysis based on authors 
and other metadata. You can ask for a tag cloud of all 
your notes. You can ask for a tag cloud of a particular 
work or collection. You can cluster the documents from 
your collection. You can track a character or set of terms 
throughout a book or collection. You can look at ex-
tracted entities like locations on a Google map [7]. You 
can look at extracted entities like date on a timeline like 
Simile [8]. You can build a social network of the people 
mentioned in your collection.
4. Data Visualization
As with the data analysis, a number of visualization tools 
exist, so we have been working to integrate with these 
tools rather than redeveloping. We have incorporated 
visualizations from D2K as applets such as frequent pat-
DIGITAL HUMANITIES 2009
Page 308
tern analysis as well as several of the predictive model-
ing visualizations. We have also leveraged code to create 
a tag cloud [9]. We are providing link-node charts and 
stacked	bar	charts	via	flare	[10].	The	collage	of	example	
visualizations below is meant to provide an idea of the 
visual metaphors being used.
5. Future Work 
We continue developing analysis and visualization ca-
pabilities that can be leveraged by Zotero. As part of our 
Pathways to SEASR Workshops, we are demonstrating 
this tool integration and are establishing collaborations 
with workshop teams. These teams are exploring spe-
cific	use	cases	to	demonstrate	scholarly	research	that	can	
be easily added to this plugin environment. We are look-
ing to improve the interaction between the plugin and the 
SEASR framework and its ability to provide users with 
visual	interfaces	for	customizing	the	execution	of	flows.
Figure 2. Collage of visual metaphors available with 
SEASR analytics.
6. Summary
In summary, we have created a tool that facilitates the 
communication of Zotero collections data with SEASR 
for further study and research. We have linked SEASR, 
a	strong	and	flexible	tool	that	can	add	research	capabili-
ties to these text assets. SEASR allows for the use of 
its existing analysis and visualization tools, and more, 
it allows for the integration of other tools through a 
mashup process. The result of this effort is a synergy—a 
strengthening of both Zotero and SEASR—as useable 
tools for cyberscholarship.
7. Acknowledgement
The SEASR project is funded by The Andrew W. Mellon 
Foundation.
8. References
1. Zotero, http://www.zotero.org
2. SEASR, http://seasr.org
3. Llorà, Ács B, Auvil LS, Capitanu B, Welge ME, 
Goldberg DE (2008) Meandre: Semantic-Driven Data-
Intensive Flows in the Clouds, in Proceedings of IEEE 
Fourth International Conference on eScience, 238-245, 
IEEE Press.
4. D2K, http://alg.ncsa.uiuc.edu/do/tools/d2k
5. OpenNLP, http://opennlp.sourceforge.net/
6. H. Cunningham, D. Maynard, K. Bontcheva, V. 
Tablan. GATE: A Framework and Graphical Develop-
ment Environment for Robust NLP Tools and Applica-
tions.  Proceedings of the 40th Anniversary Meeting of 
the	Association	for	Computational	Linguistics	(ACL’02).	
Philadelphia, July 2002.
7. Google Maps API, shttp://code.google.com/apis/
maps/
8. Simile Timeline, http://www.simile-widgets.org/time-
line/
9. Tag Cloud, http://emumarketing.uoregon.edu/
paul/2008/09/28/the-new-tag-cloud/
10.	Flare,	http://flare.prefuse.org/
DIGITAL HUMANITIES 2009
Page  309
Automatic Standardization 
of Spelling for Historical Text 
Mining
Alistair Baron
Lancaster University 
a.baron@comp.lancs.ac.uk 
 
Paul Rayson 
Lancaster University 
p.rayson@lancs.ac.uk 
 
Dawn Archer 
University of Central Lancashire 
dearcher@uclan.ac.uk
Introduction
The use of textual data in humanities research is sig-nificantly	 aided	 by	 automated	 techniques	 such	 as	
key word analysis, collocations and corpus annotation 
(e.g. part-of-speech). If a text corpus contains a large 
amount of spelling variation, there is a considerable 
impact on the accuracy of these automatic techniques. 
For example, studies in respect to Early Modern English 
(EModE) corpora - the focus of the study detailed in this 
paper - have documented the adverse effects of spelling 
variation on key word analysis (Baron et al., 2009), part-
of-speech tagging (Rayson et al., 2007) and semantic 
analysis (Archer et al., 2003).
The problem of spelling variation in corpora needs to 
be addressed in order for more accurate and meaningful 
results	 to	be	achieved	 in	fields	where	historical	 source	
texts are required. Researchers can side-step the issue 
by using modernized versions of corpora, of course, but 
these are not always available. Another potential solu-
tion is to manually standardize the spellings; this in-
cludes reading through texts, spotting any non-standard 
spellings and deciding upon a modern equivalent, result-
ing in the production of a new version of the text with 
spelling variants replaced. However, a manual standard-
izing approach is likely to be unworkable when working 
with some of the larger corpora or online databases that 
are now available. 
This paper details the current version of the VARiant De-
tector (VARD 2) tool, which can be used in various ways 
to standardize spelling variation in corpora. In particu-
lar, the tool can be used to (partially) standardize spell-
ings automatically, with no restriction on the number of 
words to be processed. Here, we focus on the ways in 
which the tool can be trained from manually standard-
ized corpora samples, particularly the letter replacement 
component of the tool, and evaluate the improvement 
that this makes to the performance of VARD 2.
Early Modern English Spelling Variation
The EModE period is of particular interest in historical 
text mining studies; book production increased sharply 
during the period, largely due to the introduction of the 
printing press (1476) and increasing literacy levels (Gör-
lach, 1991: 6). As such, the EModE period is the earliest 
period of the English Language from which a reasonably 
large corpus can be constructed and studied in detail.
Spelling variation was a major feature of EModE texts, 
the	extent	of	which	we	have	recently	quantified	in	Baron	
et	al.	(2009).	It	is	common	to	find	words	spelt	in	a	num-
ber of different forms in the same text or even on the 
same page. This was not seen as problematic, however, 
as there was no notion of the importance for a single 
spelling for each word; for example, letters would be 
added	or	removed	to	ease	line	justification.	Table	1	be-
low shows some typical spelling variants found in EM-
odE texts, whilst Vallins and Scragg (1965) and Culpeper 
and Archer (forthcoming) describe the spelling variation 
trends in more detail.
Variant Modern  
Equivalent
Notes
“goodnesse” “goodness” ‘e’	often	added	to	end	of	
words.
“brush’d” “brushed” Apostrophes used instead 
of	‘e’.
“encrease” “increase” Vowels often 
interchanged.
“spels” “spells” Consonants often doubled 
or singled.
“deliuering” “delivering” Common	for	‘u’	and	‘v’	
to be interchangeable.
“conuay’d” “conveyed” Many combinations of 
the above.
 
Table 1. Examples of common EModE spelling variants.
We have shown the effect of this spelling variation on 
textual analysis techniques in previous and forthcoming 
papers: key word analysis (Baron et al, 2009), part-of-
speech tagging (Rayson et al., 2007) and semantic tag-
ging (Archer et al., 2003). All of the studies showed that 
spelling variation causes considerable problems to the 
accuracy and meaningfulness of results, and that deal-
ing with spelling variation (even partially) can achieve 
substantial improvements in annotation accuracy.1 The 
production of standardized or modernized versions of 
historical corpora therefore allows for more accurate 
DIGITAL HUMANITIES 2009
Page 310
automated text mining techniques to be applied to the 
corpora.2
VARD 2 and DICER
Our solution to the spelling variation problem described 
in the previous section has been the development of the 
VARD 2 tool,3 a piece of software designed to assist re-
searchers	in	standardizing	historical	corpora	(specifically	
EModE texts) both manually and automatically. VARD 
2 uses a manually created list of variant – replacements 
mappings as well as employing methods from modern 
spell checking software; such as phonetic matching, let-
ter replacement heuristics and Edit Distance. An earlier 
version of the VARD 2 software is described in more 
detail and evaluated in Rayson et al (2008). The cur-
rent version can cater for user-created letter replacement 
rules,	which	will	 be	 used	 by	 the	 tool	 to	 find	 potential	
variant replacements. In addition, XML provision has 
been improved, processing speed increased, and a new 
word reference list4 added. Screenshots of the latest ver-
sion, VARD 2.2, are shown in Fig. 1 and Fig. 2. 
Fig. 1 Screenshot of VARD 2.2 showing the interactive 
mode which allows the user to manually standardize texts 
and train the tool on samples of a corpus
One way in which VARD 2 can be used is to automatical-
ly standardize the spelling variation in an entire corpus. 
For EModE texts, this can be done immediately, with 
no training. However, for better results and to use the 
tool with other varieties of English, the user can train the 
software on a particular corpus by using the interactive 
version to manually process samples from the corpus. 
The tool will improve its ability to deal with a corpus 
based on decisions made by the user in the interactive 
version. It does this by learning which of its methods are 
most	 successful	 in	 finding	 the	 correct	 replacement	 for	
variants and adjusting its method weights accordingly 
(these are used when ranking potential replacements). 
The	tool	will	also	edit	its	dictionary	and	its	list	of	specific	
variant replacements based on changes made by the user.
A new development to allow for further training of 
VARD 2 on a corpus is a tool named DICER (Discover 
and Investigation of Character Edit Rules). DICER can 
search XML output from VARD 2 for variant – replace-
ment mappings or be provided with a list of such map-
pings. Each mapping is analyzed and a set of character 
edit rules are produced which can transform the spell-
ing variant into its modern equivalent. The details of 
these character edit rules are then collated into a data-
base, which can be viewed through a set of web pages.5 
The main table produced by the analysis, shown in Fig. 
3, displays details of the individual character edit rules 
along with various frequencies. By clicking on individ-
ual rules, further information is available such as which 
characters typically occur before and after the rule oc-
curs;	this	is	shown	for	the	rule	‘Delete	e’	in	Fig.	4.	Any	
frequency in the tables can be clicked to view a list of oc-
currences producing that frequency. The data produced 
in the DICER analysis is vast and thus cannot be detailed 
in full here.
Fig. 2 Screenshot of VARD 2.2 showing the batch-
processing mode where users can automatically 
standardize a chosen group of texts
Fig. 3 Screenshot of DICER analysis on a manually 
standardized 5,000-word sample of Shakespeare’s First 
Folio
DIGITAL HUMANITIES 2009
Page  311
Fig. 4 Screenshot of DICER showing the rule ‘Delete 
E’ in a manually standardized 5,000-word sample of 
Shakespeare’s First Folio
By using DICER to analyze manually standardized sam-
ples of a corpus, a list of common character edit rules can 
be viewed. These character edit rules can then be added 
to VARD 2 and the tool will be better equipped to make 
judgments on the correct replacement for variants found 
whilst automatically standardizing the corpus.
In	 order	 to	 test	VARD	2	 and	DICER’s	 training	 ability	
a	5,000-word	sample	of	Shakespeare’s	First	Folio6 was 
manually standardized in the interactive-mode of VARD 
2 as training data, the entire corpus was then automati-
cally standardized. Using this small amount of training 
data (6% of the entire corpus) increased the proportion 
of spelling variants replaced7 from 70.33% to 73.75%.
The automatic standardization (after training) resulted 
in 10,601 unique variant replacements. 70.35% of these 
replacements	could	be	achieved	through	VARD	2’s	orig-
inal set of character edit rules alone. DICER analysis 
was then produced on the manually standardized Shake-
speare	sample;	this	is	shown	in	Fig.	3.	VARD	2’s	rule	list	
was augmented with additional rules from the DICER 
analysis: any rule occurring 10 or more times was added, 
if not already present. Using this new rule list 77.66% of 
the 10,601 unique replacements could now be found, an 
increase of 7.31%.
The results are extremely promising, and increasing the 
size of the manually standardized sample should im-
prove	these	figures	even	further.	DICER	can	also	be	used	
to provide probabilities dictating how likely a charac-
ter edit rule should be applied in a certain position with 
specified	 surrounding	 characters.	Modifying	VARD	 to	
use these probabilities could see even greater improve-
ments in performance.
Calculating	 the	precision	of	VARD	2’s	methods	 is	dif-
ficult	without	 a	manually	 checked	 standardized	 corpus	
of decent size. We have recently acquired such a corpus 
and present the results of using this corpus to train and 
evaluate	VARD	2’s	methods	in	Baron	and	Rayson	(forth-
coming).
Conclusion
This paper has described the problems that variant 
spellings cause for historical text mining, particularly 
for automated methods in historical corpus linguistics, 
such as part-of-speech tagging and key words analysis. 
In	previous	and	forthcoming	work,	we	have	quantified	
the errors or differences that result from the application 
of untrained tools and techniques on historical data that 
has not been standardized. Our proposed solution is the 
VARD tool, which offers the potential to standardize 
spelling in historical texts automatically and with high 
accuracy. We have described recent improvements to 
VARD 2, such as the inclusion of a much larger modern 
dictionary that enables better detection of historical vari-
ants and matching with modern forms. 
VARD 2 has been developed to deal with spelling varia-
tion in EModE texts; the tool can be used with its de-
fault settings to achieve partial standardization automati-
cally. However, with some training, we have shown that 
VARD	 2’s	 performance	 is	 enhanced.	 Further	 training	
could allow the tool to be used with other varieties of 
non-standard English (e.g. SMS corpora and weblogs).
In the future, we will evaluate the extent to which varia-
tion	 that	 can	only	be	detected	 contextually	 (e.g.	 ‘then’	
for	 ‘than’	 and	 ‘bee’	 instead	 of	 ‘be’)	 contributes	 to	 the	
problem. Dealing with this problem requires more ad-
vanced techniques, e.g. POS tagging, to be used in the 
detection phase.
Notes
1Of course, spelling variants themselves are important 
linguistic features and thus worthy of study: as such, 
although our focus relates to how we might deal with 
spelling variation within historical data as a means of 
enabling the (more) effective use of automated analytical 
techniques, we advocate that any solution to this prob-
lem should always retain the original spelling. VARD 2 
does so using an xml tag to note a replacement with the 
original spelling stored as an attribute of the tag.
2It should be noted that the accuracy of annotation is 
likely to be affected by additional factors, including 
differences in the grammar of the EmodE period when 
compared to present-day English (see Kytö and Vouti-
lainen, 1995) and the possibility of a semantic shift in 
words from EModE to present-day English (see, for ex-
ample, Knapp, 2000).
DIGITAL HUMANITIES 2009
Page 312
3The tool is available to download online, with a user 
guide also provided. The software is free to use for 
academic purposes from http://www.comp.lancs.
ac.uk/~barona/vard2/ 
4Derived from the Spell Checking Oriented Word List 
(SCOWL). See http://wordlist.sourceforge.net/scowl-
readme 
5Available at http://juilland.comp.lancs.ac.uk/dicer/
6Available from the Oxford Text Archive:  http://ota.
ahds.ac.uk
7Variants here are words which VARD 2 deems to be 
variants, i.e. words which are not in its modern lexicon. 
It should be noted that words will be incorrectly marked 
as variants (particularly proper names) and some vari-
ants will be incorrectly marked as modern words (partic-
ularly read-word errors,	such	as	‘bee’	for	‘be’	and	‘doe’	
for	‘do’).
References
Archer, D., McEnery, T., Rayson, P. and Hardie, A. 
(2003). Developing an automated semantic analysis sys-
tem for Early Modern English. In Archer, D, Rayson, P., 
Wilson, A. and McEnery, T. (eds), Proceedings of the 
Corpus Linguistics 2003 conference. UCREL technical 
paper number 16. UCREL, Lancaster University, pp. 22 
- 31.
Baron, A. and Rayson, P. (forthcoming). Automatic 
standardization of texts with spelling variation, how 
much training data do you need? To appear in Corpus 
Linguistics 2009.
Baron, A., Rayson, P. and Archer, D. (2009). Word fre-
quency and key word statistics in historical corpus lin-
guistics. In Ahrens, R. and Antor, H. (eds.) Anglistik. In-
ternational Journal of English Studies, 20 (1), pp. 41-67.
Culpeper, J. and Archer, D. (forthcoming). The His-
tory of English Spelling. In Culpeper, J., Katamba, F., 
Kerswill, P., Wodak, R. and McEnery, T. (eds), English 
Language and Linguistics. Palgrave Macmillan, Basing-
stoke, UK.
Görlach, M. (1991). Introduction to Early Modern Eng-
lish, Cambridge University Press, Cambridge.
Knapp, P. A. (2000). Time-Bound Words: Semantic and 
Social Economies from Chaucer’s England to Shake-
speare’s. Anthony Rowe Ltd, Chippenham, Wiltshire, 
UK.
Kytö, M. and Voutilainen, A. (1995). Applying the 
Constraint Grammar Parser of English to the Helsinki 
Corpus. ICAME Journal 19, pp. 23-48.
Rayson, P., Archer, D., Baron, A., Culpeper, J. and 
Smith, N. (2007). Tagging the Bard: Evaluating the ac-
curacy of a modern POS tagger on Early Modern Eng-
lish corpora. In proceedings of Corpus Linguistics 2007, 
July 27-30, University of Birmingham, UK.
Rayson, P., Archer, D., Baron, A. and Smith, N. 
(2008). Travelling Through Time with Corpus Annota-
tion Software. In Lewandowska-Tomaszczyk, B. (ed) 
Corpus Linguistics, Computer Tools, and Applications 
– State of the Art. Palc 2007. Peter Lang, Frankfurt am 
Main.
Vallins, G. H., and Scragg, D. G. (1965). Spelling. An-
dré Deutsch.
DIGITAL HUMANITIES 2009
Page  313
Generalizing the International 
Children’s Digital Library
Benjamin B. Bederson 
University of Maryland 
www.cs.umd.edu/~bederson
Patrick Rutledge 
University of Maryland 
 
Alex Quinn
University of Maryland
 
Abstract
The	 International	 Children’s	 Digital	 Library	 is	 a	website	 of	 exemplary	 free	 children’s	 books	 from	
around the world (www.childrenslibrary.org). Since 
we launched the website in 2002, we have recently em-
barked on an effort to generalize what we have learned, 
creating a version (to be made open source when it is 
finalized)	 that	 is	 suitable	 for	more	general	content	and	
a more traditional adult audience.  We also have been 
exploring mobile deployment – providing access to the 
ICDL’s	 picture	 books	 with	 an	 interface	 that	 displays	
readable	 text	 in	context	on	Apple’s	 iPhone.	 	Together,	
these efforts demonstrate how we can learn from one 
successful project to expand into other areas of content 
and platform, and in this paper we attempt to summarize 
the core lessons learned that can be applied elsewhere.
Introduction
The	 International	Children’s	Digital	Library	 (ICDL)	 is	
an established electronic archive, providing children and 
their	adults	easy	access	to	thousands	of	children’s	books	
in almost 50 languages which can be read online for free. 
Over the six years since we deployed the ICDL, we have 
learned a lot about what it takes to make a usable digi-
tal library (Bederson 2008).  Motivated by a need of the 
Boston Public Library and the Knowledge Commons 
(formerly the Open Content Alliance) to offer a highly 
usable range of interfaces for the millions of books they 
are scanning, and by the recognition that people are ac-
cessing more and more books on mobile devices, we de-
cided to start branching out, and applying those lessons 
to other content domains, audiences, and platforms.
Content
The books in the ICDL are all digitized versions of tradi-
tionally printed paper books which are selected for chil-
dren ages 3-13. Almost half are picture books. Due to the 
narrow range of content, we have been able to manually 
catalog	the	books	in	the	library	with	a	fixed	ontology	of	
just under 300 categories in a hierarchical structure (i.e., 
Appearance->Format->Picture Books). We use a combi-
nation of traditional categories (such as “Short Story”) 
and more customized categories that we developed in the 
course of working with children (such as “Book cover 
color” and whether the books are “Happy” or “Sad”).
In order to design the ICDL interface to support a broad-
er set of content, we had to rethink how we categorized 
the books and presented those categories.  A few hun-
dred	categories	do	not	offer	a	fine	enough	distinction	for	
millions of books (Baker 1996).  In fact, the standard 
MARC records of the Library of Congress use many 
thousands of categories. The traditional solution is re-
move the focus on categories, and to make textual search 
the primary mechanism for searching for books in large 
collections.  However, this gives up the power of catego-
rization, especially when the person has some particu-
lar attributes they want to specify, or if, for example, a 
teacher wants to specify a set of categories that might be 
useful for their students to use for more focused search-
ing.  Another scaling problem with our original design is 
that we used manually created icons for each category. 
This was a lot of work for a few hundred categories, but 
impossible when there are many more.
We came up with a new solution (Figure 1) that lets the 
user choose which categories are displayed in the pri-
mary search window and integrates keyword search, the 
search results, and a preview of the book. A tightly cou-
pled advanced search (not shown) adds several features, 
including a more hierarchical display of categories, 
counts of how many books are in each category, explicit 
search	fields	for	title	&	author,	and	more.
Figure 1: Current prototype for generalized version of 
ICDL For broader content and older users.
DIGITAL HUMANITIES 2009
Page 314
Audience
Our aim of supporting adult users and not just children 
also	gave	us	some	flexibility	in	designing	this	prototype.	
Our research with children has led us to understand the 
importance of using large objects to click on the screen, 
fewer abstractions in the interface and a generally sim-
pler visual display (Hutchinson 2003).  The interface we 
created thus correspondingly uses more text, smaller hit 
targets, and a visually denser display.
Platforms
In the same spirit of trying to make books available to 
more people, we designed a version of ICDL for the Ap-
ple iPhone platform.  This platform is unusual because 
while small, it has a relatively high resolution display 
and powerful graphics that enable smooth animated tran-
sitions.	We	built	an	application	that	gives	offline	access	
to four picture books.  It uses the “ClearText” technology 
that ICDL developed for making text legible on small 
displays (Quinn et al. 2008) and smooth animated transi-
tions that make it easier to understand where you are in 
the interface and book as the reader navigates a complex 
information space on a very small display.
Concluding Thoughts
As the trends of increasing technological capabilities 
and broad increases in access continue to occur, we must 
refine	and	innovate	the	way	we	deliver	access	to	books.	
It	is	crucial	that	people	have	an	unfettered	ability	to	find	
what they are looking for, and to deeply engage with the 
content of books once they have found it.  The efforts de-
scribed here show two approaches to achieving this goal.
The lessons from the web interface include the impor-
tance of building an all-in-one interface with varying 
levels of search complexity along with search results 
and book previews all integrated into a single screen.  A 
related	lesson	is	that	you	can’t	ignore	issues	of	scale	in	
your design, and that one important approach is to cou-
ple a very good and simple initial experience with the 
possibility	of	significant	end-user	customization.
The lessons from the iPhone interface are that you must 
re-think what you are offering for this tiny and mobile 
package.  You cannot and should not do everything that 
you offer on your full website.  It was a tough call for 
us, but we completely eliminated search! The other is-
sue	is	that	while	difficult,	you	must	balance	engagement	
and usability.  We tried to use interesting animations to 
keep the experience playful while not getting in the way 
of reading – and we also spent a lot of time creating an 
interface that supports the core mission of the applica-
tion – which is clear and legible text for easy reading.  As 
the application is just being released now, time will tell 
whether we were successful with these goals.
References
Baker, S.	 (1996).	 “A	Decade’s	Worth	 of	 Research	 on	
Browsing Fiction Collections.”  
In Kenneth Shearer (Ed), Guiding the reader to the next 
Figure 2. (Left to right): a) ICDL for iPhone home screen shows the entry point to 4 books for iPhone and iPod Touch 
platforms; b) the result of zooming into the top-right book (Blond ear … black ear); c) the result of zooming into a page of 
that book, and the result of tapping on a textbox to display it in a font that is large enough to read.
DIGITAL HUMANITIES 2009
Page  315
book. New York, NY: Neal-Schuman Publishers, Inc. 
45-72.
Bederson, B.B. (2008) “Experience the International 
Children’s	 Digital	 Library”,	 Interactions Magazine, 
ACM Press, 50-54.
Hutchinson, Hillary Brown	 (2003).	 Children’s	 Inter-
face Design for Hierarchical Search and Browse. In pro-
ceedings of ACM SIGCAPH Computers and the Physi-
cally Handicapped.  75: 11-12 
Quinn, A., Hu, C., Arisaka, T., & Bederson, B.B. 
(2008) Readability of Scanned Books in Digital Li-
braries, in proceedings of ACM CHI (CHI 2008), ACM 
Press, 705-714.
Snake’s Nest: Untangling the 
Relationships between Classic 
Maya States
Alex Bennett 
University of Portsmouth 
Alex.Bennett@port.ac.uk 
The political organisation of the Classic Maya world has	generated	almost	as	much	conflict	and	conten-
tion among scholars as it did amongst the ancient Maya 
themselves. As Prudence Rice [2004] has amply dem-
onstrated, prevailing theories have swung from a homo-
geneous empire to a fractious patchwork of independent 
statelets existing in precarious balance and most con-
ceivable organisational models between. This article 
discusses the the problems in trying to untangle the com-
plex inter-state relations of the Classic period and the de-
velopment of a software tool to support further research 
in this area.
The ancient Maya have a complex historiographical 
relationship with the modern world. Without wheeled 
transport or metal tools they developed an intricate and 
sophisticated culture that after its disintegration around 
900AD vanished almost completely back into the forests: 
until the middle of the nineteenth century hardly anyone 
in the western world knew much more than the wild tales 
of shining cities buried in the jungle and the search for 
El Dorado. The ancient Maya came to popular attention 
at the height of the age of reason, as archaeology was 
evolving from an aristocratic pastime into a science. In 
an age that equated civilisation with literacy, the Classic 
Maya were all but mute. Early explorers were sure their 
monuments recorded written information but the script 
remained largely undeciphered. An understanding of the 
calendrical system allowed early historians to develop a 
broad chronology but in the absence of any meaningful 
testimony from the Maya themselves, imagination was 
employed	to	fill	the	blanks.
Many	of	these	gaps	were	filled	by	forcing	the	Maya	to	
conform to the social and political templates provided 
by better understood ancient cultures, with limited suc-
cess. The emphasis on dates in the epigraphic record fu-
elled	an	optimistically	utopian	vision	of	a	pacifist	people	
living in splendid isolation under the rule of benevolent 
astronomer priests obsessed with the cycles of time. Its 
most famous advocate Eric Thompson [Drew 1999] ac-
knowledged	evidence	of	conflict	but	dismissed	it	as	little	
more than minor border disputes or contested inheritanc-
DIGITAL HUMANITIES 2009
Page 316
es: to follow the argument to its logical conclusion, these 
events might have been recorded for rarity value.
Breakthroughs in decipherment made the utopian ideal 
increasingly untenable. The emerging epigraphic record 
presented a tangled, often bloody history of warrior rul-
ers, holy lords recording their achievements in birth, al-
liance, conquest and death in a medium designed to awe 
their subjects and cow their enemies. Recent research 
[Martin	&	Grube	2000]	 has	 identified	glyphs	 for	 hier-
archical relationships amongst rulers suggesting a web 
of coalition and patronage, covering vast distances, that 
may represent anything from an ephemeral expression 
of dominance to a long-term relationship such as that at-
tested to between Tikal and Palenque.
The richest and most widely reproduced diagrammatic 
representation of this political structure comes from 
Simon Martin [Martin & Grube 2000, p21]. It demon-
strates that, for much of the Classic period, the rivalry 
between the polities of Calakmul and Tikal formed the 
nexus around which the wider political system revolved.
Known in Classical times as Kaan – the Snake King-
dom – Calakmul was a giant by the standards of the an-
cient world, boasting an urban population estimated by 
population density analysis to be around 90,000 with a 
suburban rural population of over 2.5 million under its 
immediate authority [Braswell et al in Demarest, Rice & 
Rice 2004]. It is the most frequently referenced site on 
monuments throughout the region and has been labelled 
a	‘super	state’	or,	 rather	more	emotively,	a	superpower	
[Coe 1967; Martin & Grube 2000 inter alia]. Unlike oth-
er powerful states such as Tikal and Palenque, the quality 
of local stone at Calakmul has meant that many inscrip-
tions are now fragmentary or illegible: the majority of 
what	is	known	about	Calakmul’s	political	machinations	
comes from other sites. The prevailing impression, co-
loured	 perhaps	 by	Western	 conceptions	 of	 the	 snake’s	
character, is of an acquisitive and aggressive evil em-
pire, overcome by the dogged resistance of Tikal and her 
allies.
There is no doubting this interpretation could be accu-
rate: epigraphic evidence and the Martin diagram clear-
ly show Calakmul was politically and militarily active 
throughout the Maya world. The Martin diagram may, 
however, be contributing to an impression of continuous 
interference over the entire span of the Classic period 
that	cannot	be	supported	by	the	fluid	nature	of	political	
relations in the Maya world demonstrated elsewhere. In 
a social system where power and prestige are directly in-
fluenced	by	the	strength	and	even	charisma	of	the	ruling	
lord, many inter-state relationships would come and go 
in a handful of years.
The strength of the Martin diagram is its ability to sum-
marise a wealth of detail about the relationships between 
the largest Maya polities. In many ways, it resembles the 
network map of the London Underground, with the same 
advantages and attendant weaknesses. While it distin-
guishes	types	of	association,	for	example	conflict	from	
diplomacy, and gives some indication of the frequency of 
those links, it is unable to demonstrate their duration or 
the	evidence.	A	conflict	link,	for	example,	may	represent	
a single act of aggression like a religiously sanctioned 
Star War or sporadic low-intensity warfare over an ex-
tended period. In cases such as the relationship between 
Calakmul	and	Yaxchilan	which	shows	both	conflict	and	
marriage alliance, it is impossible to distinguish which 
came	first	or	indeed	whether	they	were	related	at	all:	the	
marriage attested to between Bird Jaguar of Yaxchilan 
and Lady Evening Star of Calakmul [Drew 1999] could 
have been arranged to cement peace or the two events 
could be generations apart.
The very short period between 680 and 685 AD provides 
a good example of the complexity the Martin diagram 
is	poorly	equipped	to	show.	Those	five	years	saw	a	dra-
matic shift in the fortunes of many of the major powers 
across the Maya world: Calakmul, Palenque and Dos Pi-
las all lost rulers who had been in power for over forty 
years, notably long reigns even by modern standards; 
in contrast Tikal and Naranjo were both emerging from 
periods of weakness and internal division. Piecing to-
gether the direct effects of these changes on individual 
polities	has	proved	difficult	enough	but	recognising	the	
effects on the wider political landscape is almost impos-
sible with a diagrammatic model that cannot distinguish 
change over time.
The aim of this research is to produce a new interac-
tion mapping tool capable of overcoming some of these 
weaknesses in the Martin diagram. The Dynamite (Dy-
namic Maya Information Tool) software is designed to 
DIGITAL HUMANITIES 2009
Page  317
produce a rich, updateable model of political interac-
tions that can be extended and customised to support the 
needs of individual researchers. In its standard format, 
it provides an enhanced version of the Martin diagram, 
allowing	users	 to	filter	 the	data	on	a	range	of	different	
criteria, including regionally and temporally. It can also 
support	more	 specialised	 filtering	 to	 show	 interactions	
and source evidence by type: employing the full range of 
filters	it	would	be	possible	to	examine	nothing	but	Cal-
akmul’s	military	conquests	for	the	period	580	to	620	AD,	
derived solely from epigraphic sources.
Dynamite uses a model of loose data connectivity based 
on	XML	structured	data	 to	provide	maximum	flexibil-
ity and extensibility. Beyond compatible data typing, it 
makes no assumptions about the relationship between 
objects:	 all	 existing	filters	 are	provided	 through	 stored	
queries.	This	allows	researchers	to	be	flexible	and	inno-
vative in how they use and expand the source data while 
the XML format supports the easy sharing of results and 
helps Dynamite move towards a structure of plug-and-
play data.
The suite of Dynamite support tools allows researchers 
with even minimal computing experience to examine, 
update and extend the stored data as well as create new 
queries to run against that data. There are also shortcuts 
for experienced developers providing more direct access. 
It is hoped that the Dynamite software will contribute 
positively to the next generation of research tools for 
exploring the growing body of archaeological and epi-
graphic data on the relationships between Maya polities 
of the Classic period.
Bibliography
Aoyama, K [2005] Classic Maya Warfare and Weapons: 
Spear, Dart and Arrow Points of Aguateca and Copan; 
Ancient Mesoamerica 16
Boot, E [2002] The Life and Times of Balah Chan Kawil 
of Mutal According to Dos Pilas Heiroglyphic Stairway 
2; MesoWeb
Coe, M [1967] The Maya 7th Edition 2005; Thames and 
Hudson
Coe, M [1992] Breaking the Maya Code 2nd Edition 
1999; Thames and Hudson
Demarest, A, Rice & Rice (eds.) [2004] The Terminal 
Classic in the Maya Lowlands: Collapse, Transition and 
Transformation; University Press of Colorado
Drew, D [1999] The Lost Chronicles of the Maya Kings; 
Orion Books
Johnston, K [2001] Broken Fingers: Classic Maya 
scribe capture and polity Consolidation; Antiquity Vol-
ume 75
Kistler, A [2004] The Search for Five-Flower Mountain: 
Re-evaluating the Cancuen Panel; MesoWeb Online Ar-
ticles
Martin, S [2005] Of Snakes and Bats: Shifting Identities 
at Calakmul; PARI Online Publications
Martin, S and Grube, N [2000] Chronicle of the Maya 
Kings and Queens: Deciphering the Dynasties of the An-
cient Maya Revised Edition 2008; Thames and Hudson
Rice, P [2004] Maya Political Science: Time, Astronomy 
and Cosmos University of Texas Press
Stuart, D [2004] The Paw Stone: The Place Name of 
Piedras Negras, Guatemala PARI Online Publications
DIGITAL HUMANITIES 2009
Page 318
Clustering the Short Stories of 
Edgar Allan Poe  
Using Word Groups and Formal 
Concept Analysis
Roger Bilisoly
Central Connecticut State University 
bilisolyr@ccsu.edu
The proposed poster will summarize recent work on finding	clusters	of	Edgar	Allan	Poe’s	short	stories.	
These	are	defined	by	high	rates	of	word	usage	for	five	
word	collections,	each	of	which	is	defined	by	a	similar	
meaning.  For example, one of them focuses on death 
and has terms such as corpse, dead and die.  By choos-
ing word groups pertinent to the types of stories that Poe 
wrote, the resulting clusters make more intuitive sense 
than other clustering algorithms that are common in in-
formation retrieval.
Humans enjoy dividing texts into similar clusters.  For 
example, the literary critic Daniel Hoffman discusses 
Poe’s	stories	in	thematic	groups	(Hoffman,	1990).		For	
instance, vendors such as Amazon.com create clusters 
that consist of recommendations made to users, which 
are based on past purchases.  Consequently, there is 
value in creating a clustering algorithm that can explain 
how the grouped texts are similar in terms that a human 
can appreciate.
Unfortunately, the most popular techniques in infor-
mation retrieval for computing text similarity produce 
results that lack appeal to human sensibilities.  For ex-
ample, the vector space model (Section 2.1 of Grossman 
and Frieder, 2004) reduces a text to a table of (usually 
weighted) word counts where each column represents a 
text and each row represents a word.   Each column can 
be thought of as a vector, so the geometric idea of angles 
between texts can be introduced, where smaller angles 
correspond to higher similarity.  This geometric point of 
view is powerful because it is well understood by math-
ematicians, and it already has been applied to studying 
information.  For example, this is the focus of the book 
Geometry and Meaning (Widdows, 2002).  However, the 
dimensions of these vectors can be quite high (in the tens 
of thousands), which makes the results hard to reconcile 
with human intuition.
The technique used in this poster builds on the vector 
space model as follows.  Instead of including all the 
words that Edgar Allan Poe uses in his short stories 
(about	 20,000	 total),	 these	 are	 analyzed	 to	 find	 a	 few	
clusters,	 each	defined	by	 a	 shared	meaning,	which	 are	
built by using a thesaurus.  Note that this removes func-
tion words such as the, of, and, a and to, which primarily 
serve grammatical roles and are less interesting to a hu-
man reader.  Five groups are used in this poster, which 
are based on the following themes: death, body, spiri-
tual, horror, and family.  For example, the top ten most 
frequent death words in Poe are death, corpse, dead, 
murder, died, die, deceased, dying, fatal, and deadly. 
Anyone familiar with his short stories would not be sur-
prised by these word groups.  For example, many of his 
stories are about people who die (for instance, “Morel-
la,” “Eleonora,” and “The Oval Portrait”) or are killed 
(for instance, “The Black Cat,” “The Murders in the Rue 
Morgue,” and “The Tell-Tale Heart”). 
Since word frequencies are a function of text length, 
Poe’s	story	lengths	must	be	taken	into	account.		This	can	
be done with word rates, that is, the proportion of a word 
in a text.  Unfortunately, these can also depend on text 
length (see discussion in Chapter 1 of Baayen, 2002), so 
stories are grouped by length, which are then analyzed 
separately.  Three groups are used in this poster: 2000 to 
3000, 3001 to 4200, and 4201 to 6000.
For each of these three ranges, matrices are constructed 
where each row represents a word group, each column 
represents a Poe short story, and the entries are word 
group rates.  Although the vector space model could be 
used in this situation, the alternative method of formal 
concepts is used because it is more interpretable.   For-
mal concept analysis (FCA) is a technique to create a 
double lattice of concepts given a list of objects and an-
other one of attributes.  It is a central tool in concept data 
analysis (Carpineto and Romano, 2004) and is used to 
organize information in a way more similar to humans. 
For	this	application,	the	objects	are	Poe’s	short	stories,	
and the attributes are a story having a word group rate 
above the median.  
A formal concept consists of a set of objects and a set 
of attributes, where each object shares all the attributes, 
and each attribute is shared by all the objects.  This set 
of objects is a cluster, which is describable by its shared 
attributes.	 	Applied	 to	Poe’s	 short	 stories,	 each	 cluster	
is	determined	by	its	use	of	the	five	word	groups	defined	
above.  Since a list of death words is both straightfor-
ward to compute and is evocative for a human reader, 
such story clusters have intuitive appeal.  Moreover, 
there	 are	 several	 algorithms	 published	 that	 efficiently	
find	all	 the	 formal	concepts.	 	For	 this	project	Ganter’s	
algorithm is used (Ganter, 1984).
DIGITAL HUMANITIES 2009
Page  319
Here	is	a	specific	example.		There	are	thirteen	Poe	stories	
that are between 2000 and 3000 words long.  We use the 
five	attributes	defined	above,	one	for	each	word	group.	
A story has one of these if its word group rate is above 
the median rate for all thirteen stories.   For example, 
the median death word rate is 1.22 words per thousand 
(wpt).  The six stories with higher rates have the attribute 
death, the other seven do not.  For body words, the me-
dian is 5.44 wpt, so the six stories above this threshold 
have the body attribute.  The same computation is done 
for the remaining three word groups.
Figure 1.  Formal concept lattice for Edgar Allan Poe 
stories with lengths between 2000 and 3000 words. 
Formal concepts always form two lattices, called a Galois 
lattice.  One focuses on the stories, and the other focuses 
on attributes.  Both of these are ordered by subsets, and 
these are related: as the number of attributes increases, 
the number of objects decreases, and vice versa.  In this 
application, nineteen formal concepts are found, which 
are given in Fig. 1.  We consider one here, which consists 
of the Poe stories “The Tell-Tale Heart,” “The Masque of 
the Red Death,” and “Morella,” and the attributes death, 
body, and horror. Although the plots of these stories dif-
fer, there are similarities apparent to a human.  First, all 
three stories are about death.  In “The Tell-Tale Heart” 
the narrator tells how he kills his roommate.  The red 
death is a pestilence, and in “Morella” the narrator tells 
of	his	wife’s	obsession	with	death	and	rebirth.		Second,	
all	three	stories	discuss	the	body.		The	first	one	features	
an evil eye and a beating heart, while the red of the sec-
ond one refers to blood.  Morella dies giving birth, and 
her daughter grows to be just like her mother physically, 
which is described in the story.  Finally, it is no surprise 
that stories about death that include bodily descriptions 
would have many words related to horror.  
The tools of FCA and Galois lattices used in this poster 
are	flexible.		It	is	this	author’s	belief	that	additional	ap-
plications will be found, both for other literature and for 
other types of texts.
References
Baayen, R. H. (2002).  Word Frequency Distributions. 
New York: Springer.
Carpineto, C. and Romano, G. (2004). Concept Data 
Analysis: Theory and Applications.  Chichester: Wiley.
Ganter, B. (1984). Two basic algorithms in concept 
analysis.  Technical Report FB4 – Preprint No. 831, TU 
Darmstadt, Germany.
Grossman, D. A. and Frieder, O. (2004).  Information 
Retrieval: Algorithms and Heuristics, 2nd Edition.  Dor-
drecht: Springer.
Hoffman, D. (1990). Poe Poe Poe Poe Poe Poe Poe. 
New York: Marlowe & Co. 
Widdows, D. (2004). Geometry and Meaning. Palo 
Alto: Center for the Study of Language and Information.
DIGITAL HUMANITIES 2009
Page 320
“Song(s) of Myself”: Flexing 
Leaves of Grass
Olin Bjork 
University of Texas, Austin 
olin.bjork@gmail.com	
Scott Herrick
In his monograph Radiant Textuality, Jerome Mc-Gann	reflects	on	the	first	decade	of	work	on	the	Ros-
setti Archive at the Institute for Advanced Technology 
in the Humanities and regrets their “failure to consider 
interface in a serious way….when we worked out the 
archive’s	 original	 design,	we	 deliberately	 chose	 to	 fo-
cus on the logical structure and to set aside any thought 
about the Interface for delivering the archive to its us-
ers.  We made this decision in order to avoid committing 
ourselves to a delivery mechanism.”1 Accordingly, the 
site’s	 design	 editor,	 Bethany	 Nowviskie,	 predicts	 that	
the interface will “always have something of a tacked-
on quality.”2 Such an outcome, Matthew Kirschenbaum 
argues,	is	consistent	with	the	standard	workflow	of	Digi-
tal Humanities projects: “Too often put together as the 
final	 phase	 of	 a	 project	 under	 a	 tight	 deadline	 and	 an	
even	tighter	budget,	the	interface	becomes	the	first	and	
in most respects the exclusive experience of the project 
for its end users.”3 This assessment suggests that future 
editors and directors should consider making interface 
design a preliminary stage in the process of constructing 
electronic editions and archives.
In 2004, Professor John Rumrich and myself, then a 
graduate student at the University of Texas at Austin 
(UT-Austin),	 decided	 to	 reshuffle	 the	 priorities	 of	 the	
Digital Humanities in two ways: by considering inter-
face design before text encoding and by privileging 
pedagogical applications over scholarly ones.  Noting 
that most electronic editions and archives have either 
deliberately	or	unreflectively	adopted	the	standard	win-
dows-based interface design of non-literary Web sites, 
we resolved to make our digital classroom edition of 
John	Milton’s	Paradise Lost (hereafter, PL) resemble a 
book lying open on a table.  This approach, we hoped, 
would increase readability for students and appeal to 
bibliophile instructors.  But we refused to rely solely on 
visual usability—we wanted to integrate an audio track 
of the poem with our electronic text.  To this end, we 
recruited several colleagues at UT-Austin to make a re-
cording	of	one	of	the	epic’s	twelve	books.		From	2005	
to 2007, with the support of a UT-Austin Liberal Arts 
Instructional Technology Services (LAITS) grant, we 
developed a prototype “audiotext,” which we demoed at 
the 2007 Digital Humanities conference (DH07).  In our 
abstract, we offered the following rationale for the edi-
tion’s	synthesis	of	audio	and	text:
Whether students run through excerpts from PL in a soph-
omore survey or pore over the entire epic in an upper-di-
vision	course,	they	famously	find	Milton’s	poetry	difficult	
to	 follow.	 Instructors	 usually	 assume	 that	 this	 difficulty	
owes	to	its	unfamiliar	ideas	and	Milton’s	intimidating	eru-
dition—and surely these are part of the problem. But we 
have found that when students hear an instructor declaim 
passages from PL as they follow along in their textbooks, 
the thrust of the lines suddenly becomes plainer. Recent 
research on multimedia learning indicates that distinct, 
additive cognitive pathways mediate the aural and visual 
reception of language (Mayer 2001). Reading and listen-
ing to the same text demonstrably improves understand-
ing and recall.4
The prototype uses Adobe Flash technology to synchro-
nize the audio playback with a karaoke-style highlight 
that moves over each line as it is spoken.  Since the con-
ference, two more books have been added to the project 
Web site at http://www.laits.utextas.edu/miltonpl. The 
PL audiotexts are used by several instructors each se-
mester in a range of British literature courses at UT-Aus-
tin.  Student survey data and instructor comments have 
been overwhelmingly positive.
In 2007, Professors Coleman Hutchison and Michael 
Winship, Americanists intrigued by the interface con-
cept, applied for and received an LAITS grant to cre-
ate	an	audiotext	of	Walt	Whitman’s	“Song	of	Myself,”	
arguably the best-known long-form poem in American 
literature.  The rationale they stated in their grant pro-
posal was not overcoming a syntactical barrier to com-
prehension so much as countering the problem that 
Whitman’s	long,	idiosyncratic	lines	and	extensive	use	of	
poetic catalogs can *look* like prose.  Hearing as well 
as seeing the text, Hutchison and Winship argued, should 
help students gain an understanding of the aural register 
of the poem and thereby become receptive to the con-
catenation	of	Whitman’s	 aural	 and	visual	 effects.	 	The	
editors also proposed that six different readers would 
each voice the entire poem, whereas in the PL audio-
texts each character in the poem is voiced by a different 
reader.  The project would thus highlight an understud-
ied aspect of the poem: despite its title, “Song of My-
self” is a polyvocal poem, one in which distinct personae 
and voices compete.  Furthermore, by offering a menu 
of six audio tracks from which a user can select at any 
time, changing tracks without interruption, the new in-
terface would call attention to the subjective nature of 
such vocalization factors as accent, pronunciation, and 
DIGITAL HUMANITIES 2009
Page  321
emphasis.  This “remixability” addresses an issue raised 
by otherwise enthusiastic listeners/observers at DH07 
and elsewhere concerning the PL prototype.  Although 
Professor Rumrich and I considered our audio track to 
be	no	more	“accurate”	a	rendering	of	Milton’s	lines	than	
the visual representations we included in the illustration 
section, we overlooked the possibility that a single audio 
recording would give the impression that we were claim-
ing	to	have	realized	Milton’s	auditory	intentions,	just	as	
a single critically edited text strikes some textual critics 
as more autocratic than an archive of versions.  
Hutchison and Winship hired me, now a postdoctoral 
fellow	at	Georgia	Tech,	as	the	project’s	technical	editor.	
The project manager, Emily Cicchini, hired Scott Her-
rick, a systems analyst for the Division of Instructional 
Innovation and Assessment at UT-Austin, as lead devel-
oper.  Herrick had experience constructing a streaming 
flash	architecture	for	the	Aswaat Arabiyya project (http://
www.laits.utexas.edu/aswaat/).  In order to minimize the 
time necessary to load six audio tracks, we decided to 
stream the audio tracks from a Flash Server instead of 
embedding the audio in the Flash movie, as had been 
done in the PL prototype.  Herrick and I were aware that 
the choice of Flash as the core technology might prove a 
liability	for	the	project’s	long	term	sustainability	as	well	
as its adaptability to other contexts and purposes, but we 
had developed a familiarity with the platform and were 
not	confident	that	we	could	achieve	the	desired	anima-
tion and synchronization effects using open-source alter-
natives.  Although the Flash Player is a free download, 
it is proprietary to Adobe, whose standard Integrated 
Development Environment (IDE) for Flash is a closed, 
commercial	product.		Adobe	released	the	specifications	
for its playable Flash formats (SWF and FLV) in 2006, a 
move that some believe was intended to discourage the 
development of a free and open-source alternative while 
increasing the number of third party applications capable 
of	generating	SWF	files.
Lately, Adobe has been promoting Flex Builder, an al-
ternative Flash IDE built on the open source Eclipse 
platform yet itself proprietary.  The new IDE resembles 
a Java development suite and combines pre-built appli-
cation components with MXML, an XML-based mark-
up language, and ActionScript 3.0, an object-oriented 
scripting language.  Flex Builder is designed to appeal 
to programmers and designers of Rich Internet Applica-
tions who were never comfortable with the animation 
and movie metaphors of the timeline-based standard 
Flash IDE.   Noting that Adobe is currently offering Flex 
Builder for free to educators, Herrick suggested that we 
build the “Song of Myself” audiotext in that environ-
ment.  To minimize the constraints of Flash, we sepa-
rated all of our content from the interface.  The audio 
tracks stream from the server as needed, while the text 
is imported into the interface from a TEI (Text Encoding 
Initiative)	XML	file	 at	 runtime.	 	 Suloni	Robertson,	 an	
LAITS graphic designer and artist, executed the back-
ground image, which resembles an opened copy of the 
original edition of Leaves of Grass, published in 1855.
The poem that would later be titled “Song of Myself” 
opens this volume, and the beta of our audiotext version 
is now available at http://www.laits.utexas.edu/leavesof-
grass.  In addition to the audio controls, the application 
includes a zoom function and an options panel with a 
search engine and selection tools.  In the next phase of 
the project, we will add explanatory notes and an ani-
mated page-turn effect.  Ultimately, we plan to include a 
full transcript, critical apparatus, and facsimile images of 
the entire book.  At that point, the audiotext will become 
a	 full-fledged	 scholarly	 edition	 without	 compromising	
its pedagogical utility or its innovative interface design. 
The editors of the Walt Whitman Archive have gener-
ously allowed us to use their transcript from a copy of 
the 1855 edition (http://www.whitmanarchive.org/pub-
lished/LG/1855/whole.html) as the basis for our elec-
tronic text.  Professor Hutchison has had conversations 
with these editors about adapting the audiotext concept 
to	create	a	more	robust	front-end	option	for	the	archive’s	
resources.  To this end, we are exploring the possibility 
of using an open-source alternative to the Flex IDE to 
create a set of audiotext templates that could be used by 
other projects.
Notes
1Jerome McGann, Radiant Textuality: Literature after 
the World Wide Web (New York: Palgrave, 2001), 141.
2Bethany Nowviskie, “Interfacing the Rossetti Hyperme-
dia Archive,” (paper, Humanities and Technology Asso-
ciation Conference, Charlottesville, VA, October 2000), 
http://www.iath.virginia.edu/~bpn2f/1866/dgrinterface.
html. 
3 Matthew G. Kirschenbaum, “‘So the Colors Cover the 
Wires’:	Interface,	Aesthetics,	and	Usability,”	A Compan-
ion to Digital Humanities, ed. Susan Schreibman, Ray 
Siemens, John Unsworth (Oxford: Blackwell, 2004), 
525.
4Olin R. Bjork and John P. Rumrich, “The Paradise Lost 
Flash Audiotext” (poster, ACH/LCC Digital Humanities 
conference, Champaign-Urbana, IL, June 2-8, 2007), 
http://www.digitalhumanities.org/dh2007abstracts/xht-
ml.xq?id=216.
DIGITAL HUMANITIES 2009
Page 322
Can Pliny be one of the muses? 
How Pliny could support 
scholarly writing
John Bradley 
King’s	College	London 
john.bradley@kcl.ac.uk 
In Bradley 2008a the software Pliny (Pliny 2007) is described as a tool to support traditional scholarship, 
which, in turn, is assumed to be based on the reading 
of primary and secondary texts and the eventual writ-
ing of new secondary texts that describe an interpretation 
that	has	emerged	 in	 the	scholar’s	mind	as	s/he	worked	
with his/her materials (p. 266).  In that paper I described 
mechanisms – there called affordances – that Pliny pro-
vides to support personal annotation and the use of per-
sonal annotation to support the development of an inter-
pretation.
Fig. 1 (taken from my presentation at the DH2008 con-
ference: Bradley 2008b) shows, in a way somewhat dif-
ferent from that shown in Bradley 2008a, where Pliny 
is	meant	to	fit	in	the	three-stage	processed	described	in	
Bradley 2008a.
Fig. 1: Pliny and a scholar’s personal space
Perhaps	this	figure	makes	it	more	evident	that	Pliny	fits	
between two borders that separate public and personal 
space: one that takes non-personal materials as inspira-
tion in (on the left) and a second one (on the right) that 
puts personal materials out into the public domain again 
in the form of a book or article.  In past work on Pliny 
I have focused primarily on the boundary on the left in 
Fig. 1 and on how the computer might support the devel-
opment of a personal interpretation (in the middle).  For 
this	poster	I’d	like	to	extend	some	thinking,	and	promote	
some thinking by others, about the boundary between 
the public and personal space that is shown on the right 
– output from Pliny rather than input into it.
Section 6 of Bradley 2008a described the beginnings 
of	a	kind	of	strategy	for	how	to	use	Pliny’s	 interpreta-
tion development affordances to support the writing of 
a traditional presentation.  It also touches on the way 
that Pliny can export its formal model of the materials 
it holds about an interpretation into a computer-ontolog-
ical-kind of format – currently Topic Maps (Biezunski et 
al., 2002).  There is certainly interesting further work to 
do to explore the potential of the formal output of Pliny 
materials to represent scholarly thinking in, say, XML, 
but this is not the focus of this poster. Instead, I would 
like to focus on strategies that could be built into Pliny to 
assist its user in the transformation of materials that s/he 
has put into Pliny into a traditional prose text.
The task of putting ones thoughts into the linear order of 
a text that is understandable to others is quite evidently 
a	difficult	one,	 and	although	a	2D	Pliny-like	 represen-
tation of the things one wishes to discuss is helpful, it 
seems, by providing a holistic personal overview of your 
materials that could be used to guide writing, the task 
of taking this overview and transforming it into prose 
is	 still	 difficult.	 	 	There	 seem	 to	 be	 two	 issues:	 one	 is	
captured	by	the	seeming	difficulty	of	fully	representing	a	
2D hierarchical Pliny reference space as a 1D (temporal) 
hierarchical	object	that	is,	superficially	at	least,	the	basis	
of scholarly writing.  The second relates to what hap-
pens to personal materials such as the kind of notes one 
makes in Pliny – insights perhaps – when they become 
public objects.  Catherine Marshall (Marshall 1998, pp. 
40-42, and again in Marshall and Brush 2004) categories 
personal annotation, for example, as different from pub-
lic annotation – more informal, more terse, likely often 
even enigmatic to the outside reader – and it is in this 
difference that perhaps the challenge of transforming a 
collection of these things into prose meant to be read by 
others also resides.
There is, of course, a substantial amount of work that 
theorises about the act of writing and reading.  I confess 
that I am not in a position to fully take all of this in, 
although among my readings in this area I have found 
the work of George Landow (see, for example, Landow 
1997), to be as close as anything else I have read that 
tries to blend theoretical and practical in ways that pro-
vide useful pragmatic clues about the process. My aim 
with Pliny is primarily practical rather than theoretical, 
although I would wish, where possible, to do work here 
that would be informed by whatever practical insights 
can be drawn from the theorising that has been done 
about	 scholarly	 writing.	 	 An	 alternate,	 and	 definitely	
more practical stream of thinking comes from computer 
science in the work of Marshall, Frank Shipman and oth-
ers on software like VIKI and VITE and with social sci-
DIGITAL HUMANITIES 2009
Page  323
ences theorists in their thinking about tools to support 
qualitative analysis (see Shipman et al 1995, and Hsieh 
and Shipman 2000 for a computer science perspective 
and Pandit 1996 for an overview of that from the so-
cial sciences). In between, perhaps, is the work of Linda 
Flower, who in Flower 1988 recognises the rhetorical el-
ement in the act of transforming ideas into prose.  Much 
of	Flower’s	article	is	based	around	the	transformation	of	
what	 she	 calls	 the	 “writer’s	web	of	 purpose”	 (pg	532-
534): a verbal image that seems compatible to the graph-
oriented model for ideas that Pliny supports.
Work	 in	 Pliny	 so	 far	 in	 this	 area	 has	 been	 influenced	
mostly	 by	 some	 of	Marshall	 and	 Shipman’s	work	 and	
has centered on recognising the challenges inherent in 
taking a 2-D Pliny representation of an interpretation 
and expressing it satisfactorily in the seeming essentially 
1-D temporal context of scholarly writing. The Pliny 
user	can,	of	course,	create	an	object	specifically	to	rep-
resent a piece s/he is writing, assemble issues of interest 
and	play	around	with	them	to	find	a	set	of	relationships	
between them that work best, and then use the resulting 
2-D space to think about a 1-D ordering that will guide 
her/his writing. Pliny can currently look for and exploit 
in a rudimentary way such visual structures that the user 
has created to guess how the space might be best mapped 
into one dimension, and work is now underway to en-
hance	Pliny’s	ability	in	this	area	through	applying	some	
of the facilities described in writings about VIKI.  Some 
work has been done as well to assist the user in manag-
ing the mix of public and private objects in his/her Pliny 
connection so that ones aimed at the public are exported 
to be taken up in the writing of an article.
The questions to be asked, then, are (a) what are the in-
tellectual challenges to taking a holistic 2D model of an 
interpretation of the kind one can build in Pliny and turn-
ing it into a text, and can Pliny assist this in ways better 
than it does now, and (b) what issues arise in the trans-
formation of private materials such as one accumulated 
in Pliny into a public text, and can Pliny help there more 
than	 it	 does	 now.	 	 I’d	 be	 delighted	 to	 hear	 ideas	 from	
conference attendees.
References
Biezunski, M., Newcomb, S., and Pepper, S. (ed.) 
(2002). ISO/IEC 13250 Topic Maps (2nd edn). Inter-
national Organization for Standardization. Online at 
http://www.1.y12.doe.gov/capabilities/sgml/sc34/docu-
ment/0322_files/iso13250-2nd-ed-v2.pdf	 (accessed	 7	
Novemberr 2008).
Bradley, John (2008a). Thinking about Interpretation: 
Pliny and Scholarship in the Humanities. In Literary and 
Linguistic Computing Vol. 23 No, 3, 2008, pp. 263-79. 
doi: 10.1093/llc/fqn021. Online at http://llc.oxfordjour-
nals.org/cgi/reprint/fqn021?ijkey=3UzJDubDB0FRQcR
&keytype=ref . (accessed 7 November 2008).
Bradley, John (2008b), “Playing together: modular 
tools and Pliny”. Peer reviewed paper given in the Dig-
ital Humanities 2008 conference, University of Oulu, 
Oulu, Finland, 28 June 2008.  A draft version of this is 
online at http://pliny.cch.kcl.ac.uk/docs/oulu-paper.html. 
Accessed 7 November 2008.
Flower, Linda (1988). “The Construction of Purpose in 
Writing and Reading”. In College English, Vol. 50. No. 
5 9Sept. 1988). pp. 528-550.
Hsieh, Hao-wei and Frank M. Shipman III (2000). 
“Vite: A Visual Interface Supporting the Direct Manipu-
lation of Structured Data Using Two-Way Mappings”. 
In Procedings of the IUI Conference, New Orleans. pp. 
141-48.
Landow, George P. (1997). Hypertext 2.0: Being a Re-
vised, Expanded Edition of Hypertext: the Convergence 
of Contemporary Critical Theory and Technology. Balti-
more, MD: John Hopkins University Press.
Marshall, Catherine C. (1998). “Towards an ecology 
of hypertext annotation”. In Proceedings of HyperText 
98. New York: ACM. pp. 40-9.
Marshall, Catherine C. and A.J. Bernheim Brush. 
(2005). “Exploring the Relationship between Personal 
and Public Annotations”. In Proceedings for the JCDL 
’04 conference. New York: ACM.
Pandit, Naresh R. (1996). “The Creation of Theory: A 
Recent Application of the Grounded Theory Method”. 
In The Qualitative Report, Volume 2, Number 4, De-
cember, 1996.  Online at http://www.nova.edu/ssss/QR/
QR2-4/pandit.html.
Pliny 2007. Pliny project homepage.  Online at http://
pliny.cch.kcl.ac.uk/index.html
Shipman, Frank M., Catherine C. Marshall, and 
Thomas P. Moran (1995). “Finding and Using Implicit 
Structure in Human-Organized Spatial Layouts of Infor-
mation”. In Proceedings of CHI 95, Denver, Colorado, 
May 7- 11, 1995, pp. 346-353
DIGITAL HUMANITIES 2009
Page 324
The Harvester of Iconclass 
Metadata: a web service for 
subject	classification	and	subject	
retrieval in cultural heritage 
information systems  
Hans Brandhorst 
Arkyves 
jpjbrand@xs4all.nl	
Etienne Posthumus
Summary 
The importance of controlled vocabularies for the cataloguing of literary and visual sources by mu-
seums, libraries, and archives can hardly be exaggerat-
ed. Important organizations like the Library of Congress, 
the Getty Foundation, and OCLC have been promoting 
standardization	 by	 making	 thesauri	 and	 classification	
systems available online. However, complying to such 
a standard often means transferring descriptors from the 
online vocabulary system to another application, thereby 
isolating them from their context. While this is annoying 
during the production phase, it is fatal at retrieval time, 
because an end user should have access to the context 
of a descriptor while querying a catalogue. This poster 
demonstrates a web service for Iconclass, a multilingual 
subject	classifica	tion	system	for	cultural	content	that	is	
used from Finland to Italy and from Germany to the US, 
which solves this problem. The service makes the Icon-
class system available as an ‘add on’	to	data	base	man-
agement systems and online electronic catalogues. The 
Iconclass metadata harvester uses OAI-PMH to gather 
Iconclass codes, supporting special retrieval browsers 
within local websites, while creating a single access 
point for thematic searches across multiple online data-
bases. 
1 The authority paradox 
When	using	an	online	classification,	a	thesaurus	or	some	
other form of controlled vocabulary for cataloguing, we 
are sooner or later confronted by what could be labelled 
the authority paradox. The core of this paradox is that 
using a terminology authority in essence means select-
ing specific terms from the authority system and copy-
ing them to records of a local catalogue. However, the 
instru ments that are available to cataloguers to select the 
most appropriate descriptors from the authority system, 
are not automatically available to the end user of a cata-
logue. 
By	definition	vocabularies	like	classifications	or	thesauri	
—or, perhaps more fashionably, ontologies—are sys-
tems with a structure. This structure may be more or less 
complex, but at the very least there will be some basic in-
ner organization. An example of such an organizational 
structure is that of the broader and narrower terms of 
a hierarchy. Simple hierarchical subordination is only 
one	 of	 various	 techniques	 to	 help	 a	 user	 find	 descrip-
tors.	Keywords	may	be	added	to	help	the	user	find	the	
most appropriate concept; cross references may connect 
related concepts; redirects may point from non-preferred 
to preferred terms; scope notes explain the intention of a 
term, while links connect concepts across the languages 
of a multilingual system... these are just a few examples 
of addi tional, often quite sophisticated instruments to 
help one make the most of the authority system. No mat-
ter how simple or complex the structure of the author-
ity system is, its default use will often be limited to the 
transfer of the standardized term for an artist, a place-
name, a profession, or an iconographical feature from 
the system to the catalogue. It would obviously cause 
an enormous amount of redundancy if one were also to 
copy all of the parent terms in a hierarchy, let alone if 
one were to transfer all the terms to which a chosen de-
scriptor may be related—if that were at all possible. 
Based on the simple fact that it copies its descriptors 
from ‘authority system X, Y or Z’,	a	catalogue	may	claim	
to be compliant with that authority. However, this proce-
DIGITAL HUMANITIES 2009
Page  325
dure of transferring terms, iso lating them from the struc-
ture that embeds them, clashes with the purpose of com-
plying with an authority system. Or, at the very least, the 
resulting catalogue will fail to offer to its end users the 
features	that	turn	authority	systems	into	actual	‘systems’.	
2 SKOS version of Iconclass as an external 
thesaurus add on 
How the web service of Iconclass—in a SKOS version 
—functions as an add on	may	be	efficiently	demonstrat-
ed with the help of the preceding illustration, a screen-
shot of the Imdas database man agement system which 
offers Iconclass as an external thesaurus to its users. 
Using a simple i-Frame and a local style sheet, Iconclass 
is shown—here in its German form—as a tree of con-
cepts inside the Imdas application. Any browse action 
refreshes the content of the tree by triggering a request 
for updated information to the Iconclass server. 
3 Restoring the link for retrieval: the 
Harvester for Iconclass Metadata (HIM) 
The previous illustration suggests how terms may be 
transferred from the external authority system to the lo-
cal database at production time. It also suggests that the 
transfer of the alphanumeric nota tion that accompanies 
every concept and assigns it a unique place in the hierar-
chy,	suffices	to	link	a	database	record	to	a	concept	and	its	
complete context in the vocabulary system. It does not 
illus trate how the link between the individual terms that 
are copied to the application database and the Iconclass 
system is restored at retrieval time. 
Before we look at how the Harvester for Iconclass Meta-
data (HIM) service restores this link, we should list some 
rather obvious assumptions: 
A.  The catalogue is available on the internet or at least 
on an intranet that is linked to the internet. 
B.	All	 items	 in	 the	 catalogue	 are	 identifiable	with	 the	
help of some unique property, e.g. an inven tory number. 
C. This unique property can be used to retrieve an item 
from the catalogue. 
Needless to say that the assumption about the Iconclass 
system is that this is a web service, perma nently avail-
able on the internet. It may be consulted by human cata-
loguers, but it is also available for information exchange 
between computers. 
Back to the question of how to restore the link between 
the copied terms in an application database and the Icon-
class	system’s	server.	Actually,	the	answer	is	quite	sim-
ple. Although it is theoretically possible to enrich a cata-
logue by absorbing major parts—or even the whole—of 
the Iconclass sys tem, by far the easier strategy is to re-
verse the procedure, limit the information stored in the 
local database to Iconclass notations and then enrich the 
Iconclass system with information about the catalogue. 
What makes it so easy to export information about a 
catalogue that uses Iconclass for its subject access, is 
the	simple	fact	that	Iconclass	is	a	classification	system.	
Therefore, every concept in the system, with all of its 
links to other concepts and its translations, corresponds 
to	a	single	code,	or	‘notation’.	Like	barcodes	or	ISBN-
numbers these notations are thus very concise containers 
of in formation. These concise containers can easily be 
harvested using the Open Archives Initiative’s Protocol 
for Metadata Harvesting (OAI-PMH) or customized 
variants thereof. 
Although	 the	 Iconclass	codes	and	 the	unique	 identifier	
of the object (e.g. a catalogue item) to which they have 
been assigned can be supplemented with other types of 
metadata,	 the	Iconclass	codes	and	the	identifier	are	the	
most essential requirements for the harvesting service to 
work. 
The illustration above summarizes in a single picture the 
essential elements of the service. What you see there is 
the	first	row	of	a	thumbnail	gallery	incorporated	in	the	
French Emblems website at Glasgow University (http://
www.emblems.arts.gla.ac.uk/french/search.php). Above 
the thumbnails you see the concept “song-birds: crow 
25F32(CROW)” highlighted. Its broader terms are listed 
above it. Below it, its narrower terms that were actually 
used for this catalogue are also listed. Whenever a con-
DIGITAL HUMANITIES 2009
Page 326
cept is clicked, the corresponding Iconclass notation is 
sent to the central Iconclass server. The server then re-
turns	 the	object	 identifiers	 to	 the	 local	database.	These	
identifiers	 are	 subsequently	 used	 to	 retrieve	 the	 corre-
sponding objects from the local database. In the small 
box at the top simple and complex keywords searches 
may be entered in the various languages of Iconclass, i.e. 
English, German, Italian, or French (partial translations 
exist in Finnish and Dutch).
By providing Iconclass through a web service, all users 
have access to the same, i.e. the present version of the 
system. Editorial changes are instantly available. If a lo-
cal database merely stores codes, its users have access to 
the various languages of the system, and the full context 
of	every	concept—an	efficient	way	to	overcome	the	au-
thority paradox. 
The aggregator website www.arkyves.org—a single ac-
cess to over 150,000 objects indexed with Iconclass—
will be shown as part of the poster, in addition to the 
Iconclass web service.
Iconclass computing: 
Arkyves 
W.G. Plein 124 
1054 SG Amsterdam 
The Netherlands 
tel +31 20 616 1039 
e-mail:	info@arkyves.org 
website: http://www.arkyves.org 
 
Iconclass content: 
Rijksbureau Kunsthistorische Documentatie 
PO Box 90418 
2509 LK The Hague 
The Netherlands 
tel +31 70 33 39 777 
fax +31 70 33 37 789 
e-mail:	iconclass@rkd.nl 
website: http://www.rkd.nl
Appendix—reaction to reviewers’ comments
Review 1: A somewhat outdated description of the basic 
idea of the Harvester of Iconclass Metadata can be found 
at: http://mnemosyne.org/IIHIM/overview.rst.html 
Although some details and names have changed, the 
underlying principle is unchanged: classification	codes 
and a unique	identifier for the item to which the codes 
(desciptors) have been assigned are harvested and stored 
in a central database. The codes are parsed and interpret-
ed.	All	of	its	implicit	properties	(textual	definitions	and	
their translations, keywords, hierarchical links, cross ref-
erences, etcetera) are then extracted from the Iconclass 
datafile	 and	 linked	 to	 the	 code.	All	 search	 and	browse	
actions	 at	 the	 client’s	website	 are	 then	 compared	with	
the information stored in the central database and results 
are	 sent	 back	 to	 the	 client’s	 system.	This	 procedure	 is	
necessary for two reasons: A) technically Iconclass is a 
complex system and it would be very expensive—and 
redundant—to create search and browse software for 
each client system; and B) for a single catalogue item 
many descriptors (sometimes dozens of codes) may be 
used simultaneously. The textual information and other 
properties implicit in all codes that were assigned to a 
single item have to be made available to the end user 
simultaneously.	The	way	to	manage	this	efficiently	is	by	
storing all information in a central database.
Due to special features that were designed prior to the 
digital age, Iconclass is technically more complex than 
most	classifications,	so	 if	 the	software	works	 for	 Icon-
class—which it does—it can in all likelihood cope with 
other	classification	systems	as	well.	The	add-on	is	a	pro-
prietary tool, but it is made available for free to any insti-
tution which is prepared to share (part of) its (meta)data.
Review 2: Since there is only one online version of the 
vocabulary all changes are made centrally. Most chang-
es	will	be	additions	of	more	specific	concepts,	 so	 they	
won’t	 affect	 existing	 documents.	At	 most	 older	 docu-
ments	will	not	take	full	advantage	of	increasing	specific-
ity in later versions of Iconclass, unless they re-edit cer-
tain descriptions. Existing concepts have almost never 
been withdrawn or given new meaning in the 35 years of 
Iconclass’	usage	history.
There is no room here to expand on the idiosyncracies 
of	Iconclass	as	a	classification,	but	their	presence	is	well	
documented in earlier literature (in particular in spe-
cial issues of Visual Resources). The complexity of the 
code	structure	sets	Iconclass	rather	apart	as	a	classifica-
tion. However, there are some parallels with biomedical 
searching	 techniques	 (Collexis,	 fingerprinting,	 Knowl-
ets, WikiProfessional) that may be worth investigating. 
It	goes	without	saying	that,	financially,	these	tools	are	in	
a very different league...
The ambition of HIM is not to be innovative per se, but 
to offer a cost-effective solution to a problem—“how to 
make the most of the use of Iconclass”—that would oth-
erwise require expensive software and the reinvention of 
the wheel for every collection using the system. In the 
world of the Humanities that in itself may be seen as an 
innovation.
DIGITAL HUMANITIES 2009
Page  327
Modelling the Prosopography of 
the Royal Portuguese Court in 
the Sixteenth Century
Andreia Carvalho 
King’s	College	London 
andreia.carvalho@kcl.ac.uk
 
The poster will describe the digital component of the 
research project developed in the course of my PhD in 
Digital	Humanities	at	King’s	College	London.
The project features a prosopographical study of the high 
officials	of	the	Royal	Court	during	the	reign	of	John	III,	
king of Portugal (1521-1557). The poster will demon-
strate how a relational model was used for that purpose: 
the research aims at identifying a particular group of 
people, detecting their socio-economic characteristics, 
and uncovering their patterns of behaviour in the context 
of early modern court politics.
Despite being a small country, in the sixteenth century 
Portugal possessed a large royal court. I am interested 
in	 the	 human	 configuration	 of	 this	 structure;	 who	 are	
the men who govern the kingdom and surround the 
king? This question can be addressed through an anal-
ysis	of	 their	offices	and	of	 their	social	and	economical	
background. A further analysis of the nature of the re-
lationship between these actors and their (collective or 
individual) relationship with the king will also provide 
insights	on	the	nature	of	influence	and	the	negotiation	of	
power in royal courts during the Renaissance.
The ultimate aim of the project is to publish an online 
version of the database, which will make it available to 
other researchers as well as the general public. However, 
this poster will describe my research rather than focus on 
the implementation of that database.
The use of relational databases for prosopographical 
purposes has been a recurrent practice in historical re-
search since the 1980s. At the Centre for Computing 
in the Humanities there are several projects devoted to 
prosopography. Both the Prosopography of Anglo-Sax-
on England (PASE) and the Prosopography of the Byz-
antine World (PBW) have now become leading resourc-
es in the academic world and exemplars in this practice. 
Having	emerged	from	the	specific	needs	and	constraints	
required by historians, the databases have now evolved 
in order to allow new avenues of enquiry that go beyond 
the initial aims of the researchers. Both projects have be-
come became laboratories for the relational modelling of 
historical sources. This led to the development to what 
Bradley and Short called a ‘highly structured approach 
to	historical	data’,	or	‘new-style	prosopography’	(Brad-
ley and Short 2005). 
One of the major challenges of this project was to test, 
adapt and revise the relational model used in PASE and 
in PBW.
A prosopographical database is essentially comprised 
of three elements: sources containing ‘factoids’ about 
persons. These elements should provide a guide to the 
modelling	stage.	The	poster	will	focus	on	specific	issues:
1. How to incorporate different types of sources? The 
research will include published sources, original 
(manuscript) archival material and edited manu-
scripts. The sources are of a wide range, mainly 
literary and administrative. This diversity chal-
lenges, for instance, notions of authorship. Who is 
the author of a royal letter: the king, the person who 
drafted	the	letter,	or	all	the	officials	who	authorized	
and signed the document?
2. How	 to	 deal	 with	 ‘factoids’	 involving	 several	 ac-
tors/agents? In the relational databases developed at 
King’s	College	London,	a	factoid	is	an	‘assertion	by	
a	source	about	one	or	more	persons’	 (Bradley	and	
Short 2001). The database revises the solution ad-
opted	in	PASE	and	PBW,	where	an	‘event’,	although	
being	a	‘factoid’,	was	effectively	separated	from	the	
‘factoid’	table.	The	model	presented	here	reconfig-
ures these tables and adopts a different approach. 
The solution was to introduce an intermediary table 
called	‘PersonInFactoid’,	which	combines	‘factoids’	
and persons, associated with an authority table that 
specifies	the	role	played	by	each	person	in	a	given	
‘factoid’.
3. How	 to	 organize	 the	 different	 types	 of	 ‘factoids’?	
This is done by the use of authority lists that in fact 
can	‘manage’	the	‘factoids’.	The	database	retains	the	
authority tables used in PASE while simplifying and 
at the same time tightening their structure.
4. Finally, can it be possible to design a database that 
allows users to search for things not envisaged when 
doing the initial research? In fact, either the devel-
oper or the future users might want to access further 
data.	Although	the	diverse	types	of	factoids	reflect	
the	 specific	 research	 questions	 of	 the	 project	 they	
do not compromise the assertions conveyed by the 
sources – these are kept in the factoid table.
DIGITAL HUMANITIES 2009
Page 328
The process of analysis and database design produced 
a	 ‘lighter’	 and	modified	version	of	 the	PASE	database	
structure. The poster will show the different tables cre-
ated and their relationships, highlighting the problems 
and constraints of modelling the sources to the digital 
model. The input process will be done using a source-
driven approach, that is, the information in the database 
will be information found in the corpora of sources used.
References
Prosopography of Anglo-Saxon England (PASE): http://
www.pase.ac.uk (accessed 14 November 2008).
Prosopography of the Byzantine World (PBW): http://
www.pbw.kcl.ac.uk (accessed 14 November 2008).
Bradley, J. and Short H. (2001). Using Format Struc-
tures to create complex relationships: the prosopography 
of the Byzantine Empire – a case study, http://www. cch.
kcl.ac.uk/legacy/staff/jdb/papers/pbe-leeds/body.html, 
accessed 14 November 2008.
Bradley, J. and Short H. (2005). Texts into Databases: 
The Evolving Field of New-style Prosopography, Liter-
ary and Linguist Computing 20: 3-24.
Inventing the Future of AI  
for Games: 
Lessons from EMPath
Sherol Chen  
University of California, Santa Cruz 
sherol@soe.ucsc.edu
The	space	of	Narrativity	has	had	the	benefit	of	being	explored for centuries. Technology, however, intro-
duces a new factor to pioneer by expanding the possibili-
ties of both the experiencing and the telling of a story. 
Advancements of AI provide a multitude of techniques 
for the process of authoring and the actual authoring of 
stories in games. This process, however, is not so straight 
forward, and has proved to be extremely challenging in 
practice. Narratives in games, although sharing similar 
narrative qualities of its medium predecessors, deliver 
a highly interactive experience, making games a matter 
of new media and new analysis. The purpose of this talk 
is to demonstrate and give an idea of what the contrast 
is between the theories and models created in academia 
and the practice of industry in using technology to ex-
press compelling and believable experiences.
Introduction
With emphasis on the advancements of believability in 
areas of sound and scene, the advancements in the be-
lievability or complexity toward intelligent interactions 
within commercial video games are comparably lack-
ing.	Certain	games	aim	to	leave	the	story	up	to	the	user’s	
imagination, while others reduce user agency to create 
an artistically inspired story, and still, others are focused 
on procedurally creating a story that has both high user 
agency	and	dramatic	significance.	Arguments	 that	sup-
port story games demonstrating high levels of intelli-
gence claim that with the incorporation of Drama Man-
aging, Multi-Agent Systems, Reactive Planning, and 
other sub-areas of AI will come a new frontier in expe-
riencing	and	telling	stories	reducing	traditional	conflicts	
(Mateas 2001). On the other hand, game developers in 
industry are resistant against these ideas due to the com-
plications that come along with adding such methods. 
Efforts in commercial games have been more successful 
in making the most out of scripted stories, maximizing 
from	sound	and	scene,	 and	sacrificing	certain	qualities	
in order to strengthen others.  In addition to surveying 
the areas that are currently researched in academia and 
discussing games developed through research, there will 
be a demonstration of the EMPath project. EMPath is a 
prototype sized, adventure game that utilizes the Declar-
DIGITAL HUMANITIES 2009
Page  329
ative Optimization-Based Drama Manager (DODM). 
This prototype game, developed at UC Santa Cruz, is a 
real-time playable game that uses the DODM architec-
ture and has been tested on over 100 users. The purpose 
of this demo is to exhibit a novel AI-based approach to 
interactive storytelling, as well as provide a concrete ex-
ample of the challenges in the design process.  Figure 1 
is a screen shot of the dungeon map in the original EM-
Path game (Sullivan 2009).
Figure 1. The 5x5 map world of EMPath.
Drama Management
A drama manager (DM) monitors an interactive experi-
ence, such as a computer game, and intervenes to shape 
the	global	experience	so	that	it	satisfies	the	author’s	ex-
pressive	goals	without	decreasing	a	player’s	interactive	
agency.  Most research work on drama management has 
proposed AI architectures and provided abstract evalua-
tions of their effectiveness. A smaller body of work has 
evaluated the effect of drama management on player ex-
perience, but little attention has been paid to evaluating 
the authorial leverage provided by a drama management 
architecture: determining, for a given DM architecture, 
the additional non-linear story complexity a DM affords 
over traditional scripting methods. This poster will pro-
pose three criteria for evaluating the authorial leverage 
of a DM: 1) the script-and-trigger complexity of the DM 
story policy, 2) the degree of policy change given chang-
es to story elements, and 3) the average story branch-
ing factor for DM policies vs. script-and-trigger policies 
for stories of equivalent quality. Figure 2 illustrates the 
decision-tree representation approach in evaluating au-
thorial leverage.  For preliminary studies in evaluating 
complexity of the drama manager, thousands of partial 
stories were generated and used to train and test decision 
trees using the J48 algorithm implemented in Weka, a 
machine-learning software package.1 Partial stories (the 
independent variable) are represented by a set of boolean 
flags	indicating	whether	each	plot	point	and	DM	action	
has happened thus far in this story, and, for each pair of 
plot points a and b, whether a preceded b if both hap-
pened (Chen 2009).
Figure 2. Zoomed-in view of a decision tree that has 
learned from the DM.
Lessons from EMPath
Gains for creating a more intelligently interactive story 
need to be proven through subject testing and other types 
of evaluation.  In particular, there are two metrics that 
need to be established: one to show overall improved 
experience, and one to show the lightening of authorial 
burden.  These approaches are demonstrated in previ-
ous experiments with EMPath; however, the prototype 
games	that	are	feasible	for	research	have	difficulty	dem-
onstrating	 significant	 results	 due	 to	 the	 scale	 of	 these	
smaller games.  Other challenges that arise from imple-
menting and evaluating AI systems are as follows (Chen 
2009):
•	 Authoring: Incorporating an AI system creates the 
burden of domain understanding, forcing an author 
to break traditional habits in authoring.
•	 Evaluation Measures: Story qualities must be 
mathematically represented in order to show im-
provement or to encourage better interactions.
•	 Player Modeling: AI systems often depend on 
predicting and anticipating user actions and moti-
vations.  The system would need to model human 
tendencies mathematically.
•	 Simulation: Experiments	 designed	 to	 run	 offline,	
according	 to	 the	 system’s	 user	 model	 are	 needed	
to provide authorial leverage and sanity checks for 
story evaluation comparisons.
•	 User Testing: Users need to notice a difference in 
their overall experience when using the AI system 
versus not using the AI system. Experiments need to 
DIGITAL HUMANITIES 2009
Page 330
be carefully designed to show improvement in the 
user data.
•	 Game Design: The game must be designed to be 
able to demonstrate such differences.  In general, 
games that are created for research purposes are of-
ten	not	expansive	or	 large	enough	to	show	signifi-
cant results.
•	 Trail Blazing: Finally, a great challenge in creat-
ing AI systems is that evaluation measures and user 
study approaches have not been rigorously tested 
for these purposes
Dimensions of Narrativity & Interactivity
Taking another look at Narratology, establishing a more 
complete understanding of narrative may help amelio-
rate some of the challenges in designing and testing AI 
systems.  If there is a better understanding of the objec-
tives that an AI system is aiming toward, then the space 
of story that it creates may be more easily evaluated.  For 
instance,	reducing	the	space	of	possible	stories	by	fixing	
the ontological variations in an interactive story space 
reduces the variety of experiences, but results in a more 
focused	output.	With	a	fixed	and	linear	diegetic	universe,	
both	the	author’s	artistic	vision	is	maintained	and,	as	a	
result,	the	dramatic	significance	of	the	vision.	Research	
can	 begin	 by	 finding	ways	 of	 delegating	 types	 of	 dis-
course	actions	and	performing	them	on	a	fixed	set	of	plot	
points contingent on the actions of the user in trying to 
optimize user agency under those conditions.
Figure 3. Adaptations of Ryan’s 8 narrative dimensions.
By establishing narratives as a relationship among scalar 
properties,	Marie-Laure	Ryan’s	dimensions	of	narrativ-
ity gives a solid means to compare and analyze interac-
tive narratives (Ryan 2006).  For Ryan, her dimensions 
are more to establish conditions for the mark of the nar-
rative.  For the purpose of the discussion, a variation of 
Ryan’s	dimensions	will	be	used	analyze	the	experiential	
variations that result from interactivity in narratives. 
Figure 3 visually illustrates the analogous dimensions.
The new adaptation, instead of being a model for narra-
tives, will serve as a model for interactive experiences. 
The four dimensions are: temporal (an axis to situate 
time), event space (an axis for delineating the occurrenc-
es in a story space), foci (the experiential views from 
intelligent existents or perspective), and discourse (an 
axis for determining the means of how a story is told). 
For further explanation of the relationship between these 
dimensions,	it	helps	to	“fix”	or	ignore	one	or	more	of	the	
dimensions.  
Conclusion
Overall, this presentation will deliver a broad under-
standing of the ways in which AI can integrate with 
interactive stories and create a diversity of experiences 
and outcomes. In contrast to commercial games, the pro-
cess of interactive storytelling provides insights into the 
endeavors of universities and research institutions pio-
neering the area through advancements in AI. Addition-
ally, there will be a live demo of the EMPath project in 
conjunction	to	a	discussion	of	the	difficulties	and	chal-
lenges in the counter-intuitive design process of a story 
that utilizes concurrent technologies of AI. Ultimately, 
this demo will be a case study towards gaining a deeper 
understanding of the challenges to be faced before what 
is possible in storytelling can be made practical through 
the intersections of the arts, sciences, industry, and aca-
demia.
Notes
1http://www.cs.waikato.ac.nz/ml/weka/
References
Chen, Nelson, Mateas (2009). Evaluating the Authorial 
Leverage of Drama Management. AAAI Spring Sympo-
sium, Interactive Narrative II.
Chen, Sullivan, Nelson, Wardrip-Fruin, Mateas (2009). 
Intelligent Interactive-Stories: Theory versus Practice. 
Game Developers Conference, San Francisco, CA.
Mateas (2001), A preliminary poetics for interactive 
drama and games. Digital Creativity, vol 12, number 3.
Ryan (2006). Narrative, Media, and Modes: Avatars of 
Story. University of Minnesota Press.
Sullivan, Chen, Mateas (2009). Abstraction to Reality: 
Integrating drama management into a playable game 
experience. AAAI Spring Symposium, Interactive Nar-
DIGITAL HUMANITIES 2009
Page  331
rative II. Fine Rolls in Print and on the 
Web: Progress on a Reader 
Study
Arianna Ciula 
King's College London 
arianna.ciula@kcl.ac.uk		
Tamara Lopez 
King's College London 
tamara.lopez@kcl.ac.uk	
Faye Thompson 
King's College London 
faye.thompson12@hotmail.co.uk
This poster will report on progress made in the Reader Study related to the use of the Henry III Fine Rolls 
project resources. A poster on the research framework 
and phase one of the study was presented at the Digital 
Humanities conference in 2008. Since then (June 2008), 
the authors have continued work on later phases of the 
study,	 completing	 the	 document	 analyses,	 defining	 a	
research	profile	out	of	the	sample	data,	and	evaluating,	
developing and revising the methodological framework. 
This	poster	will	report	and	comment	on	these	findings.
Project Summary
The Henry III Fine Rolls (http://www.frh2.org.uk) is a 
collaborative project funded by the Arts and Humanities 
Research Council (UK) between the National Archives 
in the UK, the History and Centre for Computing in 
the	Humanities	departments	at	King’s	College	London,	
the Department of History and American Studies at 
Canterbury Christ Church University. At the core of the 
project is the study of the  medieval primary sources 
known as the Fine Rolls. Dating back to the 1170s, these 
documents written in Latin on membranes of parchment 
were issued by the royal Chancery to record agreements 
made with the king to pay a certain sum of money for 
specific	benefits.	Those	that	witness	writs	for	the	whole	
reign of Henry III (1216-1272) were never published 
properly and in their entirety before the Henry III Fine 
Rolls project took the initiative to do so.
The outcome of the project, currently in its second phase, 
is both a resource website and a set of printed volumes1 
containing the calendared edition (an English summary 
of the Latin records) of the Fine Rolls. 
DIGITAL HUMANITIES 2009
Page 332
Reader Study  
This dual editorial effort has raised questions about 
presentational formats pertinent to the two media and 
presented challenges for the historians as well as the 
digital humanities researchers involved in the project 
and the publisher. The process of negotiating and 
envisaging	different	‘material’	solutions	for	the	two	
types of published resources presented the authors 
with an opportunity to examine this parallel production 
process and to inquire about the social processes that 
influence	the	ways	in	which	scholarship	is	embedded	in	
published outputs, both in print and digital form. 
Therefore, the reader study was conceived to:
•	 Reflect	 on	 the	 material	 actualisations	 of	 the	 Fine	
Rolls hybrid edition;2 
•	 Evaluate the use of this hybrid edition to tackle old 
and new research questions;
•	 Establish the effectiveness of particular features of 
each medium in creating bridges between the print 
and web resources;
•	 Articulate general heuristics that can be used in the 
design of other hybrid digital humanities projects.
As summarised below, the poster presented in 2008 
developed a framework for this exploratory study using 
established data collection methods from information 
seeking research, in particular those developed for the 
report Scholarly Work in the Humanities and the Evolving 
Information Environment (Brockman, Neumann, Palmer 
& Tidline, 2001).
This methodological approach is qualitative, striving not 
to	achieve	statistical	significance	but	rather	to	develop	a	
nuanced picture of the work performed by scholars using 
this and like material. The decision to use qualitative 
methods has forced the authors to limit the number of 
participants to a small group, collecting information by 
email and face-to-face interviews and through content 
analysis of research material.  The participant sample 
throughout draws from members of the Fine Rolls 
research team who have written materials using the 
edition, but also on a small sample of researchers from 
the wider scholarly community who perform text-based 
archival research with documentary primary sources that 
date back to the late Middle Ages.
Analysis	 of	 the	 materiality	 of	 the	 fine	 rolls	 edition	
in 2008 was coupled with an initial examination of 
distributed questionnaires and document analysis of 
material written by study participants on the use of this 
or similar editions. Emerging trends from this exercise 
suggested that scholars do posses a high familiarity 
with primary sources through continuous direct access. 
In addition, the authors found that participants tend to 
cite a core set of sources over and over again. These 
key sources mainly consist of facsimiles, editions or 
reference works that give access to a substantial corpus 
of primary sources in various forms.  
This	 poster	 will	 report	 more	 specifically	 on	 how	
scholars from this community perceive and overcome 
obstacles related to access or use of materials in 
performing research,  and share general attitudes about 
what constitutes a successful research process (Dervin, 
1992; 1998).  In addition, it will identify and report on 
information	 patterns	 for	 the	 community,	 defining	 the	
types and formats of sources that are used, including 
insights into use of access resources (Brockman et 
al., 2001; Palmer, 2005).  Findings  from this phase 
will	 confirm	 whether	 or	 not	 the	 research	 process	 of	
participants	 conforms	 to	 Palmer’s	 mode	 of	 access	 for	
humanities scholars (2005).
In addition, this poster will isolate the bridging tactics 
(Wilson, 1999) employed by scholars when moving 
between digital and print materials, in order to more 
fully develop a lexicon of connective structures for 
hybrid editions.  Finally, a preliminary testing plan for 
evaluating the design of the Fine Rolls will be presented.
Bibliography
Brockman, W., L. Neumann, C. L. Palmer, T. J. Tidline. 
(2001) Scholarly Work in the Humanities and the Evolv-
ing Information Environment. Digital Library Federation 
Council on Library and Information Resources, Decem-
ber 2001, Council on Library and Information Resourc-
es.
Ciula,	A.	and	Lopez	T.	“Reflecting	on	a	Dual	Publica-
tion: Henry III Fine Rolls Print and Web”.  Literary and 
Linguistic Computing (forthcoming).
Dervin,	B.	(1992)	“From	the	mind’s	eye	of	the	user:	the	
sense-making qualitative-quantitative methodology”. 
In Glazier, J.D. and Powell, R.R. (eds.) Qualitative Re-
search in Information Management.Libraries Unlimited, 
1992. pp. 61-84.  Pre-print accessed 14 November, 2008 
at: http://www.ideals.uiuc.edu/html/2142/2281/Dervi-
n1992a.htm.
Dervin, B. (1998) “Sense-making theory and practice: 
an overview of user interests in knowledge seeking and 
use”.  Journal of Knowledge Management. Vol. 2 No. 2, 
December 1998.  pp. 36-46.
DIGITAL HUMANITIES 2009
Page  333
Dryburgh, P., Hartland, B. eds., Ciula, A., Vieira J. M. 
tech. eds. (2007) Calendar of the Fine Rolls of the Reign 
of Henry III preserved in The National Archives. Volume 
1: 1216–1224. Woodbridge: Boydell & Brewer.
Dryburgh, P., Hartland, B. eds., Ciula, A., Vieira J. M. 
tech. eds. (2008) Calendar of the Fine Rolls of the Reign 
of Henry III preserved in The National Archives. Volume 
II: 1224–1234. Woodbridge: Boydell & Brewer.
Finkelstein, D. and McCleery, A. (2002) The book his-
tory reader. London: Routledge.
Palmer, C. L. (2004) “Thematic Research Collections”. 
In Schreibman, S., R. Siemens and J. Unsworth (eds.) A 
Companion to Digital Humanities. Blackwell Compan-
ions to Literature and Culture. Oxford: Blackwell, 2004. 
pp. 349-365. 
Palmer, C. L. (2005) “Scholarly work and the shaping of 
digital access”. Journal of the American Society for In-
formation Science and Technology Vol. 56 No 11, 2005. 
pp. 1140-53.
Wilson, T. (1999) Models in information behaviour re-
search. Journal of Documentation Vol. 55 No 3, 1999. 
pp. 249-270.
Notes
1The	 first	 two	 volumes	 (Dryburgh	 et	 al.,	 2007;	 2008)	
have already been printed in collaboration with the pub-
lisher Boydell & Brewer, and a third is under production: 
another	five	volumes	will	be	published	to	complete	the	
edition.
2That is, extant both in a physical and virtual environ-
ment.  For a fuller treatment of this, see Ciula and Lopez 
(forthcoming).
Digital Tools from the Center  
for History and New Media: 
Present and Future
Dan Cohen 
Center for History and New Media 
dan@dancohen.org
Tom Scheinfeldt 
Center for History and New Media 
tom@foundhistory.org
Jeremy Boggs 
Center for History and New Media 
jboggs@gmu.edu	
Dave Lester 
Center for History and New Media 
dave@omeka.org	 
The need to “develop and maintain open standards and robust tools” was one of eight key recommenda-
tions in Our Cultural Commonwealth: The Report of the 
American Council of Learned Societies Commission on 
Cyberinfrastructure for the Humanities and Social Sci-
ences. It has become increasingly apparent that if digi-
tal humanists are to have the right tools for their work, 
they will have to build some of them themselves. Indeed, 
many humanities scholars and social scientists have al-
ready built valuable digital tools over the years, includ-
ing software and systems for text processing, markup, 
visualization, metadata generation, cataloging, GIS, and 
a number of other humanities-related tasks. 
In the past seven years, the Center for History and New 
Media at George Mason University has expanded its fo-
cus from creating web resources for educational, schol-
arly, and general audiences to include several major 
open source software projects. Making this transition 
has involved a considerable challenges, but also some 
significant	opportunities.	In	this	panel	the	directors	and	
developers of these digital tools will speak about the 
challenges of creating software for scholarship, research, 
web publishing, and course management; discuss where 
their tools are today and how others can get involved in 
their production; and talk about future plans and trends. 
Three projects will be highlighted:
Zotero
For research management, CHNM has created Zotero 
(http://zotero.org), an easy-to-use yet powerful research 
DIGITAL HUMANITIES 2009
Page 334
tool that helps users gather, organize, and analyze sourc-
es (citations, full texts, web pages, images, and other 
objects), and share the results of their research in a va-
riety of ways. An extension to the popular open-source 
web browser Firefox, Zotero includes the best parts of 
older reference manager software (like EndNote)—the 
ability	 to	 store	 author,	 title,	 and	 publication	fields	 and	
to export that information as formatted references—and 
the best parts of modern software and web applications 
(like iTunes and del.icio.us), such as the ability to inter-
act, tag, and search in advanced ways. Zotero integrates 
tightly with online resources; it can sense when users 
are viewing a book, article, or other object on the web, 
and—on	 many	 major	 research	 and	 library	 sites—find	
and automatically save the full reference information for 
the	 item	 in	 the	correct	fields.	Since	 it	 lives	 in	 the	web	
browser, it can effortlessly transmit information to, and 
receive information from, other web services and appli-
cations;	since	it	runs	on	one’s	personal	computer,	it	can	
also communicate with software running there (such as 
Microsoft	Word).	And	it	can	be	used	offline	as	well	(e.g.,	
on a plane, in an archive without WiFi).
Zotero has surpassed the milestone of over a million us-
ers and more than a hundred colleges and universities 
now actively recommend Zotero to their students, facul-
ty, and staff. The Zotero project has received recognition 
in	PC	Magazine’s	“Best	Free	Software”	issue.	Through	
the open source community, Zotero has been translated 
into 40 languages, ranging from Arabic to Vietnamese.
Omeka
For web publishing, CHNM has created Omeka (http://
omeka.org), a free and open source collections based 
web-based publishing platform for scholars, librarians, 
archivists, museum professionals, educators, and cul-
tural	enthusiasts.	CHNM’s	new	open	source	platform	for	
publishing collections and exhibitions online hit a major 
milestone with the release of the 0.10 Beta release. De-
signed for cultural institutions, enthusiasts, and educa-
tors, Omeka is easy to install and modify and facilitates 
community-building around collections and exhibits. It 
is designed with non-IT specialists in mind, allowing 
users to focus on content rather than programming. Its 
“five-minute	setup”	makes	 launching	an	online	exhibi-
tion as easy as launching a blog. Omeka is designed with 
non-IT specialists in mind, allowing users to focus on 
content and interpretation rather than programming. It 
brings Web 2.0 technologies and approaches to academic 
and cultural websites to foster user interaction and par-
ticipation. It makes top-shelf design easy with a simple 
and	flexible	template	system.	Its	robust	open-source	de-
veloper	and	user	communities	underwrite	Omeka’s	sta-
bility and sustainability. Until now, scholars and cultural 
heritage professionals looking to publish collections-
based research and online exhibitions required either ex-
tensive technical skills or considerable funding for out-
side vendors. By making standards based, serious online 
publishing easy, Omeka puts the power and reach of the 
web in the hands of academics and cultural professionals 
themselves.
ScholarPress
As educators increasingly use blogs in their classrooms, 
CHNM has explored ways to provide open-source tools 
for educational blogging. Currently an unfunded, staff-
generated project, ScholarPress (http://scholarpress.net) 
is a development hub for educational and scholarly pl-
ugins for the WordPress blogging platform. ScholarPress 
currently offers two plugins: Courseware, which en-
ables teachers to manage a class with a WordPress blog 
with a schedule, bibliography, assignments, and other 
course information, and WPBook, which works with 
the Facebook Development platform to create an (add-
able by users within the site) using a Wordpress blog. 
Thus, someone using both Courseware and WPBook can 
create a WordPress blog, add a course schedule, read-
ing list, assignments, and annoucements, and share that 
information with students through the blog and through 
a Facebook application that students add. ScholarPress 
plans future plugins for grading, research, bibliography 
management, and is working to make plugins compat-
ible with multi-user versions of WordPress, so services 
like Edublogs can add the plugins and make them avail-
able to educations using their services.
“Digital Tools from the Center for History and New Me-
dia: Present and Future” will provide audience members 
with an introduction to each of these tools, the chal-
lenges involved in building each, the similarities and 
differences in building tools for different purposes and 
audiences, and some lessons learned in making the tran-
sition from web development to software development. 
The format for the session will feature short, 15 min-
ute presentations by four key members of the CHNM 
tool development team. CHNM director Dan Cohen will 
discuss Zotero, Managing director Tom Scheinfeldt and 
Omeka Developer Dave Lester will discuss Omeka, and 
Creative Lead Jeremy  Boggs will discuss ScholarPress. 
These short project-focused presentation will be fol-
lowed by a 15 panel discussion among the presenters in 
which they will discuss the similarities and differences 
between the three projects and some of the more general 
aspects of software development. The balance of the re-
maining time will be left to audience questions and an-
swers. All four participants live in the Washington D.C. 
area and will be available during the week of DH09. CVs 
DIGITAL HUMANITIES 2009
Page  335
Access versus Ownership
Navigating the Tension between Mass 
Digitization of Archival Materials and 
Intellectual Property Rights
Maggie Dickson
University of North Carolina, Chapel Hill 
mdickson@email.unc.edu
Amy Johnson
University of North Carolina, Chapel Hill 
johnsoal@email.unc.edu
Natasha Smith
University of North Carolina, Chapel Hill 
nsmith@email.unc.edu
Lynn Holdzkom
University of North Carolina, Chapel Hill 
uholro@email.unc.edu
Stephanie Adamson-Williams
University of North Carolina, Chapel Hill 
sadamson@email.unc.edu
The advent of digital technologies has presented ar-chivists with opportunities to provide unprecedent-
ed, and, increasingly, expected, online access to collec-
tions to their patrons. Some archival repositories are 
now exploring, and in some cases, such as the Archives 
of American Art Collections Online and the Library of 
Congress’s	 American	 Memory	 Project,	 engaging	 in,	
the mass digitization and Web presentation of their col-
lections, which allows patrons to perform much of the 
same	 research	 from	 their	homes	or	offices	at	 any	 time	
of day, for which they would traditionally have had to 
travel great distances and been subject to limited hours 
of operation.1 
In 2007 the Carolina Digital Library and Archives 
(CDLA) and the Southern Historical Collection (SHC) 
began work on a pilot project to explore the most effective 
methods of mass digitization and gather data on which a 
full-scale program could be founded. A 2-year, $300,000 
grant from the Watson-Brown Foundation of Thomson, 
GA funded the project. The correspondence series of the 
Thomas E. Watson Papers, housed in the SHC, was cho-
sen as the subject of the pilot project. Thomas E. Watson, 
for each are attached.
DIGITAL HUMANITIES 2009
Page 336
of Thomson, Ga., was a prominent Populist politician, 
author, and lawyer, and his correspondence series con-
sists of 8 linear feet of letters and related material written 
by Watson and his family, friends, and political and busi-
ness colleagues.  The date range of the correspondence 
series is 1873-1986, with the bulk of the letters dating 
between the 1880s and 1920s.
One of the challenges facing this project is navigating 
between providing comprehensive online access while 
at the same time respecting the intellectual property 
rights of any copyright holders who may be represented 
in the collection. Unpublished manuscript materials, 
such as those found in the Thomas E. Watson Papers, 
are protected by copyright for 70 years plus the life of 
the author. For us, that means that any letters written by 
a correspondent who died prior to 1939 (2009 being the 
projected publication date for our digital collection) are 
fair game under general copyright rules. However, any 
letters written by a correspondent who died after 1939 
are potentially still in copyright.
So, if we were not to claim any exemptions to copyright 
statutes, and we wanted to publish the entire correspon-
dence series on the Web under a strict interpretation of 
copyright law, we would need to identify all of the au-
thors of the letters in the series, determine the date of 
their deaths, locate their descendants if their death dates 
were after 1939, contact those descendants, and request 
and then obtain permission to use their deceased family 
members’	letters.
Even though we suspected this effort had little chance of 
success, we determined that because our project is not 
only about digitizing the Thomas E. Watson Papers, but 
also serves as a pilot for a much larger effort aimed at 
digitizing the entire SHC, we would attempt to do in-
tense copyright research on the materials in the series to 
investigate thoroughly this aspect of digitizing archival 
materials. Prior to beginning copyright research, we had 
already gathered basic metadata (correspondent and re-
cipient names, places from which letters were written, 
and dates, for example) from all of the 8400+ docu-
ments. It took us approximately 91 hours to go through 
the 8 linear feet of materials in the correspondence series 
to compile this data. From the information we gathered, 
we were able to condense and regularize the correspon-
dent list down to approximately 3300 names.
Using a variety of sources, including Wikipedia, the 
Social Security Death Index, Ancestry.com, and print 
references, we attempted to positively identify the 3300 
names.	What	resulted	was	a	list	of	3280	confirmed	and	
questionable	identifications,	and	24	unknowns	that	were	
simply impossible to identify. We were able to locate 
birth	or	death	dates	for	1709	of	the	identified	correspon-
dents, while for 1571 no dates were available via the 
consulted sources. Of the correspondents for whom we 
located dates, 1101 died after 1939, while 608 died dur-
ing or before 1939.
Of	 the	 positively	 identified	 individuals	who	 died	 after	
1939, we found that just over 50 had dedicated manu-
script collections (or materials in the manuscript col-
lections of other individuals) deposited in repositories. 
In these cases, we contacted the repositories, asking for 
their latest acquisition information, in the hopes that it 
might lead us to descendents of the correspondents from 
whom we could request permission to digitize their rela-
tive’s	materials.	In	most	cases,	no	information	was	avail-
able, and when it was, it tended to be outdated, often 
well over 20 years old. We were able to obtain current, 
dependable contact information for the descendents of 
only two of the more prominent correspondents – Upton 
Sinclair and Hamlin Garland, both of whom are well-
known writers with established literary estates.
The	fact	that	it	was	so	difficult	to	obtain	contact	informa-
tion for the descendents of people who are prominent 
enough to have dedicated manuscript collections indi-
cates that locating the descendents of the bulk of the cor-
respondents would be daunting and in most cases impos-
sible.
Extrapolating from our experience with the Watson cor-
respondence, we believe that an effort on the scale antic-
ipated in digitizing the entire SHC would be stymied by 
trying to do in-depth exploration of copyright status and 
attempting to obtain permission to digitize unpublished 
archival materials that are under copyright. If we hope to 
make mass digitization an integrated part of processing 
archival materials, it is simply untenable for us to con-
sider doing this type of research to determine and obtain 
copyright.
This does not mean that we should avoid digitizing ma-
terials which may still be in copyright, but it does mean 
that	we	need	to	find	a	different	way	to	approach	copy-
right law to accommodate our needs.
Relying on Fair Use
While copyright law was intended to protect creative ex-
pression, it was at the same time not intended to be an 
impediment to further creative expression. Because of 
that, there are limitations to exclusive rights and rem-
edies in sections 107 to 122 of the Copyright Act that 
allow for the use of copyrighted materials under some 
circumstances. As the current copyright law was written 
DIGITAL HUMANITIES 2009
Page  337
in 1976, however, its authors did not anticipate the ways 
in which digital technologies would change the poten-
tial uses of copyrighted materials, and we must interpret 
these limitations and remedies to determine which might 
best apply to the mass digitization of archival materials.
A close examination of the possible limitations and rem-
edies available to us in the law as it stands indicated that 
the most reasonable option for us is to use the fair use 
provision. Section 107 of the Copyright Act – Limita-
tions on Exclusive Rights: Fair Use, states that ‘use by 
reproduction in copies or phonorecords or by any other 
means	 specified	 by	 that	 section,	 for	 purposes	 such	 as	
criticism, comment, news reporting, teaching, scholar-
ship,	or	research,	is	not	an	infringement	of	copyright.’2 
The Supreme Court, in its ruling in favor of the defen-
dant in Stuart v Abend, stated that: ‘fair use … permits 
courts to avoid rigid application of the copyright stat-
ute	when,	on	occasion,	it	would	stifle	the	very	creativity	
which	that	law	is	designed	to	foster.’3 
Weighing our project against the four fair use factors4 
and taking into account the existing case law (of which 
very little applies to archives and special collections) we 
have developed an argument which we feel allows us 
to legally publish our digitized manuscript collections 
online. Unfortunately, the only way to know with cer-
tainty that a use is considered a fair one is to have it 
resolved in a federal court. The thought of such a court 
battle constitutes a worst-case scenario for us, but given 
the precedents already set by the courts, we are unlikely 
to become involved in such a situation. 
Given these circumstances, it is reasonable for us to con-
tinue to serve our patrons in the most effective ways pos-
sible by accepting this risk and forging ahead with mass 
digitization. In order to maintain the level of service 
researchers are increasingly coming to expect, it is im-
perative that archives and special collections forge ahead 
with mass digitization without fear of recrimination. 
References  
Copyright Act of 1976, 17 U.S.C. § 107
Erway, R., and Schaffner, J. (2007). Shifting Gears: Gear-
ing Up to Get Into the Flow. Report produced by OCLC 
Programs and Research. Published online at: http://www.
oclc.org/programs/publications/reports/2007-02.pdf
Stuart v Abend, 495 U.S. 207, 236 (1990)
Notes
1Ricky	Erway	and	Jennifer	Schaffner’s	paper,	“Shifting	
Gears: Gearing Up to Get Into the Flow,” discusses the 
changing expectations of archival user communities, as 
well as the changing role of the archivist in the face of 
developing digital technology.
2Copyright Act of 1976, 17 U.S.C. § 107. Limitations on 
exclusive rights: Fair use
3Stuart v Abend, 495 U.S. 207, 236 (1990)
4They are, in brief: 1. the purpose and character of the 
use, including whether such use is of commercial nature 
or	is	for	nonprofit	educational	purposes;	2.	the	nature	of	
the work itself [whether it is a factual or creative work]; 
3. the amount and substantiality of the portion used in 
relation to the copyrighted work as a whole; 4. the effect 
of the use upon the potential market for or value of the 
copyrighted work.
DIGITAL HUMANITIES 2009
Page 338
Implementing Greek 
Morphology
Helma Dik 
University of Chicago 
helmadik@mac.com	
Richard Whaling 
University of Chicago 
rwhaling@uchicago.edu
In this poster we discuss the nuts and bolts of our im-plementation	of	Greek	morphology	 in	 a	five-million	
word corpus, that of the Perseus Greek texts. Many dis-
parate elements, and the efforts of many different peo-
ple have come together in this project. Dik & Whaling 
(2008) describe how initial data was gathered from mul-
tiple sources; the current paper describes what went into 
the	final	product:	
Disambiguated Greek texts: Early Greek 
Epic and the New Testament
The two sources of data from which we were going to 
bootstrap	our	project	came	with	their	own	specific	fea-
tures: The two disambiguated corpora used different 
data	specifications,	which	had	to	be	compared	and	made	
uniform, and brought up to the standard that we want-
ed. However, it did bring us two large swaths of data 
of 350K words in total with which to seed our part-of-
speech analyser, TreeTagger.
Morphological analysis from the Perseus 
project
The training data alone were not going to be adequate 
to produce a full lexicon for TreeTagger. We decided to 
supplement	it	with	the	output	from	Perseus’s	Morpheus	
tool (Crane 1991) of all possible parses for the full cor-
pus.	 This	 greatly	 enhanced	 TreeTagger’s	 accuracy	 on	
rare words not encountered in the training data, but also 
generated many redundancies and inconsistencies which 
made it hard for TreeTagger to build a proper decision 
tree. It was a continuing dilemma to those involved in 
the project whether we should allow more correct input 
to eventually weed out the many incorrect parses, or to 
remove incorrect parses from the input directly. Some ef-
fort was made to remove incorrect input: A 28 MB lexi-
con was reduced to less than 23 MB, but this represented 
only a portion of problematic entries.
TreeTagger
TreeTagger (Schmid 1995) is the proprietary software 
we used to assign part-of-speech tags (in effect, a full 
morphological disambiguation in a ten-slot morphologi-
cal code plus a lemma). We trained TreeTagger in the 
first	 instance	 on	 the	 basis	 of	 the	New	Testament	 data,	
and added 40,000 words total in representative 1000 
word samples from the rest of the corpus. New sets of 
samples were prepared with TreeTagger disambiguation, 
for which we used earlier disambiguated samples, plus 
our initial New Testament samples, as input. 
Disambiguation (internal)
On the basis of earlier work on Czech and other languag-
es, we decided that 40,000 words would be an adequate 
sample. This disregarded the fact that most research in 
natural language processing is actually done on more 
homogeneous texts than our samples of Greek literature, 
such as Reuters news items. Clearly, a more homoge-
neous input makes for higher accuracy within the source 
corpus, but we had no such luxury. An early indication 
was the high accuracy rate achieved by TreeTagger when 
trained and tested on Homeric Greek, which is a highly 
homogeneous, formulaic, subset of our texts. Perhaps 
the Homeric corpus is in fact the best parallel to Reuters 
and similar corpora in modern languages - at least the 
accuracy was comparable. 
In more practical terms, undergraduate students of Greek 
were	hired	to	‘pick	the	right	parse’	from	among	possible	
parses	identified	by	TreeTagger.	The	disambiguation	in-
terface allowed the students to signal alternative parses 
or lemmas if none of the TreeTagger choices was accu-
rate.	Next,	in	an	‘admin’	layer,	items	about	which	there	
were disagreements among the students or about which 
comments were entered, were highlighted for review, so 
that the principal investigator could review these items 
especially, prior to feeding fully disambiguated texts 
back to TreeTagger. 
Implementation
The centerpiece of our implementation is a SQLite da-
tabase backend, containing the tokens and parses for the 
full corpus. It connects the three major components of 
the system: 
•	 The	original	Perseus	XML	files,	in	which	the	tokens	
have been given unique ids as follows, keeping in-
tact all previous markup: 
<w	id=”276565”>ὦ</w>	
<w	id=”276566”>ἄνδρες</w>
<w	id=”276567”>Ἀθηναῖοι</w
DIGITAL HUMANITIES 2009
Page  339
•	 TreeTagger, which accepts token sequences from 
the database and outputs parses and probability 
weights, which are stored in their own table. 
•	 PhiloLogic,	which	serves	as	a	highly	efficient	search	
and retrieval front end, by indexing the augment-
ed	XML	files	as	well	as	 the	contents	of	 the	linked	
SQLite	 tables.	 PhiloLogic’s	 highly	 optimized	 in-
dex architecture allows near-instantaneous results 
on	complex	inquiries	such	as	‘any	infinitive	forms	
within 25 words of (dative singulars of) lemma X 
and	string	Y’,	which	would	be	a	challenge	for	typi-
cal relational database systems. 
For a concrete example, in a standard PhiloLogic search 
box,	entering		‘lemma:μῆνις’	will	produce	this	word	from	
the	first	line	of	the	Iliad,	as	will	a	search	for	‘pos:*fa*’,	
as	will	a	search	for	the	original	string,	‘μῆνιν’.	Criteria	
can	be	combined	as	well,	so	that	‘lemma:μῆνις;pos:*fa*’	
produces only feminine accusative forms of the particu-
lar	lemma	μῆνις.
We continue to explore the possibility of natural lan-
guage searching as a substitute or alternative to this 
highly technical way of querying the corpus, and will 
demonstrate our progress on this front at the conference. 
The goal is to make it possible for users to type ‘femi-
nine	accusative’	as	opposed	to	‘pos:*fa*’,	which	will	re-
main daunting to all but the most determined. 
Conclusion
We are happy to have disambiguated a large corpus, 
making	available	for	the	first	time	a	large,	representative	
corpus of Classical Greek for morphological searching 
in addition to searching by string and by lemma — in-
tegrated into the existing reading and browsing environ-
ment for the texts. However, we are now also prepared 
to start crowd-sourcing the long tail of incorrect parses. 
Besides looking up the statistically most probable parse 
according to TreeTagger and other possible parses, users 
can	‘vote’	to	correct	TreeTagger’s	chosen	parses.	Once	
these votes have been inspected and accepted into the 
main	database,	future	updates	to	the	corpus	will	reflect	
both these local corrections and, over the full extent of 
the corpus, a more accurate TreeTagger. It is our hope 
that with the assistance of our users we will approach 
higher and higher levels of accuracy, making this tool 
ever more useful to scholars of Classical Greek. 
References: 
Crane, Gregory (1991). Generating and parsing classi-
cal Greek. In Literary and Linguistic Computing, 6(4): 
243-245, 1991.
Dik, Helma and Richard Whaling (2008) - Bootstrap-
ping Greek Morphology. Digital Humanities 2008. 
Schmid, Helmut (1995) - Improvements in part-of-
speech tagging with an application to German. In Pro-
ceedings of the ACL SIGDAT-Workshop. http://www.
ims.uni-stuttgart.de/ftp/pub/corpora/tree-tagger2.pdf
Website URL: http://perseus.uchicago.edu
DIGITAL HUMANITIES 2009
Page 340
Synergies: An Overview and 
Progress Report  
Michael Eberle-Sinatra 
Université de Montréal, President Synergies
michael.eberle.sinatra@umontreal.ca
This poster will offer an overview of the CFI-funded project Synergies: The Canadian Information Net-
work for Research in the Social Sciences and Humani-
ties, and offer a progress report on the developments that 
took	place	in	the	first	year	of	its	four-year	funding	cycle,	
to be accompanied by a live demonstration of the alpha 
version of the web-based search interface. 
Synergies is a four-year project intended to be a national 
distributed platform with a wide range of tools to sup-
port the creation, distribution, access and archiving of 
digital objects such as journal articles. It will enable the 
distribution and use of social sciences and humanities 
research, as well as to create a resource and platform for 
pure and applied research. In short, Synergies will be a 
research tool and a dissemination tool that will greatly 
enhance the potential and impact of Social Sciences and 
Humanities scholarship. 
Canadian social sciences and humanities research pub-
lished in Canadian journals and elsewhere, especially 
in	 English,	 is	 largely	 confined	 to	 print.	 The	 dynamics	
of print mean that this research is machine-opaque and 
hence invisible on the internet, where many students 
and scholars begin and sometimes end their background 
research. In bringing Canadian social sciences and hu-
manities research to the internet, Synergies will not 
only bring that research into the mainstream of world-
wide research discourse but also it will legitimize online 
publication in social sciences and humanities. The ac-
ceptance of this medium will open the manner in which 
knowledge can be represented. On one plane, research-
ers will be able to take advantage of an enriched media 
palette—color, image, sound, moving images, multime-
dia. On a second plane, researchers will be able to take 
advantage of interactivity. And on a third plane, those 
who query existing research will be able to broaden their 
vision by means of navigational interfaces, multilingual 
interrogation and automatic translation, metadata and 
intelligent search engines, and textual analysis. On still 
another plane scholars will be able to expand new areas 
of knowledge such as bibliometrics and technometrics, 
new media analysis, scholarly communicational analy-
sis and publishing studies. This poster will introduce the 
main goals of the Synergies project and the impact it will 
have on the production and dissemination of Canadian 
research. 
Scholarly research and communication are undergoing 
an evolutionary transformation. Research environments, 
scholarly communication, knowledge sharing and ser-
vices are moving to the digital and becoming network-
oriented. This evolution raises many new questions 
about models of knowledge sharing. Emerging research 
environments, data providers, publishers, and libraries 
will need to develop and deploy a wide variety of new 
resource models to address these new realities. These re-
source models will lower the barriers to access and ex-
ploitation of research and information resources, serve 
the needs of individuals and both general and specialized 
communities, and integrate new models of publication, 
annotation, communication and knowledge sharing. 
Synergies will provide a needed infrastructure for the 
Social Sciences and Humanities Research Council 
(SSHRC) to follow through its in-principle commitment 
to open access and facilitate its implementation by ex-
tending the current venues and means for online pub-
lishing in Canada. With Synergies in place the funding 
of journals based on dissemination effectiveness rather 
than sales levels will become both feasible for journals 
and possible as an evaluative criterion for SSHRC fund-
ing. The Canadian Federation for the Humanities and 
Social Sciences, with a membership of over 30,000, has 
also recently adopted a position in favor of open access 
and indicate the role that Synergies can play. 
Alongside a new web interface and tools for accessing 
information produced in Canada (with the alpha version 
to be demonstrated alongside the poster), Synergies will 
be a digital publishing platform for scholarly publica-
tions,	with	 its	 first	 goal	 being	 to	 offer	 digital	 publish-
ing services prepared to international standards with the 
lowest cost possible for the editorial production side. 
This project will thus work as a sustainable, open, e-
publication infrastructure for the academy. 
In sum, this poster will contain an overview of the proj-
ect,	and	progress	reports	on	its	five	regional	components.	
Synergies is	the	result	of	a	collaboration	among	five	core	
universities which have been working together for sev-
eral years. With each partner bringing its own expertise 
to the initiative, a genuine collaboration resulted in an 
infrastructure which was conceived from the start as 
truly scalable and extendable. Each regional node will 
integrate the input of current and future regional partners 
in the development of Synergies, thus continuing to ex-
tend its pan-Canadian dimension. For instance, the PKP 
project will be introduced within the broader context of 
DIGITAL HUMANITIES 2009
Page  341
scholarly communications. The Ontario region will be 
presented as a case study, with particular emphasis on 
project integration with Scholars Portal, a digital library. 
(The other three regions will also be included in this 
progress report to the Digital Humanities community.) 
Fostering Cultural Literacies 
through Digital Scholarship:  
The Yaddo Archive Project 
and Yaddocast as Multimodal 
Humanities Projects
Richard L. Edwards 
Indiana University
edwards7@gmail.com
Micki McGee 
Fordham University
mmcgee@fordham.edu
Yaddo and the Yaddo Archive
Founded	 in	1900	by	 the	financier	and	philanthropist	Spencer Trask and his wife Katrina, Yaddo is an art-
ists’	community	located	on	a	400-acre	estate	in	Saratoga	
Springs, New York.  Its mission is to nurture the creative 
process by providing an opportunity for artists to work 
without interruption in a supportive environment.  Since 
its	 first	 official	 season	 in	 1926,	 the	 artists’	 retreat	 has	
hosted more than 5,500 artists, writers, composers and 
other	 creatie	workers	 including	 legendary	figures	 such	
as Milton Avery, James Baldwin, Leonard Berstein, 
Truman Capote, Aaron Copland, Philip Guston, Patricia 
Highsmith, Langston Hughes, Ted Hughes, Alfred Ka-
zin, Ulysses Kay, Jacob Lawrence, Carson McCullers, 
Sylvia Plath, Katherine Ane Porter, Mario Puzo, Clyf-
ford Still, and Virgil Thomson.
In	1999,	 in	 celebration	of	 the	community’s	 centennial,	
Yaddo and the New York Public Library entered into 
a	 unique	 partnership	 to	 ensure	 that	Yaddo’s	 archive—
more than 550 boxes of rare letters, journals, guest ap-
plications, photographs, artworks, sound recordings, and 
other ephemera—would be preserved for posterity and 
available to scholars and the public.  The Yaddo Records, 
now	housed	in	the	NYPL’s	Manuscript	and	Archives	Di-
vision, constitute a unique resource for scholars in the 
humanities and of organizational development in that 
they	 include	 intimate	 letters	 between	Yaddo’s	 director	
and	many	 of	 20th	 century’s	most	 distinguished	 artists	
and	 nearly	 complete	 records	 of	 the	 organization’s	 op-
erations from 1900 to the present.  In addition, Yaddo 
has kept detailed records of the arrivals and departures 
of its guests for nearly every year of operations, mak-
ing	charting	 the	organization’s	contribution	 in	building	
DIGITAL HUMANITIES 2009
Page 342
social relationships more viable than would otherwise be 
expected.
Multimodal Humanities and Yaddo:  
Making American Culture Exhibition
When Yaddo opened as an artist retreat in 1926, the New 
York Times hailed it as “a new and unique experiment, 
which has no parallel in the world of arts.”  That state-
ment is still true today.  Yaddo has not shed its experi-
mental roots, and as part of its ongoing outreach efforts, 
it has dynamically begun to embrace 21st century tech-
nologies and techniques.  And while the experience of 
the artists at Yaddo might not have dramatically changed 
in decades, the cultural outreach of Yaddo  has been en-
chanced by digital scholarship projects in the multimodal 
humanities.  David Theo Goldberg and Tara McPherson 
have advanced the concept of the “multimodal humani-
ties.”  As McPherson states, the multimodal humanities 
“bring together databases, scholarly tools, networked 
writing and peer-to-peer commentary while also lever-
aging the potential of the visual and aural media that so 
dominate contemporary life.”  In the case of Yaddo, that 
media archive can encompass textual, visual, gestural, 
and	aural	works	spanning	the	fields	of	literature,	paint-
ing,	composing,	choreography,	sculpture,	film,	video	and	
mixed media.
As part of efforts to foster culture literacy around the 
opening of a new exhibition on the history of Yaddo at 
the New York Public Library in October 2008, the Yaddo 
Archive Project (a database project) and Yaddocast (an 
enhanced podcast series) were both created.  In each 
project, database logics, participatory architectures, and 
interpretive spaces coalesce to tackle the complex ques-
tion	of	Yaddo’s	impact	on	American	culture.		While	there	
were different technical and production issues raised by 
these two projects, this paper will focus on the design of 
these projects as part of a larger epistemological ques-
tion around how digital projects in the multimodal hu-
manities mode can further our understanding of cultural 
literacies.
The Yaddo Archive Project
The Yaddo Archive Project (YAP) was not planned as 
the typical “exhibition website” that simply repurposes 
exhibition content with online presence.  Rather, YAP 
aimed to use digital technologies to map key informa-
tion in the Yaddo archive, from other records retained in 
Yaddo,	and	from	secondary	sources	(artists’	biographies,	
leters,	and	journals)	to	explore	how	Yaddo,	as	an	artists’	
community, created cultural capital by fostering social 
capital.
The planned key components of YAP are an interactive 
online platform that 1) charts the network of relation-
ships that made Yaddo a formidable force in 20th cen-
tury American arts and letters, 2) map the relationships 
that were forged during Yaddo visits that later impacted 
American arts and letters, 3) charts art works (primarily 
published	books,	as	those	are	most	readily	identified	by	
date and accessed via online catalogs such as the Library 
of Congress and Worldcat) made before, during, and af-
ter Yaddo visits with an eye toward demonstrating ef-
fects of a Yaddo residency or residencies on subsequent 
artistic productivity, and 4) an access-protected entry 
point where scholars and archivists familiar with Yaddo 
and its artists can contribute new information on the rela-
tionships between members of this community (see Fig. 
1 and Fig. 2).
Fig. 1. Yaddo archive Project wireframe, 2 degrees of 
relationship.
Fig. 2. Yaddo Archives Project:  Visit overlaps and 
relationships in table and timeline format
Yaddocast
In addition to the Yaddo Archive Project, the exhibition 
also led to the creation of another digital humanities 
project entitled Yaddocast.  Yaddocast is an enhanced 
podcast series on Yaddo artists.  Yaddocast was created 
to	tell	the	stories	behind	Yaddo’s	artist	guests	and	their	
creative processes.  Each episode of Yaddocast was pro-
DIGITAL HUMANITIES 2009
Page  343
duced as a scholarly investigation into creative activity 
associated	with	Yaddo,	and	the	project’s	utilization	of	a	
popular form of web-based media will be addressed as 
part of its pedagogical aim toward fostering cultural lit-
eracy.
Fostering Cultural Literacies through 
Yaddo-related Digital Scholarship
Multimodal humanities projects can be designed to take 
advantage of the complex database logics and immense 
bodies of knowledge related to the history of Yaddo. 
The cultural impact of Yaddo far exceeds the 5,500 art-
ists who have been guests in Saratoga Springs.  Artists 
associated	with	Yaddo	produce	cultural	works	that	influ-
ence other cultural producers, and members of the Yaddo 
interact with one another in novel and surprising ways. 
YAP, for example, utilizes the latest techniques to make 
those connections and relationships manifest.  Rather 
than	publishing	one	scholar’s	take	on	artistic	cross-pol-
lination at Yaddo, YAP is a dynamic hypermedia system 
that	can	be	reconfigured	based	on	different	searches	and	
metadata criteria.  Moreover, YAP is a read-write data-
base, where knowledgeable users or Yaddo artists can 
add further relevant information.
Second, Yaddo requires a rich media approach to repre-
sent its range of cultural artefacts.  Since Yaddo is not a 
doctrinaire artist colony, it supports all different kinds of 
artworks, aesthetic techniques and artistic styles.  Yad-
do’s	creative	output	and	the	richness	of	its	archives	are	
uniquely positioned to show a broad view of American 
culture since 1926.  But that broad purview also requires 
innovative approaches to representing a variety of me-
dia types and forms in digital scholarship.  This paper 
will address some of the techniques used in these two 
projects.
Third, YAP and Yaddocast engage in a hermeneutics of 
creativity activity.  The relationship between informa-
tion, form and aesthetics all matter in these multimodal 
humanities projects.  Rather than just being bits of cul-
tural data, the process behind the creative output and the 
relationships among Yaddo artists are important vectors 
of meaning, and part of the process of cultural literacy 
is not just an examination of the end products (books, 
films,	 dances,	 etc)	 but	 a	more	 thorough	 understanding	
of the usually hidden aspects of the creative process. 
The	artistic	legacy	of	Yaddo’s	guests	spans	almost	every	
art	field,	and	allows	for	a	richness	of	content	that	is	un-
matched by even large museums.  But unlike normal mu-
seum	collections,	Yaddo’s	archives	are	tied	to	a	specific	
space of creative endeavor.  That linkage is useful from 
a multimodal humanities development perspective since 
it allows for a new set of recombinant cultural possibili-
ties relating to how Yaddo operates as a social network 
of artists.
Taken as a whole, the digital humanities projects related 
to Yaddo begin to suggest that, within the digital human-
ities, there are opportunities for fostering cultural litera-
cies through collaborating with larger publics around 
web-based forms of scholarship that utilize architectures 
of participation.
References
Bordieu, P. (2002). “The Forms of Capital,” in The Soci-
ology of Economic Life, Mark S. Granovetter and Rich-
ard Swedberg, eds. Cambridge, MA: Westview Press, 
pp. 96-111.
DeCarlo, D. (2004). Extreme Project Management.  New 
York:  Wiley-Jossey.
Lin, N. (2001).  “A Network Theory of Social Capital,” 
in Social Capital:  Theory and Research, Nan Lin, Karen 
S. Cook, and Ronald S. Burt, eds. New Brunswick, NJ: 
Transaction Publishers.
McGee, Micki, ed. (2008). Yaddo:  Making American 
Culture. Columbia University Press.
McPherson, T. quoted in Spiro, L. (2008). “Doing Digi-
tal Scholarship.” http://digitalscholarship.wordpress.
com/2008/08/11/doing-digital-scholarship-presentation-
at-digital-humanities-2008/ (Accessed 14 November 
2008)
O’Reilly,	T.	(2004).		“The	Architecture	of	Participation.”	
http://www.oreillynet.com/pub/a/oreilly/tim/articles/ar-
chitecture_of_participation.html (Accessed 14 Novem-
ber 2008)
Unsworth, J. (2007). “Scholarly Primitives: what meth-
ods do humanities researchers have in common, and 
how	mght	our	tools	reflect	this?”	http://jefferson.village.
virginia.edu/~jmu2m/Kings.5-00/primitives.html (Ac-
cessed 14 November 2008)
DIGITAL HUMANITIES 2009
Page 344
Ask Not What Your Text Can 
do For You. Ask What You Can 
do For Your Text (a Dictionary’s 
perspective)
Carlos Monroy
Texas A&M University
cmonroy@csdl.tamu.edu	
Richard Furuta 
Texas A&M University
furuta@csdl.tamu.edu		
Filipe Castro 
Texas A&M University
fvcastro@tamu.edu	
 
1. Introduction
This	 paper’s	 title,	 a	 modified	 version	 of	 President	Kennedy’s	 well	 known	 quote,	 in	 a	 metaphorical	
sense suggests shifting the role texts play in a collection. 
This shift is based on the growing number of available 
tools that can be used to enhance textual analysis. We 
are investigating the effect of tools as generators of what 
can be done with the texts. We are not advocating a con-
ceptual change in the role and nature of the texts from 
the literary or textual studies perspective. Rather we sug-
gest a pragmatic role change. In doing so, we believe 
that new hypothesis about the content of the texts can be 
posited, or at least their use can be augmented.
Our motivation is based on our experience in the cre-
ation and use of a multilingual glossary of nautical terms 
for the Nautical Archaeology Digital Library (Monroy 
et al, 2006). In this context, we have seen two major 
benefits	from	the	glossary	that	affect	both	the	scholarly	
practices and the collection. First, it has enabled collabo-
ration among scholars and researchers geographically 
scattered. Second, how it has broadened the possibilities 
in the use and understanding of the textual materials—
shipbuilding treatises in our case.
2.  Dictionaries  
Dictionaries have been used extensively in numer-
ous digital humanities initiatives. The Perseus Project 
(Crane 2002) provides a good example of incorporating 
dictionaries in a classics collection. The idea behind a 
dictionary is very simple: an alphabetical list of words 
with	definitions.	Yet	it	has	great	potential	and	usefulness	
when used simultaneously with the contents of a digital 
collection.		Dictionaries	also	come	in	various	flavors—
bilingual or multilingual, thesaurus, specialized, illus-
trated, and encyclopedic—to name a few. With the use 
of information technology and the Internet, it has been 
possible to expand not only their use, but also the way 
they are created and edited.
Searching for a term in the on-line dictionary of the Real 
Academia de la Lengua Española (RAE, 2008), for ex-
ample, presents users with occurrences of that term in all 
the digitized dictionaries where it has ever been edited; 
see Fig. 1. Because all editions of the printed version of 
the dictionary have been digitized, it is possible to visu-
alize	the	evolution	of	the	definitions	of	a	given	term.	Al-
though this electronic version of the dictionary in itself 
is a great resource, one can imagine what could be ac-
complished if used in combination with a corpus of texts.
Fig. 1 A screen shot of the on-line RAE dictionary 
depicting occurrences of a term in the collection of 
digitized dictionaries, on the right is the image of a given 
occurrence
Arachne (Foertsch 2006) is an electronic repository 
(database) of the German Institute for Archaeology. 
Because archaeological objects are scattered across the 
world, Arachne provides multilingual access and thesau-
rus. The Getty Thesaurus of Geographical Names (Baca 
2004) is another good example of an external tool that 
can be incorporated into existing textual materials, en-
hancing searching and browsing.
The LEO on-line dictionary (LEO, 2008) was origi-
nally launched as a German-English dictionary in 1995. 
At present it includes German translations into French, 
Spanish, Chinese, and Italian. Since its beginning, two 
of the most remarkable accomplishments have been the 
DIGITAL HUMANITIES 2009
Page  345
integration of a larger and linguistic diverse editorial 
team. And the creation of new environments for search-
ing, using, and learning. For instance, the current version 
allows users to join groups and work together to learn 
the language; it also enables teachers to organize lessons, 
see Fig. 2.
Fig. 2 A screen shot of a bilingual dictionary—LEO—
depicting translations and definitions
3. Motivation
Although the use of tools for textual analysis is not a 
new concept in digital humanities, their emergence is re-
shaping not only the use of textual materials, but also the 
mere notion of the texts themselves. Impacting in the end 
how scholarly practices are conducted.
Traditionally, textual scholars can be seen as “consum-
ers” of the texts. They analyze, compare, and study their 
contents, how they relate to each other, and their histori-
cal and cultural contexts to name a few. Rockwell (2003) 
commenting on a well known discussion about two ap-
proaches to texts: as a hierarchical objects advanced by 
Renear, and as a performance proposed by McGann, 
states:
If	we	are	to	take	McGann’s	public	performance	of	a	read-
ing as an analogue for what we wish to achieve with these 
tools, we have to think not only about how we represent 
the text but also about the performance of analysis and the 
tools that are used to perform this analysis with a com-
puter.   
In	 this	 paper,	 following	 Rockwell’s	 statement,	 we	 de-
scribe a change in the role of textual scholars, from con-
sumers (users of the texts) into producers (augmenting 
the texts) with the use of tools. Our observations are 
based on the function external tools can play in augment-
ing the use of the texts; how they can be used; and what 
can be learned. Therefore, our goal is neither to propose 
a right approach, nor to compare approaches. 
We take this approach for two reasons. First, at a recent 
textual studies conference (CASTA, 2008) a participant 
asked one of the presenters regarding the numerous tools 
available to text scholars: “But don’t you think that quite 
often the problem with ‘tools,’ is precisely that there are 
too many of them; and we don’t know what to do?” This 
is an interesting question because—although coming 
from a literary scholar with strong background and ex-
pertise in using technology for textual studies—it shows 
the marked prominence of the role texts play in digital 
humanities, or at least in how humanists perceive their 
role. 
The second reason is based in our experience working in 
the creation of a multilingual glossary of nautical terms. 
The glossary has allowed the incorporation of a new 
layer to the original transcriptions. For example it would 
be possible to search for a given term in one language 
and retrieve occurrences in the transcriptions in multiple 
languages. Also categories associated to the terms can be 
used for retrieving occurrences in various contexts.
Used in the context of the Willa Cather Archive (The 
Willa Cather Archive, 2008), Evince—a non-invasive 
text analysis tool that mediates the integration of ana-
lytical data with the text—shows how a tool can enhance 
the textual materials. What is interesting in this case, is 
the fact that the tool is being used to augment the study 
of the texts, hence improving what can be learned from 
them. Discussing the use of Evince, (Jewell et al., 2008) 
state:
We posit that integration of analytical data with the read-
ing text will create new possibilities for interpretation in-
formed by textual data, as it will eliminate the need to 
enter a specialized environment.
4. Our Collection of Shipbuilding Treatises
Shipbuilding treatises are ancient technical texts, both 
printed or manuscript, that describe the conception and 
construction of ships, establish the required types and 
properties of the wood and building materials utilized, 
and sometimes describe the steps to be followed in their 
construction. Given their characteristics, these texts can 
be properly considered as ancient technical manuals. 
Our collection was started with three Portuguese trea-
tises obtained with permission from the Portuguese Aca-
demia de Marinha and National Library. At present our 
collection has grown to eleven copies; and includes ma-
DIGITAL HUMANITIES 2009
Page 346
terials in Portuguese, French, Italian, and Dutch, span-
ning a period from the late 16th to the early 18th centuries. 
Additionally, an English book is already digitized and 
ready to be added.
In terms of naval and seafaring dissemination, shipbuild-
ing treatises are priceless sources for scholars working in 
ship reconstruction and studying the evolution of ship-
building techniques. Moreover, the development of un-
derwater archaeology in the last 50 years propitiated the 
growth of the archaeological data corpus, which can now 
be tested against the textual evidence pertaining to the 
conception and construction of these complex machines. 
Nautical Archaeology students, on the other hand, study 
ship treatises as part of their curriculum. Finally, for the 
general public they are a great source of historical and 
cultural	contexts	in	which	seafaring	flourished.
5. The Tool—A Multilingual Glossary of 
Nautical Terms
The need for the creation of our glossary goes back to 
an English illustrated glossary included as appendix in 
an underwater archaeology book (Steffy, 1994). Tied to 
one	language	and	a	printed	medium,	the	glossary’s	limi-
tations were evident. But the most pressing reason was 
the various languages in which the texts in our collection 
were written. Further, the glossary is essential because 
nautical archaeology is a highly specialized domain 
where technical terms need to be explained in order to 
understand their meaning and context.
Fig. 3 A partial display of the NADL glossary depicting 
our model to represent a multilingual dictionary of 
nautical terms
Our model uses term as the atomic element. Each term in 
turn has an associated matrix where columns correspond 
to roles and rows to properties. Because we are working 
on a multilingual glossary, we decided to use properties 
to map languages, while roles map synonyms and spell-
ings respectively, see Fig 3. Each cell at the intersection 
of role and language can contain zero, one, or more val-
ues separated by the symbol |. This implies that each cell 
can be represented as a vector of values.
Our	approach	allows	scalability	and	flexibility.	For	ex-
ample, we had to add a new language—Venetian—since 
it was not originally considered, and was requested by 
one of the scholars. Adding the new language was a 
straightforward process. Similarly, adding new roles en-
tails the addition of more columns. In both cases both 
the architecture and the interface scale easily. From the 
implementation standpoint, we use a relational database 
for	storing	terms,	synonyms,	spellings,	and	definitions	in	
multiple languages.
Using Lucene—an open source full-text retrieval soft-
ware (The Apache Lucene Project, 2008)—we are pars-
ing texts and automatically creating links to the glos-
sary. Fig.4 depicts a screen shot of the treatises interface 
showing the image on the left, and the transcription on 
the right, with linked terms underlined in blue. Although 
this process might seem a simple one, implementing it 
turned out more complicated than expected. The two 
main reasons were multiple-word entries and the limita-
tions	on	Lucene’s	 stemmer	 to	handle	17th-century Por-
tuguese.
Fig. 4 The treatises interface depicting linked terms in the 
transcriptions
6. Conclusion  
Our Web-based interface has enabled scholars to work 
remotely in editing the glossary, expanding its contents 
and attracting other scholars. This collaboration goes be-
yond merely the editing of materials remotely. It has al-
lowed us to obtain materials from other libraries and also 
to engage the special collections library at Texas A&M 
in the acquisition of original materials.
Profiting	from	the	rich	illustrations	nautical	treatises	pro-
vide and the numerous ship models in our collection, we 
want, in the near future, to create multilingual illustrated 
DIGITAL HUMANITIES 2009
Page  347
dictionaries, linking them to the texts. As stated earlier, 
our	goal	is	not	to	redefine	the	role	of	texts	in	the	humani-
ties. But as our experience with the introduction of the 
multilingual glossary in NADL indicates, tools are shift-
ing the way texts are perceived.
7. Acknowledgements
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. IIS-0534314.
8. References
Baca, M. (2004). Fear of Authority? Authority Control 
and Thesaurus Building for Art and Material. Catalogu-
ing & Classification Quaretly Vol. 38, No. 3/4, pp. 143-
151.
Crane, G. (2002). Cultural Heritage Digital Libraries: 
Needs and Components. In ECDL 2002, LNCS 2458, 
pp. 626-637, 2001. Springer-Verlag, Berlin, 2002.
Foertsch, R. (2006). ARACHNE - Datenbank und kul-
turelle Archive des Forschungsarchivs fuer Antike Plas-
tik Koeln und des Deutschen Archaeologischen Instituts. 
http://arachne.uni-koeln.de/ (accessed 12 October 2008).
Jewell, A., Zilig, B., and Ramsay, S. (2008). Can Text 
Analysis Be Part of the Reading Field?: The Vision of 
Evince. CaSTA 2008, New Directions in Text Analysis. 
http://ocs.usask.ca/ocs/index.php/casta/casta08/paper/
view/25 (accessed 2 November 2008).
Monroy, C., Parks, N., Furuta, R., and Castro, F. 
(2006). The Nautical Archaeology Digital Library, 10th 
European Conference on Research and Advanced Tech-
nology for Digital Libraries ECDL, Alicante, Spain, 
September 2006. In Gonzalo et al. (Eds.) - LNCS 4172 
:544-547, Berlin and Heidelberg: Springer-Verlag, 2006.
Monroy, C., Furuta, R., and Castro F. (2007). A Mul-
tilingual Approach to Technical Manuscripts: 16th and 
17th-century Portuguese Shipbuilding Treatises. ACM-
IEEE Joint Conference on Digital Libraries, Vancouver, 
British Columbia, Canada, June 2007.
Rockwell, G. (2003), What is Text Analysis, Really? Lit-
erary and Linguistic Computing 18(2):209-219, 2003. 
Steffy, D. (1994). Wooden Ship Building and the Inter-
pretation of Shipwrecks. Texas A&M University Press, 
College Station, Texas (1994).
Diccionario de la Real Academia de la Lengua Espa-
ñola, http://www.rae.es/rae.html (accessed 28 October 
2008).
CASTA 2008 New Direction in Text Analysis, http://
ocs.usask.ca/ocs/index.php/casta/casta08/index/ (ac-
cessed 17 October 2008). 
The Apache Lucene Project, http://lucene.apaches.org/ 
(accessed 10 October 2008).
The LEO dictionary, http://dict.leo.og/ende?lang=en 
(accessed 30 October 2008).
The Willa Cather Archive, University of Nebraska Lin-
coln http://cather.unl.edu (accessed 1 November 2008)
DIGITAL HUMANITIES 2009
Page 348
Visualizing Archival Collections 
with ArchivesZ
Jeanne Kramer-Smyth
University of Maryland, College Park 
jeanne@spellboundblog.com
Jennifer Golbeck 
University of Maryland, College Park 
jgolbeck@umd.edu
Archival records and manuscripts are usually ar-ranged to retain their original order when trans-
ferred into the care of archivists or manuscript curators. 
A side effect of grouping records by record creator and 
retaining	the	creator’s	original	organization	is	 that	ma-
terials are described at the group level—not at the item 
level.		The	ramifications	for	archive	searchability	are	dra-
matic: Imagine a library where, instead of being grouped 
together by subject, books were shelved alphabetically 
by	 author—and	 interspersed	with	 each	 author’s	 notes,	
drafts, expense records, and personal memorabilia. This 
basic difference between libraries and archives is key to 
understanding why subject-based access to archival re-
sources is both challenging to achieve and very useful 
when available.
With this in mind, we have developed ArchivesZ, an in-
formation visualization tool for archival collections. It 
enables users to visualize and explore aggregated infor-
mation relating to the total linear feet, inclusive years 
and subject terms for archival collections extracted from 
EAD	encoded	finding	aids.
Chris Anderson of Wired described the power of the 
“long tail” in his Wired article of the same name. He dis-
cussed that the future belonged not to the bestsellers, but 
rather to “the millions of niche markets at the shallow 
end of the bit-stream.”1 There has been much discussion 
of the long tail with regard to library resources2, but it is 
interesting note that archival materials are virtually all 
long tail. The nature of archival collections is such that 
many of those with the greatest desire to access the ma-
terials	have	very	narrow	and	specific	interests.	It	is	quite	
rare that the documents in a single archival collection 
will be popular, in the sense of a bestselling book. Fre-
quently it is a challenge for humanities scholars wishing 
to	use	archival	materials	to	figure	out	how	to	approach	
the search process. Use of a visualization tool designed 
to support the examination of aggregated information 
about archival collections could support a more seren-
dipitous process of exploration of materials and the dis-
covery of new avenues of research.
Even the most experienced historian or humanities schol-
ar has struggled with the challenge of locating relevant 
primary sources. Archival record groups and manuscript 
collections present unique challenges to researchers. For 
example, a standard search result list shows only the title 
and short description for each record group or collec-
tion. This list fails to convey the quantity of materials or 
diversity of subjects covered by the combination of col-
lections returned by the search.  A visualization tool that 
supports examination of cross-collection and cross-insti-
tution aggregated data about archival collections could:
•	 Encourage the browsing and exploration of locally 
available cultural heritage resources,
•	 Improve understanding of existing collections,
•	 Permit	 easy	 identification	 of	 locations	with	 a	 rich	
combination of collections applicable to a particular 
research project, and
•	 Increase interest in both the humanities and primary 
materials.
Encoded Archival Description (EAD) is the international 
de	 facto	 standard	 for	encoding	archival	finding	aids	 in	
an XML format. Finding aids include information about 
who created the records, when they were created, why 
they were created, what topics the records relate to, and 
the size of the collection. The archival community has 
spent	much	of	the	past	decade	encoding	existing	finding	
aids using the EAD standard. Up to this point the major 
selling point of EAD has been as a tool for simplifying 
the	process	of	publishing	finding	aids	online.	While	work	
has been done to create tools to facilitate the encoding 
of	finding	aids,	the	next	step	is	to	take	advantage	of	the	
structured	data	now	available	 in	EAD	encoded	finding	
aids. This machine readable data can support the creation 
of innovative software programs intended to extract, or-
ganize, facilitate discovery of and aggregate information 
about archival resources.
Tools for visualizing archival collections support the 
needs of three distinct user groups.  
•	 Archivists and manuscript curators can use such a 
tool to improve their understanding and validate the 
metadata of the collections at various institutions in-
cluding their own.  
•	 Literary researchers, historians and humanities 
scholars can use this type of tool to permit easy 
DIGITAL HUMANITIES 2009
Page  349
identification	 of	 institutions	 with	 archival	 collec-
tions	fitting	the	criteria	of	their	research.		
•	 Finally, this type of tool can enable exploration of 
locally held cultural heritage materials by students 
and promote use of primary sources. In contrast to 
researchers	who	frequently	have	very	specific	inter-
ests before they examine the collections held by an 
institution, students in the university setting who are 
interested in humanities topics are likely not aware 
of the primary sources available. A tool of this type 
might encourage the browsing and open ended ex-
ploration of locally available cultural heritage re-
sources, and increase interest in both the humanities 
and primary materials.
Built	 in	 spring	 of	 2007,	 the	first	 version	 of	ArchivesZ	
is a prototype for just such a tool. Designed to support 
search, exploration and visualization of archival record 
groups and manuscript collections, ArchivesZ addresses 
a major challenge facing humanities scholars - the need 
to understand the scope and quantity of available archi-
val records and manuscripts. 
Fig. 1 Screenshot of ArchivesZ Prototype (video 
demonstration online at archivesz.org)
To support organic exploration of subject terms asso-
ciated with collections, ArchivesZ leverages a unique 
dual sided histogram (see right half of Figure 1). The 
ArchivesZ prototype combines this dual sided histogram 
with a more traditional histogram displaying year data to 
permit tightly coupled, multi-dimensional browsing of 
subject and time period metadata. By representing the 
distribution of subjects and time periods using the metric 
of total aggregate linear feet, ArchivesZ permits users 
to get a better sense of total available research materials 
than they would by viewing a standard search result list. 
The subject term visualization interface may also sup-
port a deeper understanding of the relationships among 
subject terms through the lens of the currently selected 
set of collections.
Further development of ArchivesZ has been supported 
by a National Endowment for the Humanities Digital 
Humanities Startup Grant. In this Poster / Demo, we will 
present the newest version of ArchivesZ in use over a 
large	set	of	finding	aids	provided	by	a	wide	range	of	part-
ner archives. Our demo will show the newest version of 
he tool and we will discuss how this lays the foundation 
for the future creation of a public tool for visualizing ar-
chival collections.
Notes
1C. Anderson. The long tail. Wired, 12(10), 2004.
2 L. Dempsey. Libraries and the long tail: Some thoughts 
about libraries in a network age. D-Lib Magazine, 12(4), 
April 2006.
DIGITAL HUMANITIES 2009
Page 350
Active Animation: An Approach 
to Interactive and Generative 
Animation for User-Interface  
Design and Expression 
Kenny K. N. Chow
Georgia Institute of Technology
knchow@gatech.edu
D. Fox Harrell, Ph.D. 
Georgia Institute of Technology
fox.harrell@lcc.gatech.edu	
Abstract 
The traditional view of animation is a medium-spe-cific	perspective:	animation	is	a	sequence	of	images	
on	film.	In	contrast,	we	employ	a	wider,	interdisciplin-
ary theoretical lens, based on a phenomenological per-
spective of animation. We describe animation as the ex-
perience of artifacts imbued with apparent “animacy,” 
or “liveliness,” and identify a range of media artifacts 
where an account of animacy is key to understanding 
and designing their functionality. These artifacts range 
from computer interface mechanisms such as bouncing 
and stretching icons to interactive cartoons that may be 
used for informational, entertainment, or socio-critical 
purposes. We introduce the term “active animation” to 
describe this range of artifacts. Insights from textual 
analyses	in	the	humanities-based	field	of	animation	stud-
ies can enable analysis and design of active animation, 
and likewise animation studies can be informed by in-
sights	regarding	agency	in	artificial	intelligence	research,	
theories of embodied cognition and conceptual blending 
from cognitive science, and psychological approaches to 
movement and perception. To exemplify the technical 
design potential of our approach, we present a cognitive 
semantics-based interactive and generative multimedia 
system that we have implemented called the Generative 
Visual Renku system as a case study active animation. 
The upshot is that our interdisciplinary animacy-oriented 
perspective highlights how gesture and movement allow 
interactive and generative digital artifacts to convey 
non-verbal meaning to users. 
1. Introduction 
Animacy lies at the heart of many media artifacts imbued 
with an illusion of life. Puppets and avatars, examples 
of traditional and digital user manipulated characters, 
become lively under the control and enactment of per-
formers. Animatronic robots utilize mechanical means 
to produce the appearance of gesturing and perceiving 
viewers. Cartoons, manifested through sequences of pic-
tures,	can	walk	like	real	human	figures.	Although	these	
artifacts differ from each other in terms of material form, 
control mechanism, and technology, all of them are ani-
mated in a literal sense. The animation of these artifacts 
hinges upon lively motion as the primary phenomenon 
of illusion of life. 
Meanwhile, the term animation is often narrowly seen 
as referring to a particular medium, namely a type of 
film.	Indeed,	the	celebrated	filmmaker	Norman	McLaren	
describes animation as the “art of movements that are 
drawn.” (Wells, 1998) Although his quote seemingly 
privileges motion over medium, the material condition 
of imagery as drawings is still presumed. In contrast, 
we call attention to views that deemphasize medium 
and emphasize liveliness. The animation theorist Alan 
Cholodenko attempts to generalize the notion of anima-
tion as sorts of technology geared toward “endowing 
with life” and “endowing with motion.” (Cholodenko, 
2007) In parallel, many digitally mediated environments 
such as computer interfaces, websites, and handheld de-
vices have become lively, reactive, semi-autonomous, 
and graphical. They often construct meaning through 
perceived movement and embodied interaction. We call 
digital images engaged in such meaning-making pro-
cesses “active animation.” Given the ubiquity of such 
multimedia computing phenomena that are often over-
looked as animation, there is need for theory to compre-
hend how such artifacts convey non-linguistic meaning 
via animacy and to formulate theoretically-grounded ap-
proaches for designing lively multimedia artifacts. This 
paper articulates this need and presents a new approach 
to addressing it, including a new form of active anima-
tion that we have developed. 
2. Theoretical Framework 
Our approach to the analysis and design of active anima-
tion arises from an intersection of multiple disciplines. 
Animation and image studies provide us a critical vo-
cabulary for identifying the phenomenon of liveliness 
as	definitional	for	our	area	of	inquiry.	(Arnheim,	1974;	
Metz, 1974; Mitchell, 1986) For thinkers such as Ludwig 
Wittgenstein and W.J.T. Mitchell, the term “image” is 
not limited to material images (e.g. screen images) or op-
tical images, but also means perceptual images (through 
motor-sensory functions, including the kinaesthetic) or 
even mental images. It follows that the idea of anima-
tion should also extend to considering moving images 
on the basis of sensory perception and embodied cog-
nition. Cognitive semantics research provides accounts 
of embodied meaning construction and generation of 
imaginative meaning through metaphorical projection 
DIGITAL HUMANITIES 2009
Page  351
and conceptual blending. (Fauconnier & Turner, 2002; 
Lakoff & Johnson, 2003; Turner, 1996) Psychological 
and phenomenological approaches to human-computer 
interaction are also relevant departure points for investi-
gating the role of interactivity in the perception of liveli-
ness. (Norman, 1988; Shneiderman, 2003) 
3. Active Animation: Examples and 
Analyses 
Toward analyzing and designing instances of active 
animation	we	 introduce	 two	 levels	of	signification:	 the	
reactive and the metaphorical. At the reactive level, us-
ers make meaning out of liveliness of artifacts through a 
motor-sensory loop feedback between users and systems 
– i.e. users perform actions via an interface and perceive 
their animated effects in the system. Examples include 
user-interface mechanisms such as the many shrinking, 
stretching, and bouncing icons in the Macintosh OS X 
environment and interactive animated comics such as 
found at www.hoogerbrugge.com. 
Fig. 1 The “genie” effect in Macintosh OS where windows 
dynamically stretch and shrink 
Such works imbue media elements such as windows and 
icons with a sense of liveliness formerly unknown in 
user interfaces. Motion is used to focalize user attention, 
add spectacle to basic operation, and to allow embodied 
user action such as clicking to play a role in realizing the 
meaning of animated content. (Arnheim, 1974; Lakoff & 
Johnson, 1999) Basic image schemata (skeletal patterns 
of motor-sensory perception) play crucial roles in user 
understanding of such works. (Lakoff & Johnson, 2003) 
For example, the “dock” area of the Macintosh graphical 
user interface becomes a container for windows, paral-
leling the container image schema articulated in (Lakoff 
& Johnson, 2003). 
At the metaphorical level, users construct imaginative 
motion metaphors through the interaction between em-
bodied gestures and multimedia feedback. The idea can 
be demonstrated by the water-level interface designed 
for the mobile phone N702iS and the electronic advertis-
ing viral campaign www.comeclean.com. 
Fig. 3 A mobile phone interface where battery level is 
indicated via the illusion of a water-filled container 
The water-level interface in Fig. 3 comprises a concep-
tual	metaphor	 in	which	a	container	filled	with	water	 is	
integrated with a standard interface element depicting 
battery-level. This metaphor exploits the liveliness of 
animated water to present functional information in a 
lively and playful manner. The website Comeclean.com 
invokes standard interface mechanisms such as data-
entry and mouse-clicking to arrive at a metaphorical 
projection in which users can wash away the wrongdo-
ings. The site is, in fact, an advertisement for cleaning 
supplies, yet the metaphorical mapping from washing 
one’s	hands	using	particular	cleaning	supplies	to	wash-
ing away confessions of sin is enabled by the active ani-
mation interface. 
4. A New Form of Active Animation: 
Generative Visual Renku 
As an example of multimedia system design based on 
our	approach	to	active	animation	briefly	we	present	an	
expressive project that we have developed called Gen-
Fig. 2 Two screenshots from 
Hoogerbrugge.com where 
interaction drives provocative 
animated reaction
DIGITAL HUMANITIES 2009
Page 352
erative Visual Renku. (Harrell & Chow, 2008) A poly-
morphic poem is a generative digital artwork that is 
constructed differently upon each instantiation, but can 
be meaningfully constrained according to aspects such 
as theme, metaphor, affect, and discourse structure. Our 
Generative Visual Renku project presents a work of ac-
tive animation as a new form of concrete polymorphic 
poetry inspired by Japanese renku poetry, iconicity of 
Chinese character forms, and generative models from 
contemporary art. 
In the Generative Visual Renku project interactive iconic 
illustrations are conjoined by a cognitive science based 
computer program called GRIOT into a fanciful topog-
raphy. GRIOT, which is a system for composing genera-
tive and interactive narrative and poetic works, is used to 
semantically constrain generated animated output both 
visually and conceptually.
Fig. 5 A Generative Visual Renku screenshot: Users co-create 
animated maps by clicking visual icons, the system responsive-
ly selects subsequent images according to semantic constraints 
Conclusions and Implications 
Today many multimedia computing systems show spec-
tacular animated images that react to user actions with 
animated feedback. These artifacts manifest the notion 
of animation in a new horizon beyond the cinematic. The 
examples of active animation above illustrate this mani-
festation in both functional interface design such as the 
lively windows of Mac OS X and mobile phone water-
level interfaces and in expressive works such as found 
at Hoogerbrugge.com or in our own generative visual 
renku. These works all evoke senses of liveliness, not 
only with perceptual movements, but also through gen-
erative multimodal feedback loops. They bring life to the 
computer, it can now feel more intimate to users through 
perceived emotion and even intelligence. 
Active animation “enlivens” the computer by conceal-
ing its complexity with a “skin” like the shells of ani-
matronic robots. Careful understanding of how users 
interpret active animation allows designers to “stage” 
and “veil” technology in order to create spectacles, sus-
pense, surprise, and intuitive non-verbal meanings for 
users. This approach also brings concern for humanistic 
interpretation back to the center of analysis and design 
of multimedia artifacts. The integration of computational 
and cognitive research results with approaches from ani-
mation studies provides a new orientation for designing 
technologies that are more in line with our everyday, 
non-verbal, affective acts of communication and under-
standing. 
References 
Arnheim, R. (1974). Art and visual perception : a psy-
chology of the creative eye. Berkeley: University of Cali-
fornia Press. 
Cholodenko, A. (2007). Speculations on the Animatic 
Automaton. In A. Cholodenko (Ed.), The illusion of life 
II : more essays on animation (pp. 486-528). Sydney, 
N.S.W.: Power Pub. 
Fauconnier, G., & Turner, M. (2002). The way we think 
: conceptual blending and the mind’s hidden complexi-
ties. New York: Basic Books. 
Harrell, D. F., & Chow, K. K. N. (2008). Generative 
Visual Renku: Linked Poetry Generation with the GRI-
OT System, Visionary Landscapes: Electronic Litera-
ture Organization 2008 Conference. Washington State 
Fig. 4 Screenshots of an active animation web design at comeclean.com where mouse actions 
cause images of hands to wash away text input by users
DIGITAL HUMANITIES 2009
Page  353
University Vancouver, Vancouver, Washington, USA. 
Lakoff, G., & Johnson, M. (1999). Philosophy in the 
flesh : the embodied mind and its challenge to Western 
thought. New York: Basic Books. 
Lakoff, G., & Johnson, M. (2003). Metaphors we live 
by. Chicago: University of Chicago Press. 
Metz, C. (1974). Film language : a semiotics of the cin-
ema. New York: Oxford University Press. 
Mitchell, W. J. T. (1986). Iconology : image, text, ideol-
ogy. Chicago: University of Chicago Press. 
Norman, D. A. (1988). The Psychology of Everyday 
Things. New York: Basic Books Inc. 
Shneiderman, B. (2003 [1983]). Direct Manipulation: A 
Step beyond Programming Languages. In 
N. Wardrip-Fruin & N. Montfort (Eds.), The new media 
reader (pp. 486-498). Cambridge, 
Mass. ; London: MIT Press. Turner, M. (1996). The lit-
erary mind. New York: Oxford University Press. Wells, 
P. (1998). Understanding animation. London ; New 
York: Routledge. 
I Am a Black Scholar:  
A Digital Repository of 
Scholarship from within the 
Black Diaspora 
Leshell Hatley 
University of Maryland, College Park  
leshell@umd.edu	
Introducing The Black Scholars Index™ 
‘I	am	a	Black	Scholar’	is	the	slogan	for	the	digital	human-
ities project named The Black Scholars Index™ (BSI) 
sponsored by Uplift, Inc. Uplift, Inc. is a 501c3 (tax ex-
empt)	nonprofit	organization,	founded	by	Leshell	Hatley,	
a doctoral student in the College of Information Studies 
at the University of Maryland. Its mission is to support 
underrepresented groups and to research and develop 
technology-based programs, products, and services to 
encourage and produce lifelong learners, leaders, and re-
silient communities. With this in mind, the organization 
and its founder constructed The Black Scholars Index™ 
(BSI) as an online venue to highlight, record, analyze, 
and illustrate the scholarly achievements and intellectual 
movements within the Black Diaspora, descendents of 
Africans who have settled throughout the world as a re-
sult of the Atlantic slave trade. In keeping with current 
uses of Internet technology, BSI will host features such 
as: an extensive directory of Black Scholars;  Talented 
Tenth,	BSI’s	online	Journal	for	Black	Scholars;	the	BSI	
Digital Repository, a database of dissertations, research 
papers, and other work produced by Black scholars; 
online collaboration tools; a network for mentoring po-
tential students of higher education; pod/webcasts, and 
a host of Web 2.0 data visualization elements. These 
tools and features will provide analytical measurements, 
share insightful perspectives, and enhance the historical 
account of the knowledge production and of the many 
societal contributions by Black Scholars.
Investigating The Black Scholars Index™
While many researchers have previously attempted to 
gather information about the perspectives, experiences, 
motivations, and challenges of Black students in higher 
education, few, if any, have done so on large scales, from 
around the world, nor collected works and experiences 
of these scholars after they have been working in their 
fields.	 In	 the	 form	 of	 books	 and	 articles,	 some	 Black	
students have given personal accounts of their stories at 
Predominately	White	Institutions	(PWIs)	or	within	fields	
of study where Black students are underrepresented 
as attempts to share their experiences and offer advice 
DIGITAL HUMANITIES 2009
Page 354
(Green 2008; Booker 2007; Hall-Green, 2000; Scott 
1995). Over the years, the desire to provide analytical 
commentary, share knowledge, and provide advice as a 
result of lived experiences has been prominent themes 
within the Black intellectual community, especially 
within the United States. Harold Cruz (1967) provided 
a historical analysis of what he deemed the challenges 
of black leadership in his book, The Crisis of the Negro 
Intellectual. In it, he chronicled the major decisions and 
actions of prominent Black literary, artistic, and political 
leaders. This book provided an astounding glimpse into 
Black life similarly to the renowned books of W. E. B. 
DuBois, The Souls of Black Folk (1903) and Carter G. 
Woodson, The Mis-education of the Negro (1933). Nikki 
Giovanni, a prominent Black literary scholar, dedicated 
two chapters in her book, Racism 101 (1994), for advice 
to African-American students enrolled in college (or 
soon to be); and in 2003, Anna Green and LeKita Scott 
published Journey to the PhD: How to Navigate the Pro-
cess as African Americans, a collection of seventeen es-
say describing the challenges of twenty doctoral student 
from institution around the United States.
These books are often recommended to members of the 
Black community and beyond, especially those contem-
plating higher education, as representations of past ac-
complishments and challenges from which to gain in-
valuable lessons. While they are valuable resources, they 
are not as widely accessible as resources made available 
on the internet and they do not contain information over 
extended periods of time. The Black Scholars Index™ 
provides this and more. Not only will this historical web-
site serve as a repository of research and analytical ac-
counts of knowledge production, it will be an ongoing 
spotlight of the lived experiences of Black Scholars and 
offer invaluable advice throughout the life of the proj-
ect. With it, future scholars of all races will have access 
to resourceful information, authentic experiences and 
perspectives, visual representations of research trends, 
and various other tools and features that will help under-
stand the educational and intellectual activities of Black 
Scholars. 
However, gains from The Black Scholar Index™ ex-
istence	 stretch	 beyond	 these	 benefits.	 Aside	 from	 the	
project’s	 goals	 and	 the	 above	 component	 descriptions,	
the informative advantages and implications of BSI are 
tremendous and research has been underway since its in-
ception in February 2008. Two months after launching 
BSI with only word of mouth marketing, approximately 
200 Black Scholars were indexed and initial demograph-
ic statistics were measured. Measurements from this 
snapshot are displayed below:
•	 Female -66% 
•	 Male -34%. 
•	 In a relationship -22% 
•	 Married -27% 
•	 Single -48% 
•	 Parents – 33% 
•	 Have published papers about their work or similar 
interests -43% 
•	 Members of fraternities and sororities -21% 
•	 Attended an HBCU for undergraduate degree -49% 
•	 Top BS Degree obtained = BS Biology 
•	 Attended an HBCU for Masters degree -19% 
•	 Top Masters Degree obtained = MS Public Health 
•	 Attended an HBCU for Doctoral degree -3% 
•	 Top PhD Degree obtained = PhD in Computer Sci-
ence 
•	 Current faculty (tenured and non-tenured) -16% 
•	 Currently doctoral students – 37% 
•	 More	than	1/2	work	in	a	field	related	to	their	PhD	
•	 77% expressed interest in serving as BSI mentors 
The results of this preliminary snapshot were extremely 
revealing: the majority of Black scholars indexed stud-
ied health and science, a small number were employed 
in faculty positions, a large majority were interested in 
mentoring students interested in pursuing advanced de-
grees, and although approximately half attended Histori-
cally Black Colleges and/or Universities (HBCUs) while 
in undergraduate school, attendance at HBCUs dimin-
ishes	significantly	as	higher	degrees	were	sought.	Most	
of the Black scholars indexed to date reside within the 
United States and the BSI website displays a geospa-
tial map of their employment locations. Although these 
measurements	do	not	reflect	the	total	number	of	African-
Americans with doctoral degrees, it provides a glimpse 
of demographic information and illustrates the poten-
tial for the amount of information that can be garnered 
overtime. This information along with more qualitative 
analysis of the experiences, perspectives, and motiva-
tions of those indexed via future website polls, surveys, 
and	other	forms	of	interaction	will	be	beneficial	to	future	
educational programs, mentoring organizations, and re-
cruitment efforts by a variety of organizations and  in-
DIGITAL HUMANITIES 2009
Page  355
stitutions. 
Conclusion and Future Work 
Development of The Black Scholars Index™ is ongoing. 
Design revisions, expansion of content, and the design 
and implementation of the many tools including those 
described above are all underway. One of the intended 
analysis tools examines the collaboration and co-author-
ship of research projects and papers. Another tool pro-
vides an interface for data entry of well-known African 
American scholars who have lived over the past century. 
Further analysis of the all the data collected will help 
fulfill	 the	mission	of	 the	project	and	provide	a	concise	
representation of the scholarly accomplishments of the 
Black Diaspora. 
References 
Booker, K. C. (2007). Perception of classroom belong-
ingness among African American college students. Col-
lege Student Journal. 3 (41), 178-186 
Du Bois, W. E. B (1903). The Souls of Black Folk, Re-
published (1995) 100
th 
Anniversary Edition, Penguin 
Group 
Giovanni, N. (1994). Racism 101. Quill, NY 
Green, A. (2008). A Dream Deferred: The experience of 
an African American student in a doctoral program in 
science. Education Spring 2008, Vol. 128 Issue 3, p339-
348 
Green, A., Scott, L. (2003). Journey to the PhD: How to 
Navigate the Process as African Americans, Stylus, VA 
Hall-Greene, D. (2000). A Qualitative Study on African 
American and Caribbean Black Males’ Experience in a 
College of Aeronautical Science. Unpublished doctoral 
dissertation, Virginia Tech. 
Scott, D. W. (1995). Conditions related to the Academic 
Performance of African American Students at Virginia 
Polytechnic Institute and State University. Unpublished 
doctoral dissertation, Virginia Tech. 
Woodon, C. G. (1993) The Mis-education of the Negro, 
Republished (1992), UBUS Communications Systems 
Digital Editions for Corpus 
Linguistics: Encoding 
Abbreviations in TEI XML 
Mark-up
Alpo Honkapohja  
University of Helsinki, Finland. 
alpo.honkapohja@helsinki.fi
This poster will present the Digital Editions for Cor-pus Linguistics (DECL) system for encoding manu-
script abbreviations in TEI-conformant XML. First, I 
will	briefly	describe	the	DECL	project,	its	presenting	its	
aims and editorial policies. Secondly, I will go through 
the problems resulting from the silent expansion of ab-
breviations, an approach some digital editions derive 
from	traditional	editing.	And	finally,	I	will	describe	the	
possibilities of TEI P5 for encoding them, as well as the 
DECL	application	of	 the	guidelines,	 and	what	benefits	
they have for the type of historically oriented, corpus 
searchable editions we are compiling. The examples 
will come from a digital edition of Trinity College Cam-
bridge MS O.1.77, a pocket-sized late medieval medical 
handbook in Middle English and Latin, which I am edit-
ing for my PhD thesis. 
Digital Editions for Corpus Linguistics
DECL is a project, based at the Research Unit for Varia-
tion, Contacts and Change (VARIENG) at the University 
of Helsinki, which aims at developing online editions 
that combine the accurate description of historical docu-
ments	with	the	flexibility	of	search	tools	developed	for	
linguistic computing. It was formed by three postgradu-
ate students at VARIENG in 2007, who shared a dis-
satisfaction with extant tools and resources, and aimed 
to develop a more versatile and user-friendly model for 
digitised manuscripts of historical texts. The tools and 
framework are designed to meet the needs of small scale 
research projects and individual scholars. They are based 
on and compatible with version P5 of the TEI guidelines. 
On the level of editorial principles, DECL editions adopt 
the opinion of Lass (2004) that digital editions should 
preserve the text as faithfully as possible, convey it in as 
flexible	 form	as	possible,	and	ensure	 that	any	editorial	
intervention remains visible and reversible, formulating 
it into three central principles of Transparency, Flexibil-
ity and Expandability. DECL editions aim to offer the 
user diplomatic transcriptions of the manuscripts into 
which linguistic, palaeographic and codicological fea-
tures will be encoded. Additional layers of contextual, 
DIGITAL HUMANITIES 2009
Page 356
codicological and linguistic annotation can be added to 
the editions using standoff XML tagging. 
Background 
One of the most ubiquitous problems encountered in ed-
iting medieval manuscripts, is how to represent the nu-
merous abbreviations in them. There is no established 
standard for encoding these abbreviations in digital for-
mat, and many digital editions still follow the practice 
inherited from traditional book editions of expanding 
them, either silently or in italics. From the point of view 
of historical linguistics this is somewhat problematic, es-
pecially in the light of some recent discussion over what 
is required of an edition or corpus in order to constitute 
reliable data (cf. i.a. Bailey 2004; Curzan and Palmer 
2006; Dollinger 2004; Grund 2006). Most vocal in his 
criticism of existing practices has been Lass (2004), 
who demands that in order to serve as valid data for the 
historiography of language, a digital edition or a corpus 
should not contain any editorial intervention that results 
in substituting the scribal text with a modern equivalent.
Expanding abbreviations substitutes a symbol used by 
the scribe with a modern reading of it, which may, in 
the vast majority of cases, be obvious, and supported 
by	research,	but,	by	definition,	also	contains	an	element	
of editorial interpretation. In some cases this may have 
an impact on the data. For example, the irregularity of 
spelling of Middle English may result the editor to make 
decisions over which combination of letters a particular 
abbreviation stands for in text in which the abbreviated 
word may appear in several spelling variants. 
TEI
The TEI P5 module for encoding glyphs and non-stan-
dard characters offers a few alternative ways of annotat-
ing them. The abbreviations may be annotated by <g> 
tag, indicating that they are glyphs, in which case they 
are	defined	by	the	gaiji	module	in	the	TEI	header.	Or	the	
<am>  and <ex>  tags can be used to indicate the abbre-
viated sign and its editorial expansion. In these cases, the 
<choice> element may also be used to indicate that 
some of the elements are alternatives to each other. Or 
the whole word may simply be annotated as <abbr> to 
show that it contains abbreviations. The editor may also 
use the <expan> tag, indicating items which have been 
expanded without recording the abbreviation symbol.
The DECL guidelines uses an application of this that 
marks both the symbol, and its content, but does not re-
quire multiple elements inside a single word, as they can 
cause	internal	difficulties	with	stand-off	tagging.	We	use	
the <abbr> element to mark that a word contains an 
abbreviation, the <g> element to tag the content of each 
abbreviation, and give the abbreviation symbol used for 
it as its attribute.
<abbr>su<g ref=”#crossed-p”>per
</g></abbr>
The expanded part of the abbreviation, which is in fact 
editors reconstruction and in some cases may up for de-
bate gets enclosed in the <g> tags, and thus also marked 
as editorial - which is in accordance with  the DECL 
principle of transparency. 
Aims	and	Benefits	
The XML code can be dynamically processed via XSLT 
transformation scripts to create documents which dis-
play either the abbreviations or expanded words accord-
ing to the needs of the user, and DECL editions will also 
offer the user a customisable online interface, capable of 
displaying both. In addition to visual presentation and 
browsing, the interface will also offer corpus search and 
analysis functions, which can be extended to searches 
on	 the	 specified	 elements	 or	 attributes.	 	 Following	 the	
principles of open source and open access, they users of 
DECL editions will have full access to the code and may 
download and alter it, meaning that it is possible to al-
ter	the	editorial	decisions	if	the	user	is	not	satisfied	with	
them.
In the poster I will give an illustrated presentation of how 
the process of encoding abbreviations progresses from 
manuscript images, via TEI XML code to its various 
forms of presentation.
References
Bailey, Richard W. (2004). The need for good texts: 
The	case	of	Henry	Machyn’s	
Day Book, 1550–1563, Studies in the history of the Eng-
lish language II: Unfolding conversations (Topics in 
English Linguistics 45): 217–228.
Curzan, Anne and Palmer, Chris C.  (2006). The im-
portance of historical corpora, reliability, and reading, 
Corpus-based Studies of Diachronic English: 17–34.
DECL (Digital Editions for Corpus Linguistics). 
<http://www.helsinki.fi/varieng/domains/DECL.html>.
Dollinger, Stefan.	(2004).	‘Philological	computing’	vs.	
‘philological	outsourcing’	and	the	compilation	of	histori-
cal corpora: A Late Modern English test case, Vienna 
DIGITAL HUMANITIES 2009
Page  357
English Working Papers (VIEWS), 13(2): 3–23.
Grund, Peter. (2006). Manuscripts as sources for lin-
guistic research: A methodological case study based on 
the Mirror of Lights, Journal of English Linguistics, 34: 
105–125.
Lass, Roger. (2004). ‘Ut custodiant litteras: Editions, 
Corpora	 and	 Witnesshood’,	 in:	 Marina	 Dossena	 and	
Roger Lass (eds.) Methods and Data in English Histori-
cal Dialectology (Linguistic Insights 16): 21–48.
TEI (Text Encoding Initiative). <http:/www.tei-c.org>.
VARIENG (Research Unit for Variation, Contacts 
and Change in English).	 <http://www.helsinki.fi/
varieng/>
JGAAP 4.0 — A Revised 
Authorship Attribution Tool
Patrick Juola 
Duquesne University
juola@mathcs.duq.edu
John Noecker, Jr. 
Duquesne University 
jnoecker@gmail.com
Mike Ryan 
Duquesne University 
michaelryan@acm.org
Sandy Speer 
Duquesne University 
speers@duq.edu	
Authorship	Attribution	(Juola,	2006)	can	be	defined	as the inference of the author or her characteristics 
by examining documents produced by that person.  For 
some time, we have been working on a system (JGAAP 
— Java Graphical Authorship Attribution Program) to 
use advanced statistics to perform this task while not de-
manding a high degree of expertise from the user (Juola, 
et al., 2008).  With the recent release of JGAAP 3.2 and 
the	near-term	planned	release	of	JGAAP	4.0,	we	are	fi-
nally	confident	that	we	have	a	production	quality	system	
for general-purpose use.
We now report (and demonstrate) these recent improve-
ments. JGAAP now incorporates nearly 20 different ana-
lytic methods (including eight different distance-based 
nearest-neighbor algorithms), more than 20 different 
event sets and models ranging from character- and word-
based N-grams to reaction times, and several different 
preprocessors incorporating a wide variety of different 
document	types	including	remote	(Web-accessible)	files	
and text extraction from different formats.  We estimate 
that JGAAP is capable of performing more than 20,000 
different types of analysis for authorship attribution or 
similar	text	classification	tasks,	with	more	being	added	
as development continues. 
Other improvements include:
•	 GUI improvements to enhance user-friendliness
•	 Enhanced graphical output capabilities
DIGITAL HUMANITIES 2009
Page 358
•	 Full report generation capacity for scholarly inspec-
tion of the results
•	 Creation of a command-line interface 
•	 Automatic batch processing capacity for large-scale 
comparative testing
•	 Incorporation of the AAAC (Juola, 2004) test cor-
pus into the demo for comparative testing purposes
•	 Dynamic loading of new methods to encourage new 
development
We	are	finally	able	 to	perform	 large-scale	comparative	
analyses of different processing methods.  We include 
here	a	short	list	of	some	JGAAP-related	findings	(pub-
lished, submitted, or in preparation) :
•	 Introduction of a small number of character errors 
(as	exemplified	by	modern	OCR	systems)	does	not	
substantially reduce accuracy with most methods.
•	 Symmetric (“commutative”) distance-based meth-
ods tend to outperform asymmetric ones.
•	 Linear	classifiers	such	as	LDA	tend	 to	outperform	
nonlinear	 classifiers	 despite	 the	 apparent	 oversim-
plicity of the underlying model
•	 Character-based methods tend to outperform word-
based ones for authorship attribution in Chinese
•	 Both cosine distance (normalized dot product) and 
simple event-based Kullback-Leibler divergence 
tend to be the best-performing methods for distance-
based nearest-neighbor methods.
•	 The seminal word list of Mosteller and Wallace does 
not generally perform well for texts other than the 
Federalist Papers
Some	of	our	findings	have	been	submitted	under	separate	
cover to this conference, but we hope to present a sum-
mary of major results that have been achieved by June 
2009 along with a demonstration of the newest version 
of the program.  We also hope to provide examples of 
the sort of analysis that have been performed by JGAAP 
(and invite cooperation from interested researchers for 
further study).
Finally, we hope to demonstrate some example ad-hoc 
analyses during the session; it should be possible, for ex-
ample, to demonstrate that “document length” or “words 
that are palindromes” do not perform well as Event/fea-
ture sets in less than ten minutes.  While this is perhaps 
not interesting (no sensible person has proposed palin-
dromes for authorship attribution), this clearly illustrates 
the ease-of-use and of result generation.
DIGITAL HUMANITIES 2009
Page  359
The Prioress and the Jew: 
Mining the Symbolic System 
through Lexical Genre Analysis 
of Modernizations
Nathan Kelber 
University of Maryland, College Park 
nkelber@umd.edu
My research uses quantitative analysis and reception theory in order to understand how early 19th cen-
tury readers viewed and received the work of Chaucer. 
Using the HyperPo tool from the Text Analysis Portal 
(TAPoR), my analysis suggests that modernizations of 
“The	Prioress’s	Tale”	were	more	critically	successful	in	
the 19th century when they adulterated the original form 
of the tale beyond the threshold of the distinctiveness 
ratio (Hoover, 2008).  I attribute this genre shift to a 
change in the symbolic systems between Christians and 
Jews which denigrated the legenda form in the 19th cen-
tury.  The historic adulteration of the legenda form along 
with	 the	 disappearance	 of	 its	 ‘loci	 in	 life’	 illuminates	
both how and why current analysis has focused heavily 
upon the anti-Semitism of the tale while also coming to 
an impasse of critical scholarship.
Criticism of the anti-Semitic nature of the tale has tended 
to diverge and stagnate into what critic Lawrence Besser-
man	calls	‘hard’	and	‘soft’	readings.		Hard	readings	view	
both the tale and Chaucer as anti-Semitic, as was typi-
cal of 14th century England.  Soft readings focus upon 
Chaucer’s	satire	of	the	prioress,	as	well	as	adducing	am-
bivalent historical evidence of the relationship between 
Jews and Christians, in an attempt to redeem Chaucer, 
the	‘Father	of	English.’		My	research	shifts	this	dualism	
by suggesting that contemporaries of Chaucer saw the 
construction of the Jew, or virtual Jew as posited in the 
work of Sylvia Tomasch, as a purely symbolic construct 
inherent within the form and social function of the leg-
enda.  My analysis of the degradation of the legenda is 
framed by the reception theory of Hans Robert Jauss.
Jauss’s	theory,	elucidated	in	Toward an Aesthetic of Re-
ception, diachronically links literary studies and literary 
history by tracing the historical shift of genres through 
changes	to	readers’	‘horizon	of	expectations.’		According	
to Jauss, texts propagate through their ability to conform 
to and disturb the expectations of their readers.  When 
texts are well-received, they exhibit congruency with the 
history and sociology of their audience.  The reception of 
deviations from the horizon of expectations signals the 
historical	and	social	approval	or	denigration	of	a	text’s	
ability to represent the norms and realities of its read-
ers.  The structure of genre then is more than mere liter-
ary fashion.  The diachronic change of genre reception 
reflects	 instabilities	 in	 the	specific	social	and	historical	
norms and realities of readers at the time.  Stable genres 
reflect	social	and	historical	systems	that	are	well-estab-
lished.	 	 Changing	 genres	 reflect	 periods	 of	 social	 and	
historical change.
Within the 14th century context of ‘The Canterbury 
Tales,’	the	legenda	of	‘The	Prioress’s	Tale’	is	a	‘culinary	
art’	 (Unterhaltungskunst)	 because	 it	 demands	 no	 hori-
zontal change of expectations (Jauss, 1982).  The leg-
enda represents an older form of art designed for what 
Margaret	Hallisy	calls	 ‘simple	believers.’	 	The	charac-
ters function as simple allegories of weakness, holiness, 
and evil.  The social function of the legenda is to show 
how the holy but weak may paradoxically triumph over a 
powerful evil with the help of God.  The strength of this 
simple allegorical form in subsequent modernizations 
of the tale can be tracked through text analysis software 
such as HyperPo.
The form of the legenda is reliant upon clear designations 
of good, evil, and weakness.  The saint must always be 
portrayed as both good and weak.  The villain is purely 
evil with no sense of redemption.  I have used HyperPo 
to	examine	a	lexical	field	centered	about	these	terms	in	
Robert	Lipscomb’s	1795	edition,	William	Wordsworth’s	
1820	 edition,	 and	 Robert	 Anderson’s	 Middle	 English	
version in The Works of the British Poets.  I have chosen 
Anderson because he is the likely source for both writ-
ers.
My	data	from	HyperPo	suggests	that	Wordsworth’s	text	
is a fairly good modernization.  He retains the rhyme 
royal form, uses similar syntax, and foregoes translation 
where he feels the modern word does not capture the es-
sence	of	the	original	Middle	English.		The	lexical	field	
of the legenda is a strong match with Anderon (92% raw 
match), but shows some weakening.  Concerning recep-
tion,	reviews	of	Wordsworth’s	text	in	1820	either	panned	
or ignored the modernization.  This seems to suggest a 
disconnect between the function of the legenda form and 
Wordsworth’s	19th century audience.
Lipscomb’s	text	is	not	a	close	modernization.		Analysis	
shows	that	the	lexical	field	of	the	legenda	is	debased	to	
the point that the text only weakly resembles the form 
(62% raw match).  This match is below the Distinctive-
ness Ratio of .67 which David L. Hoover advocates as 
warranting attention (2008).  It might be said that Lip-
scomb’s	text	has	ceased	to	be	a	legenda	at	all.	 	Recep-
DIGITAL HUMANITIES 2009
Page 360
tion	of	Lipscomb’s	text	was	very	positive	which	shows	
that his changes create a text that is more congruent with 
the aesthetics of contemporary readers.  In comparison 
to	Wordsworth	 and	Anderson,	 Lipscomb’s	 text	 was	 a	
watered-down Chaucer which Lipscomb “pruned of in-
delicacies.”
HyperPo, as a text analysis tool, is an effective way of 
creating and organizing lexical data.  The addition of raw 
counts, relative weights, and Z-scores gives the critic a 
variety of measurements for their data.  Working between 
Middle English and Modern English creates problems. 
Anderson’s	nonstandard	spelling	causes	inaccurate	word	
counts where some words (moder/modre/mother) are 
counted separately and other words are counted together 
(“sone” meaning both “son” and “soon”).  Critics must 
be	 careful	 if	 filtering	 common	words	 that	 they	 do	 not	
remove words which might affect their data.
My	research	suggests	that	Wordsworth’s	modernization	
was a fairly accurate translation of Chaucer, but this is 
also the reason it was a failure.  Contemporary readers 
saw Chaucer and Middle English as barbaric and crude. 
Lipscomb’s	version,	on	the	other	hand,	softened	the	lan-
guage and delivered a less vitriolic modernization.  Lip-
scomb	worked	within	the	framework	of	Dryden’s	Fables 
Ancient and Modern, published in 1700, which drasti-
cally changed lines, added material, changed rhyme 
schemes, and censored what he considered indecent. 
Dryden’s	modernizations	were	very	popular	and	fit	more	
closely into the horizon of expectations of the time.  The 
social function of the legenda form, if distant in Chau-
cer’s	 time,	was	 no	 longer	 apparent	 to	 the	 19th century 
reader.  In turn, changes within the symbolic system 
between	Christians	and	Jews	render	the	Prioress’s	char-
acter opaque to modern readers.  Her anti-Semitism is 
troubling because it confronts us in the aftermath of a 
long history of Jewish persecution which culminated in 
the recent history of the holocaust.
Bibliography
Anderson, Robert,	ed.	(1795).	The	Prioress’s	Tale.	The 
Works of the British Poets. London.
Benson, Larry, ed. (1987). The Riverside Chaucer. Bos-
ton:	Houghton	Mifflin.
Graver, Bruce. (1998). Translations of Chaucer and Vir-
gil. Ithaca: Cornell UP.
Hallissy, Margaret. (1995).  A	Companion	to	Chaucer’s	
Canterbury Tales. Westport: Greenwood.
Hoover, David L. (2008). Quantitative Analysis and Lit-
erary Studies. A Companion to Digital Literary Studies, 
ed. Susan Schreibman and Ray Siemens. Oxford: Black-
well.
HyperPo. (2008). Text Analysis Portal For Research. 
http://portal.tapor.ca/portal/portal (accessed 20 October 
2008).
Jauss, Hans Robert. (1982). Trans. Timothy Bahti.  To-
ward an Aesthetic of Reception. Minneapolis: U of Minn 
P, 1982.
Lipscomb, William,	 ed.	 (1795).	 The	 Prioress’s	 Tale.	
The Canterbury Tales, complete in a Modern Version. 
Oxford.
Tomasch, Sylvia. (2000).  Postcolonial Chaucer and the 
Virtual Jew.  The Postcolonial Middle Ages. Ed. Jeffrey 
Jerome Cohen. New York: Palgrave, pp. 243-260.
Tyrwhitt, Thomas, ed. (1795). The Canterbury tales of 
Chaucer.
Wilsbacher, Greg.	 (2005).Lumiansky’s	 Paradox:	 Eth-
ics,	Aesthetics	and	Chaucer’s	‘Prioress’s	Tale.’		College 
Literature. Vol. 32, Iss. 4: 1-29. 
Wordsworth, William.	 (1947).	 The	 Prioress’	 Tale.	
Wordsworth’s	 Poetical	Works. Ed. E. Selincourt. Lon-
don: Oxford UP.
Wu, Duncan. (1993). Wordsworth’s	 Reading	 1770-
1799. Cambridge: Cambridge UP.
DIGITAL HUMANITIES 2009
Page  361
An Approach to Information 
Access and Knowledge 
Discovery from Historical 
Documents
Fuminori Kimura 
Ritsumeikan University 
fkimura@is.ritsumei.ac.jp 
Akira Maeda
Ritsumeikan University 
amaeda@media.ritsumei.ac.jp
1. Introduction
Recently, libraries, governments and major internet providers, are forming consortiums to preserve his-
torical documents stored in libraries. (e.g. Google Book 
Search, Open Content Alliance, World Digital Library, 
Hathi Trust, etc.). It means that more and more old text 
contents will be accessible on the internet in the near fu-
ture. Obviously, huge amount of knowledge in old docu-
ments is as important as recent born-digital documents 
typically available on the web, because old documents 
are the collection of wisdom from B.C. Thus, it might 
be useful to be able to access such old documents. More-
over, it is very useful to discover hidden knowledge and 
wisdom written in these old documents. 
In order to realize this purpose, it is necessary to retrieve 
important information from old documents. However, 
it is not always easy to retrieve old documents, mainly 
due to the substantial change in language and culture 
over time. Therefore, we need a method to access old 
documents written in ancient language using a query 
in modern language. We call this method “Cross-Age 
Information Retrieval”. Moreover, we should consider 
the cultural difference over time, even for the same lan-
guage. For this, we need a method of “Cross-Cultural 
Information Retrieval”.
Most of the research on information retrieval and infor-
mation access focus on documents written in modern 
language, but we believe that knowledge and wisdom 
written in old documents provide rich and valuable in-
formation which are not available in modern language 
documents, especially in web contents.
We propose a “Cross-Age Information Retrieval” meth-
od in order to tackle these problems. It aims to discover 
hidden knowledge and wisdom written in old documents.
2. Related Work
Much research on Cross-Language Information Retriev-
al has been conducted in the last 10 years, with the back-
ground of the rapid growth of the web around the world 
since	the	middle	of	1990’s.	Various	approaches,	includ-
ing query translation, document translation, and the use 
of intermediate language has been studied, and for cer-
tain language pairs (e.g. between European languages), 
adequate retrieval effectiveness has been achieved.
On the contrary, there is very little research on informa-
tion retrieval method for historical documents, and most 
of which are based on simple keyword matching. Re-
cently, some approaches have been proposed to access 
historical documents, and it could be regarded as a kind 
of Cross-Age Information Retrieval (Gerlach et al. 2007; 
Khaltarkhuu et al. 2006). Our goal is to establish a more 
effective and sophisticated retrieval method that consid-
ers not only language difference over time, but also cul-
tural difference between languages and ages.
3. The Proposed Method
We adopt dictionary-based query translation approach, 
since it is proven to be the most effective method for 
Cross-Language Information Retrieval. In order for dic-
tionary-based methods to be effective, we need to use 
precise and comprehensive dictionaries for both modern 
language and ancient language. From these two diction-
aries, we try to discover relationships between entries in 
those dictionaries, and to “translate” the query terms in 
modern language into equivalent terms in ancient lan-
guage. For this translation process, we propose the fol-
lowing method (Fig. 1):
Fig. 1 Overview of the proposed method for Cross-Age 
Information Retrieval.
1. For each entry in the modern dictionary, we look 
for an equivalent entry in archaic word dictionary 
by	calculating	the	similarities	between	the	definition	
of	 the	modern	word	 and	 all	 the	 definitions	 of	 the	
archaic words. For this process, we can use standard 
text similarity measure based on vector space model 
DIGITAL HUMANITIES 2009
Page 362
and tf-idf term weighting scheme.
2. Then,	we	take	the	most	similar	definition	in	archaic	
word dictionary, and that entry (headword) is re-
garded as an equivalent of the modern word.
3. If more than one equivalent entry exists, we disam-
biguate the translation candidates using the term 
association measure such as mutual information, to 
find	the	most	equivalent	archaic	word	for	the	mod-
ern language word.
4. Document Collections
Currently, it is not very easy to obtain historical docu-
ments in text format. However, some digital libraries 
(e.g. Google Book Search, Open Content Alliance, etc.) 
are ready to provide their collection of historical docu-
ments in text format for research purposes. Moreover, 
there are numerous existing old documents available 
online. In Japan, there is a volunteer-based effort called 
“Aozora Bunko” to digitize and to make accessible over 
7,000 copyright-expired classic literatures online. Also, 
many universities and institutions have already been 
providing collections of old documents in text format. 
We can use these huge collections of old documents for 
our proposed method.
For now, we are focusing on a Japanese historical docu-
ment called “Hyohanki”, which was written in late He-
ian era (12th century) in Japan. It is a valuable resource 
for the research of Japanese culture of that time period. 
An example of its original copy is shown in Fig. 2. Al-
though some part of it has been deteriorated and missing, 
all of the existing pages are digitized into text format. 
The existing pages consist of 2,488 diary entries.
Fig. 2 Example of the original copy of a historical 
Japanese document “Hyohanki”.
5. Language Resources
As described in Section 3, we need dictionaries in order 
to translate modern language query into archaic term(s). 
In the case of “Hyohanki”, we can use some existing 
electronic dictionaries available in CD-ROM. For Japa-
nese modern language, we use “Kojien”, one of the most 
famous and comprehensive Japanese language diction-
aries. For ancient language, we use “Kokugo-Daijiten”, 
which covers not only modern words but also archaic 
words.
6. Preliminary Experiment
We conducted a preliminary experiment to test the preci-
sion of “Cross-Age retrieval” by our proposed method. 
In this experiment, we used diary entries of “Hyohanki” 
as the ancient Japanese document collection, and pre-
pared 3 modern Japanese queries, “戦争 (war)”, “法要 
(Buddhist service)”, and “裸足 (bare foot)”. Since each 
query has an equivalent archaic term in different word-
ing, no relevant documents can be retrieved if we use 
these modern term queries. Note that, we consider one 
diary entry as one document. 
Table 1 shows the original modern Japanese query, an-
cient Japanese term(s) translated by the proposed meth-
od, and the precision of retrieval using the translated 
term(s). For the queries “法要 (Buddhist service)” and 
“裸足 (bare foot)”, the proposed method worked quite 
well and  chieved almost 100% precision (the ratio of 
relevant documents in retrieved documents). However, 
the query “戦争 (war)” resulted in very poor precision 
(27%). The reason for it is that the proposed method re-
turned two translation candidates (i.e. “戦” and “軍”) for 
this query. If we take only “戦” as the translated query, 
we could achieve 100% precision, but if we take only “
軍”, we could obtain only 3.6% precision. It is because 
the archaic term “軍” has not only a meaning “war”, but 
also	other	meanings	like	“general	(officer)”	and	“army”.	
The query “死亡 (death)” also resulted in very poor pre-
cision (15%) and the reason for it is that the translation 
“没” has several meanings, “death”, “deprivation” and 
“sunset”. These results suggest that we could improve 
the precision if we incorporate a suitable disambiguation 
method for the translated archaic terms. For that pur-
pose, we could apply existing disambiguation methods 
used in Cross-Language Information Retrieval, such as 
mutual information, etc.
DIGITAL HUMANITIES 2009
Page  363
7. Conclusion
In this paper, we proposed a novel information retrieval 
technique called “Cross-Age Information Retrieval”, 
which can be used to access old documents written in 
ancient language using a query in modern language. We 
conducted a preliminary experiment to test the precision 
of cross-age retrieval by our proposed method. The ex-
perimental results showed that our proposed method is 
potentially useful for cross-age retrieval. Although our 
proposed technique is still in an early stage, we believe 
that we can achieve adequate retrieval effectiveness by 
incorporating techniques used for Cross-Language In-
formation Retrieval.
Our goal is not only to realize cross-age retrieval, but 
also to extend this technique to more advanced text min-
ing applications in order to discover hidden knowledge 
and wisdom from large amount of premodern documents 
which are now available in digital form.
Our future work includes resolving ambiguity of trans-
lated archaic terms, large-scale experiments in other 
languages such as English, consideration of cultural dif-
ference over time, and thus extending our technique to 
realize cross-age, cross-cultural, and cross-language in-
formation access.
References
Gerlach, A. E. and Fuhr, N. (2007). Retrieval in text col-
lections with historic spelling using linguistic and spell-
ing variants. In Proceedings of the 7th ACM/IEEE Joint 
Conference on Digital Libraries (JCDL 2007), pp. 333-
341, 2007.
Khaltarkhuu, G. and Maeda, A. (2006). Retrieval Tech-
nique with the Modern Mongolian Query on Traditional 
Mongolian Text. In Proceedings of the 9th International 
Conference on Asian Digital Libraries (ICADL2006), 
pp. 478-481, 2006.
Modulating Style (and 
Expectations):  An Experiment 
with Narrative Voice in 
Faulkner’s The Sound and the 
Fury
Caitlin Crandell
Stanford University 
crandell@stanford.edu
Emily Gong 
Stanford University 
mlygng@stanford.edu
Rachel Kraus 
Stanford University 
rkraus1@stanford.edu
Tiffany Lieu 
Stanford University
Jacob Mason-Marshall 
Stanford University 
jacobm2@stanford.edu
Faulkner’s	The Sound and the Fury provides an in-teresting test bed for stylistic analysis of narrative 
and authorial voice. Readers of the text understand that 
the work contains at least four different “voices,” and 
if we include the Appendix that Faulkner wrote after 
the	 book’s	 initial	 publication,	 then	 we	 have	 five	 dis-
tinct voices, namely those of Benjy, Quentin, Jason, the 
Omniscient narrator of the fourth section, and then the 
somewhat mysterious voice of the Appendix that some 
have	called	Faulkner’s	own	voice.	As	part	of	Dr.	Mat-
thew	Jockers’s	humanities	computing	seminar	at	Stan-
ford this fall, the authors of this proposal conceived of 
an experiment to investigate narrative voice and utilize 
authorship attribution techniques to explore the extent 
to which Faulkner is able to “create” distinct narrative 
styles within The Sound and the Fury. 
Starting	with	a	 traditional	 reading	of	Faulkner’s	novel,	
our group employed the traditional tools of literary anal-
ysis as a sort of “control.”  We explored the differing 
styles	content,	and	structure	of	the	five	separate	narrative	
voices in The Sound and the Fury.  We paid close atten-
tion to the stylistic changes, and we hypothesized about 
what qualities separate the different sections.  We put all 
DIGITAL HUMANITIES 2009
Page 364
of these observations into an aggregated document; a tra-
ditional	“reading”	of	style	in	Faulkner’s	text.	
We then began a phase of speculation based upon our un-
derstanding of the sort of stylistic data that can be gener-
ated by a computer algorithm.   We speculated about how 
or	if	these	qualities	identified	in	our	traditional	reading	
might be detectable via computer-based text analysis. 
We then drew up a series of hypotheses and predictions 
about what sort of differences might be made evident by 
a quantitative, computer analysis of the text.
Despite	the	significant	shifts	in	style	detected	by	readers,	
our	first	prediction	was	that	in	terms	of	the	most	common	
words,	Faulkner’s	narrative	would	be	 fairly	consistent.	
Our prediction was informed by current studies in au-
thorship attribution, which suggest that, with frequently 
occurring words at least, differences between authors are 
greater than difference between works by the same au-
thor.  With The Sound and the Fury, we had a particular-
ly compelling test case since Faulkner worked very hard 
to create unique narrative perspectives, even changing 
between	first	and	third	person.			
Our second prediction involved the very noticeable 
changes a reader detects when moving from section 
to section.  These changes constitute a sort of “narra-
tive dissonance” that repeatedly “jars” the reader as he 
moves between sections.  We speculated that these ef-
fects	would	be	difficult	for	a	computer	to	detect.
We formulated these and other predictions/hypotheses 
into a secondary document, and then began the more 
practical work of developing the tools necessary to test 
our hypotheses.  This work involved leveraging existing 
tools	 such	as	Patrick	 Joula’s	 JGAAP	and	also	creating	
new tools.  Most challenging has been our effort to cre-
ate a “dissonance” detector, a tool which at the time of 
this writing is showing great promise but still under ac-
tive development.
The paper we propose here represents a sort of experi-
ment in which we are both the subjects and the investi-
gators.  While we are overtly exploring narrative voice 
in Faulkner we are simultaneously investigating the 
role that a computer-based methodology might play in 
a conventional study of literature.  On this point we are 
inspired	 by	 Steve	 Ramsay’s	 observation	 that	 texts	 are	
“seldom . . .transformed algorithmically as a means of 
gaining entry to the deliberately and self-consciously 
subjective	 act	 of	 critical	 interpretation.”	 	 In	 our	 final	
analysis, we transform our text into a new sort of literary 
artifact, a statistical matrix that allows us to work in a 
reverse	 order—starting	with	 objective,	 quantified	 facts	
and moving to subjective interpretations of those data. 
Ultimately, we compare our objective and subjective 
methods and their respective conclusions to determine to 
not	only	explore	the	formal	aspects	of	an	author’s	narra-
tive	style,	but	to	imagine	what	weight	quantified	data	can	
bring to the traditional literary enterprise.
DIGITAL HUMANITIES 2009
Page  365
Les techniques informatiques 
au service des connaissances 
musicales de la renaissance 
Florence Le Priol
Université Paris-Sorbonne 
Florence.Le_Priol@paris-sorbonne.fr	 
Cristina Diego Pacheco
Université Nancy 2 
cristina.diego-pacheco@univ-nancy2.fr
Louis Jambou
Université Paris-Sorbonne 
Louis.Jambou@paris-sorbonne.fr
 
Le travail présenté dans cet article est la concrétisa-tion	 d’une	 collaboration	 interdisciplinaire	 entre	 le	
groupe « Lexique Musical de la Renaissance (LMR) » et 
l’équipe	«	Langages	Logiques	Informatique	et	Cognition	
(LaLIC) » visant à construire un outil pour un nouveau 
savoir	musical.	Comme	nous	 l’avions	présenté	à	Digi-
tal	Humanities	2007,	l’objet	du	projet	est	le	développe-
ment	de	cet	outil	qui	s’appuie	sur	une	base	de	données	
relationnelle et une interface web multilingue (français, 
anglais, espagnol et portugais). On y présente le langage 
musical depuis la naissance de la théorisation musicale 
en	langue	vernaculaire	jusqu’au	seuil	de	la	formation	du	
langage	 tonal.	 Aujourd’hui	 opérationnel	 (http://www.
pm.paris4.sorbonne.fr/LMR/), cet outil propose : 
•	 un lexique musical de la renaissance, 
•	 un dictionnaire du langage musical, 
•	 un ensemble de traités musicaux en langue espag-
nole. 
Lexique musical de la renaissance 
Le lexique musical de la renaissance (LMR) est un lex-
ique multilingue, cumulatif. Chaque terme regroupe, en 
chaque langue mais dans la succession alphabétique in-
tégrale, toutes les citations réunies par les collaborateurs. 
La constitution de ce lexique exige dans un premier 
temps un travail de lecture musicologique et linguistique 
des traités et une sélection des citations les plus pertinen-
tes	en	rapport	avec	l’acte	de	la	production	et	la	pensée	
musicales.	Dans	un	deuxième	temps,	le	travail	de	saisie	
dans la base de données est réalisé grâce aux interfaces 
de saisie mises à disposition. Pour chaque traité, les cita-
tions sélectionnées et les termes du lexique qui y sont 
attachées sont saisies. 
Actuellement,	 ce	 lexique,	 constitué	 à	 partir	 de	 l’étude	
de plus de 80 traités en espagnol (73) et en français (8), 
comprend	près	de	5500	citations	 (4223	en	espagnol	 et	
1258 en français) et plus de 5100 termes (3822 en espag-
nol et 1327 en français). 
Dictionnaire du langage musical 
Le dictionnaire du langage musical, issu de la réflexion 
musicologique et linguistique, comprend un répertoire 
raisonné et sélectif de citations pertinentes des dif-
férents langues techniques musicales vernaculaires. La 
sélection, dans la langue considérée, est opérée par un 
regroupement de citations sous des champs sémantiques 
ou	 acceptions	 qui	 donnent	 lieu	 à	 des	 définitions.	 Le	
nombre de champs sémantiques est une conséquence du 
contenu	signifiant	des	citations	elles-mêmes.	Le	diction-
naire	 se	 présente	 d’une	 part	 sous	 une	 forme	 unilingue	
où	 chaque	 langue	 est	 traitée	 indépendamment,	 d’autre	
part sous une forme multilingue où, sous des termes ou 
entrées multilingues des citations sont regroupées sous 
une même acception. Les termes/entrées multilingues ne 
résultent	pas	d’une	traduction	directe	ou	littérale	mais	du	
signifiant	même	des	citations.	
Traités musicaux en langue espagnole 
(TME) 
Le TME est une base de données qui regroupe un corpus 
de textes musicaux de la Renaissance écrits en langue es-
pagnole. Il vient ainsi compléter des projets semblables 
proposés par le TML, le LmL, le SMI et CANTO. Le 
TME vient compléter le corpus saisi dans le LMR en 
offrant aux chercheurs des citations contextualisées dans 
les sources primaires, manuscrites ou imprimées. Il per-
met	d’enrichir	les	définitions	lexicographiques	à	travers	
une	pensée	musicale	plus	large	et	d’intégrer	les	citations	
du	LMR	dans	 leur	 contexte	d’origine.	Chaque	citation	
du	LMR	est,	en	effet,	reli√©e,	par	un	simple	«	clic	»	au	
traité en texte intégral dont elle est issue. Trois traités 
sont disponibles, les autres sont en cours de saisie. 
Mise en oeuvre informatique 
Les techniques informatiques utilisées pour mettre en 
place	cet	outil,	tant	pour	la	zone	de	saisie	(accès	sécuri-
sé)	que	pour	la	zone	de	consultation	(accès	libre),	sont	
celles utilisées pour développer des applications dis-
ponibles en ligne : APACHE, PHP, JavaScript, MySQL, 
XML, XSLT et CSS. 
La base de données relationnelle, implémentée avec 
MySQL,	est	l’élément	principal	du	projet,	où	sont	capi-
DIGITAL HUMANITIES 2009
Page 366
talisées toutes les données musicologiques et linguis-
tiques provenant des traités. Elle est organisée en deux 
parties,	d’un	coté	une	base	intermédiaire	où	sont	enreg-
istrées	toutes	les	données	saisies,	de	l’autre,	 la	base	de	
données	 définitive	 où	 les	 données	 intermédiaires	 sont	
transférées	après	validation.	La	base	définitive	est	organ-
isée	en	plusieurs	tables	liées	afin	de	faciliter	l’accès	aux	
informations, par exemple, la table « tblauteur » regroupe 
les informations sur les auteurs des traités, la table « 
tblsource » comprend les informations bibliographiques 
des traités, les termes du lexique sont enregistrés dans la 
table « tblentree » ... 
Deux	types	d’interfaces	ont	été	développées	:	les	inter-
faces pour la saisie des données et celles pour la consul-
tation. Ces interfaces sont développées en PHP et JavaS-
cript	afin	d’offrir	une	compatibilité	multi-plateforme.	
Les	interfaces	de	saisies	(par	exemple	la	figure	1),	dont	
l’accès	est	sécurisé	par	mot	de	passe,	sont	constituées	de	
formulaire permettant aux chercheurs de saisir aisément 
leur travail, sans avoir à accéder directement à la base de 
données.	Ces	formulaires	ont	été	élaborés	afin	de	suivre	
au	plus	près	la	réflexion	musicologique	et	linguistique.	
Figure 1. Premier formulaire de saisie pour un nouveau 
traité 
La	consultation	des	données	est	en	accès	libre,	l’interface	
est multilingue : français, anglais, espagnol et portugais. 
Elle	donne	accès	au	LMR	(figure	2),	au	dictionnaire	et	
au	TME	(figure	3).	
Le TME est réalisé en utilisant XML, XSLT et CSS. La 
standardisation	des	traités	au	format	XML	et	l’utilisation	
des	 feuilles	de	 styles	XSLT	et	CSS	permettent	d’avoir	
une homogénéité dans la présentation et de lier le LMR 
au TME. En effet, lors de la consultation du LMR, un 
simple	«	clic	»	sur	l’icône	situé	après	la	citation,	affiche	
le texte du traité dans lequel la citation est surlignée 
comme	dans	l’exemple	de	la	figure	4.	
Figure 2. Interface (en français) de consultation du LMR 
Figure 3. Interface (en espagnol) d’accès au TME
Figure 4. Extrait du traité de G. Baena 
Références récentes 
Journée	 d’études	 «	Musique	 ancienne	 en	 Sorbonne	 »,	
2 juin 2006, Maison de la Recherche, Sorbonne, Paris. 
Présentation par Florence Le Priol et Louis Jambou des 
Problmatiques et enjeux du « Lexique Musical de la Re-
naissance » (LMR) 
Digital Humanities 2007, University of Illinois, Urbana-
Champaign, 4-7 juin 2007, Louis Jambou et Florence Le 
Priol, « Un outil pour un nouveau savoir musical », p 
98-100 
XI Settimana di alti studi rinascimentali, Invisibili 
Fili,	 Musica,	 Lessicograpfia,	 Editoria	 e	 tecnologie	
DIGITAL HUMANITIES 2009
Page  367
dell’informazione	tra	XVIe	XXI	secolo,	Università	degli	
Studi di Ferrara, 28-30 May 2009 
Du lexique au dictionnaire musicologique : 
•	 Aspects Théoriques (Louis Jambou) 
•	 Aspects informatiques (Florence Le Priol) 
AfricaMap Release I, Beta 
An Infrastructure for Collaboration 
http://africamap.harvard.edu
Benjamin Lewis 
Harvard University 
blewis@cga.harvard.edu	
Suzanne Blier 
Harvard University 
blier@fas.harvard.edu
Peter Bol
Harvard University 
pkbol@fas.harvard.edu 
In November of 2008 the Phase I release (beta) version of AfricaMap was made available to the Harvard com-
munity and the public.  The application can be accessed 
at http://africamap.harvard.edu.
AfricaMap sets out to address the problem of data avail-
ability for Africa.  Much public data exists, but it is so 
difficult	to	discover,	let	alone	obtain	that	many	research	
projects on Africa spend much of their budget gathering 
data.  Most people in Africa have an even harder time ac-
cessing mapping of their own territories. When research-
ers do gather data it is often once again lost because there 
is no place to store it where it can be found.  
The AfricaMap project represents a framework for or-
ganizing Africa data which can also be applied to other 
parts of the world.  At its core is a digital base map of the 
continent, viewable dynamically at a range of scales, and 
composed of the best cartographic mapping available. 
Behind the scenes a gazetteer starting with over 1 mil-
lion	 place	 names	 provides	 rapid	 navigation	 to	 specific	
locations on a vast landscape.  As more detailed map-
ping becomes available it will be added to the system. 
There  is no limit in terms of hardware or software to the 
amount of data that can be added to the system.    
AfricaMap is not be tied to a certain discipline but is in-
terested in storing or referencing data from all disciplines. 
AfricaMap will encourage collaboration.  Researchers 
will	 be	 able	 to	 define	 geographic	 areas	 of	 research	 so	
that	others	 can	find	out	 about	 their	work.	 	The	 system	
employs a Services Oriented Architecture (SOA), which 
means that all the data that the system displays does not 
have	to	be	stored	on	AfricaMap’s	servers.		The	data	that	
is stored on the AfricaMap servers is made available to 
DIGITAL HUMANITIES 2009
Page 368
other applications as map services
The idea for AfricaMap was developed under a Provost 
Funds for Innovative Technology funded project that is 
now being overseen jointly by Suzanne Blier and faculty 
and staff at the Harvard Center for Geographic Analysis 
(Peter Bol, Wendy Guan, Ben Lewis). It has the dual aim 
of supporting Harvard research that involves GIS work 
on the continent and of making data created in the course 
of research available to others. 
AfricaMap System Characteristics:
Web-based – The system takes advantage of the latest 
techniques for making large amounts of data and map-
ping discoverable and usable through a standard web 
browser.
Public access to holdings – Core holdings will be put 
in the public domain or licensed using a Creative Com-
mons type license wherever possible.  This means that 
researchers anywhere in the world will be able to down-
load and use these original materials without major re-
striction.
Encourage replication – One reason Africa data is hard 
to	find	is	that	the	data	which	exists	is	not	yet	well	rep-
licated.  By contrast the base map for the United States 
(the	Digital	Raster	Graphics	files)	area	easy	to	find	and	
exist on hundreds of servers.
Base mapping – Historic base maps for Africa are de-
veloped by scanning, cropping, and georeferencing 
maps from the Harvard Map Collection and elsewhere. 
Maps are digitized at a range of scales and for a range of 
time periods.    
Dynamic gazetteer – The gazetteer together with the 
base map form the core of the AfricaMap system. These 
two datasets support one another over time, allowing the 
gazetteer to grow and improve, which will make it easier 
to	find	places	on	the	base	map.	
Collaborative approach – Some tools to support collab-
oration	between	researchers	are	provided.				In	the	first	
version a permalink feature will allow any view of the 
system to be captured in a URL which can be shared.  In 
the next phase user created maps and map markup tools 
are anticipated.  Researchers will be able to download 
base mapping and other datasets.
Multiple scales – The system will support research at a 
variety of scales from sites or cities to country or conti-
nent-wide projects.
Multiple media types – The system will support access 
to many types of media in addition to spatial data, in-
cluding photos, maps, text, video, audio, and KML for 
Google Earth display.
Long term data access – Once maps are scanned, digi-
tized, georeferenced it should not be necessary for any-
one in the world to repeat that work.  Making digital 
materials available over time is not easy because tech-
nology changes. Techniques will be used to ensure long 
term access to public domain digital materials wherever 
possible.  
Improves over time – While the Harvard Map Library 
has large Africa holdings, it does not always have all 
maps for a given series, and there may be important se-
ries	which	it	does	not	have.		The	goal	is	to	fill	in	holes	
in the collection over time by sharing with other librar-
ies and collections.  Users will be able to submit data to 
Harvard using an online form.
Usability – Ease of use is of primary importance. It must 
be	easy	and	quick	 for	non-technical	people	 to	find	 the	
information they need. Researchers are the end users of 
this system and will be consulted frequently to guide the 
design of the user interface.
Text-based search of contents – Google-type text search 
against the contents of the entire system is possible with 
results displayed on the map.
Interdisciplinary approach –- The system will bring to-
gether mapped data (and facilitate the mapping of data) 
from a wide range of disciplines including archaeology, 
public health, history, linguistics, literature, zoology, 
natural resources to name a few.
Global approach – The goal is to create a technical 
framework to support research on Africa which could 
also be applied to other parts of the world.  It is hope 
that aspects of AfricaMap will be useful for organiza-
tions based in Africa whether it is the underlying data, 
data hosting services, map services, or the AfricaMap 
software.
Scalable – The data in the system will be cached as it is 
used.  This approach greatly increases performance and 
reduces server load, making the system far more scalable 
than a traditional web-GIS.  
Services oriented architecture (SOA) – The system will 
support access by other web and desktop systems and 
will in turn be able to access and display the maps on Af-
ricaMap directly via web services.  This means that other 
DIGITAL HUMANITIES 2009
Page  369
systems will not have to download the data to access it 
within their applications.
Cross Platform – AfricaMap can serve data services to 
other types of GIS platforms including ArcMap desktop 
and ArcGIS Server.  In addition, AfricaMap can dis-
play data served up from other platforms.  Data formats 
used	will	be	open	specification	ones	such	as	GeoTIFF,	
JPG2000, KML, and Shape.
Open Source – The software that runs AfricaMap is 
Open Source and available for users and organizations 
inside and outside Harvard to obtain and build upon.
Forging the Future: New 
Tools for Variable Media Art 
Preservation
Marilyn Lutz 
University of Maine 
lutz@maine.edu
Jon Ippolito 
University of Maine 
jippolitto@maine.edu
Sharon Quinn Fitzgerald 
University of Maine 
quinn@maine.edu
Richard Rinehart 
University of California, Berkeley    
Rinehart@berkeley.edu
While the number of tools for making and distribut-ing culture has exploded in the last half-century, it 
is	hard	to	find	a	tool	for	preserving	inherently	ephemeral	
media, such as performances, installations, and digital 
artifacts that have been created using digital, biological, 
performative and other variable media. The increased 
use by artists of multi-media, digital and internet me-
dia raises questions about conventional strategies by 
which society preserves, cares for, and redisplays these 
cultural artifacts. While the most obvious vulnerability 
of new media art is rapid technological obsolescence, 
the study of its other aspects that defy traditional con-
servation—including	hybrid,	contextual,	or	‘live’	quali-
ties—has provoked an investigation into new strategies 
for ephemeral media art preservation. While we can pre-
serve something of new media art, it will inevitably be 
changed.  Forging the Future, a consortium of museums 
and	cultural	heritage	organizations,	aims	to	fill	 the	gap	
with a free, practical toolset designed to rescue variable 
media works from oblivion by understanding possible 
alterations, distilling which matter most to the artist, and 
planning accordingly. 
The Forging the Future tools are based on research into 
innovative preservation standards and strategies by its 
members as part of the Variable Media Network (VMN) 
and Archiving the Avant-Garde working group. These 
include the Franklin Furnace Database, which catalogs 
past versions of a work and variable media artworks and 
DIGITAL HUMANITIES 2009
Page 370
events contained in small to midsize collections; the Dig-
ital Assets Management Database, which manages digi-
tal metadata that is directly relational to all the tools; the 
Variable Media Questionnaire (VMQ), which contains 
data and metadata necessary to migrate, re-create, and 
preserve cataloged variable media objects: What is the 
work in its original incarnation? What could the work be 
in later incarnations? Beta versions of these tools are in 
use by institutions ranging from major museums such as 
the Whitney Museum of American Art to small alterna-
tive spaces such as New Langton Arts. 
Scheduled for a public release in Spring 2009, the Forg-
ing the Future toolset will include features that help the 
separate tools interconnect with each other and with po-
tential databases in museums, libraries, and archives. In 
addition to the Franklin Furnace database and the third 
generation VMQ web service, these include the Vo-
cabWiki,	a	multi-institutional	effort	to	define	key	terms	
applicable to new media and variable media artifacts; 
Media Art Notation System (MANS), an XML speci-
fication,	 based	 on	 MPEG-21	 for	 translating	 data	 and	
relationships among tools; and the Forging the Future 
Metadata Registry, which enables users to track the same 
creator or work across different collections. 
This poster presents the Forging the Future intercon-
nected	tool	set,	and	focuses	specifically	on	the	VMQ	and	
its integration with MANS, complimented by the Vo-
cabWiki and the Metadata Registry. The VMQ is an in-
strument	for	determining	creators’	intent	as	to	how	their	
work should be categorized, seen, and (if at all) recreated 
in the future; it records opinions which may vary based 
on the work, the artist, the interviewer and date, about 
how to preserve variable media. MANS metadata is de-
rived from the VMQ interview(s) which separates logi-
cal information from physical hardware. It is a metadata 
framework that accommodates description of physical 
and digital assets at the same level. Rinehart uses the 
metaphor of a musical score, as a form of declarative 
and conceptual notation of music, and likens Media Art 
to musical compositions that are able to maintain their 
original integrity while being realized by different in-
struments or in different arrangements, over evolving 
time	periods.	Ippolito’s	VMQ	is	a	very	complex	method	
for	focusing	an	artist’s	attention	on	those	aspects	of	the	
work which need to be most enduring. The VMQ and 
MANs were designed to serve the needs of documenta-
tion, recreation and preservation of variable media. 
A crosswalk of the MANS notation system or ontology 
to ten library, art, and preservation metadata standards 
(METS,	PREMIS,	EAD,	FRBR,	PBCore,	etc.)	verified	
the	 need	 for	 a	 domain	 specific	 schema	 for	 structural	
and administrative metadata. This poster demonstrates 
how the heart of the process, the VMQ, collects essen-
tial preservation information that MANS supports in a 
unique,	infinitely	extensible	structure.	MANS	is	derived	
as an interpretation of the MPEG-21 standard, primarily; 
the Digital Item Declaration Language (DIDL) a type 
of XML that allows for greater, more granular descrip-
tions of multi-component digital objects. The intent is 
for MANS to act as a backbone to the artwork by being 
specifically	suggestive	though	not	overly	prescriptive	of	
how best to delineate and then, later, reinterpret an art-
work, and thereby address long-term preservation strate-
gies for variable media art  such as storage, emulation, 
migration, reinterpretation.
References
Bekaert, K., Hochstenbach, P. and Van de Sempel, H. 
Using MPEG-21 DIDL to Represent Complex Digital 
Objects in the Los Alamos National Laboratory Digital 
Library. V.9, No.11, November 2003, D-Lib Magazine 
http://www.dlib.org/dlib/november03/bekaert/11bekart.
html
Forging the Future Alliance. http://forging-the-future.
net/
Functional Requirements for Bibliographic Records, 
Final Report. IFLA Study Group on the Functional Re-
quirements	 for	 Bibliographic	Records.	 http://www.ifla.
org/VII/s13/frbr/frbr.pdf
Introduction to Metadata Pathways to Digital Informa-
tion, Metadata Standards Crosswalks. Version 2.1, 2008. 
http://www.getty.edu/research/conduction_research/
standards/intrometadata/crosswalks.pdf
Metadata Encoding and Transmission Standard: Primer 
and Reference Manual, Version 1.6 September 2007. 
http://www.loc.gov/standards/mets/METS%20Docu-
mentation%20final%20070930%msw.pdf
MPEG-21 Digital Item Declaration WD V2.0 http://xml.
coverpages.org/MPEG21-WG-11-N3971-200103.pdf
PREMIS Data Dictionary for Preservation Metadata, 
Version 2.0.  http://www.loc.gov/standards/premis/in-
dex.html
Ippolito, Jon. Death by Wall Label.  http://vectors.usc.
edu/thoughtmesh/publish/11.php
Rinehart, Richard. 20XX. A System of Formal Notation 
for Scoring Works of Digital and Variable Media Art. 
University of California, Berkeley.
DIGITAL HUMANITIES 2009
Page  371
User Guide to PBCore, Public Broadcasting Metadata 
Dictionary. Version 1.1.  http://www.pbcore.org/PB-
Core/UserGuide.html.
Variable Media Network. http://www.variablemedia.net/
Geography of Impertinence: 
Using Historical Maps to 
Explore a Spanish Treatise on 
Piracy
Clayton McCarl 
The Graduate Center of the City University of 
New York 
clayton.mccarl@gmail.com
In recent volumes of Literary and Linguistic Computing, 
Martyn Jessop has considered digital visualization as a 
scholarly practice (2008b), and argued, in particular, for 
the expanded use of geographical information in digital 
humanities scholarship (2008a). These approaches hold 
much promise for literary and cultural studies, where at-
tention has recently been given to such topics as “prose 
cartographies” (Padrón, 2004), the “cartographic imagi-
nation” (Smith, 2008), and a broad spectrum of other 
intersections between cultural production and physical 
space (both real and imagined), and especially within 
contexts of migration, conquest and colonialism (Arias, 
2002; Brückner, 2007; Michelet, 2006). With A Geogra-
phy of Impertinence, I hope to contribute to these discus-
sions, raising questions about the visualization of geo-
graphical material in a seventeenth-century Spanish text, 
relationships between textual data and historical maps, 
and the potential for employing place names as a mecha-
nism to facilitate cross-textual readings.
This project has grown out of my doctoral dissertation 
(currently in progress), an edition of Piratas y contra-
bandistas de ambas Indias (Pirates and Smugglers of the 
East and West Indies) by Spanish navigator and privateer 
Francisco de Seyxas y Lovera (1650-c.1705). A Geogra-
phy of Impertinence responds to three central problems I 
face in the editing of this text. 
The second question driving A Geography of Imper-
tinence is	 the	 relationship	 between	 Seyxas’	 text	 and	 a	
1630 atlas by the Portuguese cartographer João Teixeira 
Albernas. In Piratas y contrabandistas, Seyxas discuss-
es this book, which he possessed for several years before 
presenting it to Charles II and the Council of Indies. The 
maps	themselves	contain	Seyxas’	handwritten	additions	
and annotations, and include a rendering by Seyxas him-
self,	 disputing	Teixeira’s	 portrayal	 of	Tierra	 del	Fuego	
and the Straits of Magellan. Seyxas even inscribes him-
self in this geography, designating his own chain of is-
lands: the “Islas de Seyxas.” While these images could 
be used as illustrations, their close relationship to the text 
DIGITAL HUMANITIES 2009
Page 372
– a relationship that is, in a sense, reciprocal – suggest 
that a “one-way” navigation from text to map is unneces-
sarily limiting. Jessop describes a visualization as being 
a “parallel (rather than subordinate) rhetorical device” 
(2008b,	 283);	 certainly	Teixeira’s	maps	 can	 serve	 this	
function in relation to Piratas y contrabandistas. Indeed, 
the interaction between map and text is a compelling 
question on its own, apart from all geographical consid-
erations. 
Lastly,	I	seek	to	understand	Seyxas’	book	within	a	larger	
corpus of printed works dealing with issues of explo-
ration and imperial competition. Just as Piratas y con-
trabandistas	 is	 tied	 to	Teixeira’s	maps,	 it	 contains	 ref-
erences to over 70 Spanish, English, Dutch and French 
books, mostly historiographical treatises and expedi-
tionary journals. These include documents produced in 
the travels of individuals like Jacob Le Maire, Joris van 
Speilbergen, Bartholomew Sharp and John Narborough, 
as well as two volumes on nautical matters by Seyxas 
himself, printed in 1688 and 1690. These represent, of 
course, only a fraction of the enormous output of such 
books in the sixteenth and seventeenth centuries. While 
the intertexual bonds between these works go far beyond 
a common geography, place names provide a useful 
mechanism for exploring their differing geographical vi-
sions. For instance, the ability to navigate between these 
writings on the basis of toponyms may help us evaluate 
the role national or cultural bias plays in the imagination 
of place. 
My presentation will demonstrate a prototype applica-
tion which seeks to address the concerns outlined here. 
Built around a collection of place names, this application 
has much in common with a digital gazetteer, and thus 
draws upon work done by such projects, including the 
Alexandria Digital Library Gazetteer. The instances of 
a given place can be either textual or cartographic, and 
exploration of the collection can begin either at the in-
dex or within the texts or maps themselves. The model 
allows for the inclusion of a scholarly apparatus, as rec-
ommended by Jessop (2008b, 291) and as encouraged by 
the principles for documentation discussed in the Lon-
don Charter for the Computer-Based Visualization of 
Cultural Heritage (8-9). This apparatus is implemented 
using an object-oriented approach, enabling the shar-
ing of editorial material across textual and cartographic 
objects, and facilitating different levels of granularity in 
annotation, depending on the context of a given place 
instance. The model permits multiple names for a given 
place, and allows for annotation in multiple languages. I 
offer as test cases some of the most problematic places 
mentioned by Seyxas, and include in my textual corpus 
a sampling of passages by authors cited in Piratas y con-
trabandistas.  
References
Alexandria Digital Library Project. (1999-). Gazetteer 
Development. Available from: http://www.alexandria.
ucsb.edu/gazetteer (accessed 12 March 2009).
Arias, S. & Meléndez, M., eds. (2002). Mapping Co-
lonial Spanish America: Places and Commonplaces of 
Identity, Culture, and Experience, Lewisburg, Pa.: Buck-
nell University Press.
Brückner, M and Hsu, H., eds. (2007). American Liter-
ary Geographies: Spatial Practice and Cultural Produc-
tion, 1500-1900, Newark: University of Delaware Press. 
Jessop, M. (2008a). The Inhibition of Geographical In-
formation in Digital Humanities Scholarship. Literary 
and Linguistic Computing, 23(1), 39-50. 
Jessop, M. (2008b). Digital Visualization as a Scholarly 
Activity. Literary and Linguistic Computing, 23(3), 281-
293. 
The London Charter (2009). London Charter for the 
Computer-Based Visualization of Cultural Heritage 
(Draft 2.1). Available from: http://www.londoncharter.
org (accessed 12 March 2009).
Michelet, F. (2006). Creation, Migration, and Con-
quest: Imaginary Geography and Sense of Space in Old 
English Literature, Oxford: Oxford University Press. 
Padrón, R. (2004). The Spacious Word: Cartography, 
Literature, and Empire in Early Modern Spain, Chicago: 
University of Chicago Press. 
Smith, D.K. (2008). The Cartographic Imagination in 
Early Modern England: Re-writing the World in Mar-
lowe, Spenser, Raleigh and Marvell, Aldershot, UK: 
Ashgate. 
DIGITAL HUMANITIES 2009
Page  373
Capturing the Social Networks 
of the Gospels through a Graph 
Clustering
Maki Miyake 
Osaka University 
mmiyake@lang.osaka-u.ac.jp
The creation of social network representation is a promising application of large-scale linguistic re-
sources as a means of capturing the patterns of social 
relationships that exist among the individuals. In recent 
years, several notable studies have produced a number 
of social networks of the Bible and a variety of its graph 
representations for gaining insights into the interactions 
between the characters (Chris Harrison 2007, ESV Blog, 
2007). Graph representation is an effective way of de-
tecting and investigating the intricate patterns of connec-
tively within large-scale corpora. A number of studies 
have applied graph theory and network analysis meth-
ods to mapping out the complex networks of word as-
sociations within linguistic resources (Dorow, Widdows, 
Ling, Eckmann, Danilo, and Moses, 2005; Steyvers and 
Tanenbaum 2005; Gfeller, Chappelier, and De Los Rios, 
2005). In terms of the biblical texts, I have successfully 
applied a graph clustering technique to data processing 
that	 utilizes	 a	 clustering-coefficient	 threshold	 in	 creat-
ing sub-networks for the Gospels of the New Testament 
(Miyake, 2008). This paper reports on the application of 
a soft graph clustering method to four social networks 
of the Gospels constructed based on the co-occurrences 
of	 the	people	and	places.	Specifically,	 I	propose	a	soft	
graph clustering technique as a method of detecting the 
community structures within the social networks of the 
Gospels. The principle objectives of this study are to 
investigate the interaction between the characters in the 
stories.
The corpus used in the present study is the Greek ver-
sion of the Gospels (Nestle-Aland, 1979) and the data 
mainly consists of names and places. A set of nouns such 
as son, father, prophet are including as well, that are re-
garded as important in representing the characteristics 
and the roles of people in the stories. In creating four 
social networks from the books of Mark, Matthew, Luke 
and John, co-occurrence data is computed for pairs of 
words that appear in the same verse (sentence) and the 
words remain morphological forms that make is possible 
to analyze the relationships between words such as “who 
says whom”.
Table 1 presents the number of co-occurrence words 
represented as nodes for each Gospel network and its 
basic statistical measure such as degree average values, 
the average shortest path and the average of clustering 
coefficient	 that	 are	 clues	 for	 examining	 the	 structural	
characteristics. Degree refers to the number of words 
that are connected to a given word and its average val-
ue shows its connectedness of nodes within a network. 
These degree average values for each Gospel indicate 
that these social networks have patterns of sparse con-
nectivity.	The	clustering	coefficient	is	known	as	the	in-
dex for investigating probabilities that an acquaintance 
of an acquaintance is also an acquaintance of yours, in 
other words, as an index of the inter-connective strength 
between neighboring nodes in a graph. Following Watts 
and	 Strogatz	 (1998),	 the	 clustering	 coefficient	 of	 the	
node	(n)	can	be	defined	as:	(number	of	links	among	n’s	
neighbors)/(N(n)*(N)-1)/2), where N(n) denotes the set 
of	 n’s	 neighbors.	 The	 coefficient	 assumes	 values	 be-
tween 0 and 1. In the social network study, the clustering 
coefficient	can	be	utilized	as	a	measure	of	role	ambiguity	
or community hubs that have numerous links. The four 
average	clustering	coefficients	for	the	total	nodes	are	all	
quite high values of 0.5 or more, indicating strong con-
nectedness between nodes, which is a characteristic of 
small-world networks.
Mk Mt Lk Joh
node 258 179 257 146
Degree 
average
5.09 5.92 5.3 6.96
 (% for the 
total nodes)
2% 3% 2% 5%
Average<C> 0.58 0.55 0.57 0.52
 Table 1
In order to discern the relationships among the words 
within the social network, this study applies a soft clus-
tering method combining the hard clustering of Markov 
Clustering	 (MCL)	 and	 the	 index	 of	 clustering	 coeffi-
cients.	MCL	is	the	bottom-up	classification	methods	that	
allow us to detect the patterns and clusters within large 
and sparsely connected data structures. Within the MCL 
process, a graph is partitioned into hard clusters. Howev-
er, one particular problem with applying MCL to linguis-
tic resources is that the hard clustering approach is not 
appropriate for words that have multiple meanings, more 
specifically	for	the	individuals	who	are	involved	in	mul-
tiple communities in the context of social networks. In 
order to overcome this problem, that hard clustering of 
DIGITAL HUMANITIES 2009
Page 374
MCL is applied to the network removed the bottleneck 
nodes	which	are	identified	as	the	hubs	by	taking	the	clus-
tering	coefficient	as	a	 threshold.	In	this	study,	 the	only	
nodes	with	 the	 clustering	 coefficient	 of	more	 than	 0.2	
were selected. After the MCL process, the resultant crisp 
clusters are expanded with neighboring nodes to produce 
overlapping clusters that include in the hub nodes.
Figure 1 plots the numbers of the MCL cluster sizes for 
each network, which illustrate the transitions occurring 
in downsizing the networks generated from graph clus-
tering. Taking the result of Mt for an example, the MCL 
resulted in 44 hard clusters, with an average of ?? clus-
ter components (SD=??). Comparing between clusters 
across the four social networks illustrates the different 
roles associated with characters in the Gospels, such as 
Jesus as the son of God in Mark and Jesus as Savior in 
Luke. 
1
10
100
1000
Node MCL
Mt
Mk
Lk
Joh
Figure 1
In order to compare between clusters across the four 
social	 networks,	 the	 Jaccard	 coefficient	 is	 appropriate	
index for measuring similarity between clusters, which 
can be computed based on the number of elements in 
the intersection set divided by the number of elements in 
the union set. The table 2 presents the results of average 
Jaccard	coefficient	value	among	four	Gospels,	indicating	
that	 the	 similarities	 among	 first	 three	Gospels	 such	 as	
Matthew, Mark, and Luke are higher than those for John. 
The	first	three	Gospels	have	been	referred	to	as	the	Syn-
optic Gospels, because a high similarity has been recog-
nized among them. Table 3 presents a set of clusters with 
the highest similarity for the Synoptic Gospels, which 
can easily refer to a phrase beginning with “Render onto 
Ceasar”. Figure 2 is a sample of the sub-networks focus-
ing on the word of Peter and its neighboring nodes. As 
the neighboring structures for the node of Peter are dif-
ferent among four networks, such a graph representation 
makes it possible to examine structural similarities and 
differences among social networks.
　 Mt Mk Lk Joh
Mt 1 0.198 0.215 0.128
Mk - 1 0.195 0.125
Lk - -　 1 0.109
Joh - -　 -　 1
Mt Mk Lk
{θεου(God:gen), 
θεω(God:dat), 
καισαρι(Ceasar:	
dat), καισαρος	
(Ceaser: gen)}
{ανθρωποις,	
θεου	(God:gen), 
θεω	(God:dat), 
ιησους	
(Jesus:nom), 
καισαρι(Ceasar:	
dat), καισαρος	
(Ceaser: gen) }
{ανθρωποις,	
θεω	(God:dat), 
καισαρι	
(Ceaser: gen) 
22:21 They say 
unto him, Caesar's. 
Then saith he unto 
them, Render 
therefore unto 
Caesar the things 
which are Caesar's; 
and unto God the 
things that are 
God's
12:17 And Jesus 
answering said 
unto them, 
Render to Caesar 
the things that are 
Caesar's, and to 
God the things 
that are God's. 
20:25 And 
he said unto 
them, Render 
therefore unto 
Caesar the 
things which be 
Caesar's, and 
unto God the 
things which be 
God's. 
 Table 2 
Figure 2
In summary, this paper has reported on the application 
of a soft clustering method combining the clustering co-
efficient	 and	Markov	Clustering.	 Especially,	 the	 graph	
clustering technique offered an effective way of control-
ling over the hub nodes that are linked to numerous other 
nodes. Examining social networks is useful in exploring 
the interactions between characters and the features that 
underlie word groups within the Gospels. In pursuing the 
precise communities the characters are involved in and 
the series of action and events in stories, the research is 
working to make the dataset more sophisticated, such as 
the treatment ambiguous names and personal pronouns.
References
S. van Dongen. (2000). Graph Clustering by Flow Simu-
lation, PhD thesis, University of Utrecht.
B. Dorow, D. Widdows, K. Ling, J. Eckmann, D. Sergi, 
DIGITAL HUMANITIES 2009
Page  375
and E. Moses. (2005). Using Curvature and Markov 
Clustering in Graphs for Lexical Acquisition and Word 
Sense Discrimination, Proceeding of 2nd Workshop or-
ganized by MEANING Project (MEANING-2005).
M. Steyvers, and J. B. Tenenbaum. (2005). The Large-
Scale Structure of Semantic Networks: Statistical Analy-
ses and a Model of Semantic Growth, Cognitive Science, 
29 (1): pp.41-78.
Gfeller, D., Chappelier, J.-C., and De Los Rios, P.. 
(2005). Synonym Dictionary Improvement through 
Markov Clustering and Clustering Stability, Internation-
al Symposium on Applied Stochastic Models and Data 
Analysis, pp. 106-113.
Maki Miyake (2008). Investigating word co-occurrence 
selection with extracted sub-networks of the Gospels 
Employing	Clustering	Coefficients,	Digital Humanities 
2008, pp.258-260.
ESV Blog. (2007). Mapping New Testament Social 
Networks, <http://www.esv.org/blog/2007/01/mapping.
nt.social.networks>
Chris Harrison. (2007). Visualizing the Bible, <http://
www.chrisharrison.net/projects/bibleviz/index.html>
Aspects of the Interoperability 
in the Digital Humanities
A Case Study in Buddhist Studies
Kiyonori Nagasaki 
Yamaguchi Prefectural University, Japan
nagasaki@ypu.jp	
A. Charles Muller
The University of Tokyo, Japan 
Masahiro Shimoda
The University of Tokyo, Japan
1. Introduction
In considering cases of interoperability in the Digital Humanities, it might be useful to focus on an example 
from	Buddhist	Studies	for	three	reasons:	The	first	is	us-
age of the multilingual Buddhist canons that were writ-
ten in Pali, Sanskrit and translated into Chinese, Tibetan, 
and so on before the 10th century. Thus, a system had to 
be established that allowed one to deal with resources 
that are composed not only by various families of written 
characters but also various languages. The second is that 
they	 are	 significant	 as	 resources	 in	history,	 linguistics,	
and	other	academic	fields	because	they	include	not	only	
Buddhist ideas but also information related to the ancient 
world of which Buddhism was a part. Thus, interoper-
ability	in	this	field	is	needed	in	order	to	synthesize	such	
related	fields.	The	third	reason	to	focus	on	this	case	study	
is	 that	 digitization	 projects	 in	 the	field	Buddhist	 Stud-
ies are in progress worldwide, having both academic and 
proselytizing purposes. Moreover, many projects house 
sub-projects	within:	for	example,	one’s	aim	might	be	to	
merely retrieve information on a local computer; or, on 
the other hand, one might want to publish critical edi-
tions of digitized canonical texts on the Web. Discus-
sion of this need for interoperability has already begun in 
the organization known as the Electronic Buddhist Text 
Initiative [EBTI]. We will discuss some aspects of that 
through our case study that follows.
2. A Case Study: SAT and the DDB
In this chapter, we will discuss the interoperability be-
tween two different projects in which we are engaged: 
the DDB (Digital Dictionary of Buddhism, http://www.
buddhism-dict.net/ddb/	 )	 and	 the	 SAT	 (SAT	Daizōkyō	
Text Database Committee, http://21dzk.l.u-tokyo.ac.jp/
SAT/ ). The DDB is a web-serviced lexicon that includes 
over 45,000 entries. The SAT project has digitized a 
scholarly edition of the Chinese Buddhist canon, con-
DIGITAL HUMANITIES 2009
Page 376
sisting of approximately 150 million Chinese characters 
in	 eighty-five	volumes.	 It	was	 compiled	 and	 edited	by	
Japanese	scholars	in	the	Taishō	Era	and	has	been	treated	
as	de	facto	standard	 text	 in	 the	field	of	Buddhist	study	
since then.
2.1. The DDB: The Digital Dictionary of 
Buddhism
The DDB was developed for the study of Buddhist texts 
written in classical Chinese and other East Asian lan-
guages that include Chinese character-based terminolo-
gy. This project was initiated in 1986 by Charles Muller, 
a specialist in East Asian Buddhism. In 1995, with the 
advent of the Internet, Muller converted his data set into 
HTML format, and placed it on the Web. During this pe-
riod from 1996-2000, the storage format of the dictionar-
ies was changed to SGML, and then to XML. In 2001, 
with the help of humanities computing guru Michael 
Beddow, the dictionaries were reset on the web in XML 
format with a search engine, and this structure remains 
in place down to the present day. The DDB features Bud-
dhist terms, texts, schools, temples, and persons. Entries 
range in scope from short glossary type, to full-length 
encyclopedic articles. Now supported by more than sixty 
collaborators with specialist's expertise in a wide range 
of areas in Buddhist studies, the expansion rate of the 
DDB has been exponential in recent years. 
A special dimension of the DDB is its usage of XML 
attributes to accredit contributors for their work at the 
level of entry sub-areas (XML "nodes") rather than only 
at the level of full entries, as seen in standard printed 
works. Furthermore, the relatively accessible XML tag 
structure (based loosely on the TEI model) has made it 
possible to integrate and interlink the DDB with other 
lexicons (such as EDict), and external text databases, 
such as the SAT text database.
2.2.	SAT:	The	SAT	Daizōkyō	Text	Database	
Committee
SAT	 is	managed	 by	 the	 SAT	Daizōkyō	Text	Database	
Committee (directed at present by Masahiro Shimoda). 
The database depends on an XML-like legacy scheme 
which	was	designed	in	1998	and	superficially	represents	
the pages of the edition. The textual corpus was digi-
tized and corrected mainly by about 200 young Buddhist 
scholars during a period of about ten years. While de-
scriptive markup has not yet been fully implemented, the 
locations of passages in the source texts, such as page, 
paragraph and line were precisely recorded so that the 
traditional methodology of the Buddhist study could be 
referred to transparently. It has been posted on the Web 
since April 2008.
2.3. The Interoperation between the DDB 
and the SAT
When the SAT Web service was started, it provided some 
interoperation with other related projects. The most im-
portant service provided is the search function for the 
entries of the DDB. The function adopts AJAX so that 
users can retrieve items transparently on their Web 
browsers. If users select a portion of the text with their 
mouse devices, they can view all terms in the text con-
tained in the DDB. The service is convenient for those 
who have interest in the text—especially beginners in 
Buddhist studies. In addition, the SAT Web service dis-
tributes some APIs. One of them is reference service 
based on the physical location. It provides a function to 
clip an arbitrary part of the texts by means of specifying 
the location or the range in a certain URI. The DDB and 
some other Web services adopt this API. The important 
merit of this interoperation is not only the usability, but 
also the management structure that allows each long-
term project to be sustained independently. It is so dif-
ficult	 to	manage	a	big	digital	project	 that	sometimes	 it	
may be disrupted, crash or even disappear. Although we 
may wish to increase our services, they often become 
unwieldy, even ending up in abandonment of the project. 
Enhancing Web services in order to support researchers, 
the interoperation with the other projects will also be a 
workable	alternative	in	the	field	of	Buddhist	studies.
3. Other Examples of Interoperation
SAT interoperates with some other projects such as 
CHISE and INBUDS. CHISE is an ontology mainly 
focusing on Chinese characters distributed under GNU 
GPL. SAT adopted CHISE to serve as a thesaurus of Chi-
nese characters in order to support its retrieval system.
INBUDS is a bibliographical database for the study of 
Indian philosophy and Buddhism. It is maintained by 
the Japanese Association of Indian and Buddhist Studies 
and includes 60,000 records that have been collected for 
twenty years. SAT implemented the interoperation with 
the INBUDS so that users could refer to the related aca-
demic resources. Some of the resources in the INBUDS 
are distributed as digital data.
4. Some Problems in Interoperability
As	 discussed	 previously,	 in	 the	 field	 of	 the	 Buddhist	
studies,	interoperability	is	quite	efficient	in	some	ways.	
On the other hand, all-too-common problems of interop-
erability	are	also	 found	 in	 the	field.	One	of	 the	 impor-
tant issues is that of organizational sustainability. If one 
side of the organization stops distribution of their own 
resources, the interoperation would end. However, it 
can be argued from another perspective that this is ac-
DIGITAL HUMANITIES 2009
Page  377
tually an advantage, because it allows ready awareness 
of systematic changes, allowing managers to adapt as 
necessary, for example, when one needs to salvage dis-
tributed data. Indeed, SAT has recently begun to support 
INBUDS because, after establishing the interoperation, 
it became clear that the programmers of INBUDS had 
been facing some problems with managing their data for 
some time. Therein, especially, in the case of a personal 
project, it is more secure to establish interoperation.
5. Conclusion
Just	 as	 “the	 fourth	 generation	 collections”	 in	 the	 field	
of the Western classics, digitizing projects for Buddhist 
studies are gradually shifting their own styles to the next 
generation which puts emphasis on interoperability. In-
teroperability not only exposes the problems inherent in 
the activities of the digitization of Buddhist studies, but 
also shows the ways to solve those problems. The same 
model will eventually hold true for the Humanities in 
general.
References
Muller, A. Charles. (2008). EBTI After 15 and CBETA 
after 10 Years: Joint International Conference on Digital 
Buddhist Studies, Chair's Report.
http://buddhism-dict.net/ebti/ebti2008report.html 
(accessed 12 November 2008).
Muller, A. Charles. (2008). The Digital Dictionary of 
Buddhism [DDB]: Present Status and Future Devel-
opments, The Ninth Annual Symposium for Scholars 
Resident in Japan, March 2008. http://www.acmuller.
net/articles/ddb-nichibunken-200803.html (accessed 12 
November 2008).
Crane, G. (2008). Fourth Generation Collections: TEI, 
FRBR,	 and	 Canonical	 Text	 Services,	 TEI	 Member’s	
Meeting 2008, Nov 2008.  http://www.cch.kcl.ac.uk/
cocoon/tei2008/programme/abstracts/abstract-160.html 
(accessed 12 November 2008).
Rehm, G. and Witt, A. (2008). Aspects of Sustainability 
in Digital Humanities, Digital Humanities 2008 , June 
2008: 21-29.
Nagasaki, K. and Shimoda, M. (2008). Outline of the 
Activities of the SAT Project, Joint International Con-
ference on Digital Buddhist Studies, at Dharma Drum 
Buddhist College, February 2008: 22-23.
Nagasaki, K. (2008). A Collaboration System for the 
Philology of the Buddhist Study”, Digital Humanities 
2008: 262-263.
MORIOKA, T. (2006). Character processing based on 
character ontology, IPSJ Technical Report, 2006-CH-
072: 25-32.
Eide, Ø., Ore, C. and Holmen, J. (2008).  Sustainability 
in Cultural Heritage Management, Digital Humanities 
2008 : 22-23.
DIGITAL HUMANITIES 2009
Page 378
The Soweto ’76 Archive: Virtual 
Heritage, Human Rights & 
Social Justice in the New South 
Africa 
Angel Nieves 
Hamilton College 
anieves@umd.edu
The Maryland Institute for Technology in the Hu-manities (MITH) and the Hector Pieterson Memo-
rial & Museum (HPMM) propose a transatlantic digital 
collaboration to create the Cultural Heritage Platform, 
an extensible web interface and toolset for the detailed 
study and conservation of historic resources. We further 
propose an extensive technical training and support pro-
gram for curators and staff at Constitution Hill, Johan-
nesburg; The District Six Museum, Cape Town; Red 
Location, Port Elizabeth; and Kliptown, Johannesburg, 
enabling them to populate the digital archive with con-
tent from their collections. This joint effort would create 
a core “digital cultural heritage trail” that could be fur-
ther extended to other related South African museums 
and that would allow students, teachers, and the general 
public from anywhere in the world to explore digital 
recreations of some of the most important places in the 
struggle against Apartheid. This paper will focus on the 
development of a digital archive, Soweto ’76, currently 
being built at the Maryland Institute for Technology in 
the Humanities (MITH) at the University of Maryland 
in collaboration with the Hector Pieterson Memorial & 
Museum. 
The Soweto ’76 project began in early 2006 as a collabo-
ration between the Maryland Institute for Technology in 
the Humanities (MITH) and the Hector Pieterson Memo-
rial & Museum (HPMM), Johannesburg, South Africa. 
The initial scope of the project included the digitization 
and preservation of the archival collections of the Mu-
seum with the intention of providing on-line access to its 
holdings for broad public use. Due to a lack of available 
resources for their care and preservation, these holdings 
were considered endangered, and the MITH project team 
began the process of digitizing them in 2006. By early 
2007 the team determined that the material in the archive 
could be best presented in an interactive 3D virtual en-
vironment with social networking functionality. Such an 
environment would stimulate critical historical thinking 
by raising questions about the nature and construction of 
historical narratives in newly developing democracies. 
In addition, the online archive would make the materi-
als in it broadly available to scholars interested in do-
ing research on the history of Soweto and the student 
movement against Apartheid. Currently Soweto ’76 is 
completing the digitization of its holdings including 
oral history interviews, video interviews, historic pho-
tographs, commemorative memorabilia, maps, and other 
material artifacts. A collection of over 200 South African 
newspaper articles from the period 1976-1980 have also 
been digitized and transcribed as part of the archive. The 
project staff has recently started the process of “tagging” 
(that	 is,	 describing)	 text,	 image,	 video,	 and	 audio	files	
from already digitized collections. 
As MITH developed the Soweto ’76 interface, they 
worked, according to the best principles of software de-
sign, to make their work as generalizable and as reusable 
as possible. It soon became apparent that the interface 
might be used for other archives to create a larger cultur-
al heritage platform for historic sites. This large, multi-
institutional archive has become the goal of the project 
we are now proposing. 
The link between human rights and the preservation of 
cultural heritage resources is often misunderstood. For-
getting our histories is politically problematic for many 
reasons, primarily because it denies the potential for 
building broader cultures of democracy. Nations must 
apologize and/or offer compensation for historical injus-
tices if there is to be a process of healing and remem-
brance. If we are truly seeking social justice, we must 
remember these historical injustices and recognize how 
they continue to shape identities even today. It is there-
fore essential to understand cultural heritage resources 
as	 a	 part	 of	 peoples’	 efforts	 to	maintain	 and	 construct	
their own identity. Historic sites are critical elements in 
the struggle for equality and democracy, and new tech-
nologies can be used to increase access to the informa-
tion kept in these important spaces. 
In addition to providing access to archival materials ex-
tant in collections such as those of HPMM, our database 
interface will allow users to explore historically accu-
rate recreations of heritage spaces in South Africa, and 
to access digital artifacts related to each particular space/
place. This focus on place is important because so much 
of the critical urban fabric of places such as Soweto, Dis-
trict Six, or Red Location was erased during the Apart-
heid era. Under the post-apartheid Mandela and Mbeki 
governments there has been an assumed connection 
between urban redevelopment and heritage programs, 
particularly if they promote foreign tourism. As some 
scholars have argued, “political transformations can cre-
ate new urban political identities, but the politics of tour-
ism can give cash value to the memorializing of select 
DIGITAL HUMANITIES 2009
Page  379
pasts.” Over a decade of democracy has brought massive 
reforms and advances across the heritage industry, but 
there remains a lack of understanding regarding the cul-
tural	 significance	of	Black	heritage	 resources	 in	South	
Africa’s	still	isolated	townships.	Unfortunately,	much	of	
the physically extant heritage does not easily lend itself 
to the traditional standards of what is considered “archi-
tecturally	significant”	or	“visually	impressive.”	Much	of	
the history of the anti-apartheid movement took place in 
the townships amongst what many heritage profession-
als would consider to be “the mundane” and ordinary 
structures and environments of the poor. Therefore, it is 
necessary to develop a different set of criteria and strat-
egies for documenting and preserving these important 
sites. 
Once the Soweto interface is complete, other cultural 
heritage institutions in South Africa concerned with 
human rights and social justice can be added easily to 
the digital heritage trail, as long as they conform to our 
metadata standards. Each additional institution thus will 
not only make its own content visible to the world, but 
will also enrich the collectively searchable content of the 
archive as a whole, which will always be greater than the 
sum of its constituent parts. In this way we hope to cre-
ate a “Digital Heritage Trail” that connects the cultural 
institutions in South Africa together in virtual space. 
Of	 course,	 creating	 the	 XML	 files	 that	 comprise	 each	
digital object can be demanding work, especially for 
those whose technical skills are not highly developed. 
For this reason, we will also provide access to the Ajax 
XML Encoder (AXE), developed in 2007-2008 at MITH 
(originally for the Soweto ‘76 project) with funding from 
a National Endowment for the Humanities Digital Start-
up grant. This tool allows non -technical users to describe 
or	“tag”	text,	image,	video,	and	audio	files	using	an	intui-
tive Web-based interface. Graduate students working on 
the Soweto project have already successfully used the 
tool, and a public beta version is planned for the end of 
the summer of 2008. Because our interface will be open 
source and our metadata standards will be readily avail-
able, they will be adaptable for the collections of other 
cultural heritage institutions in South Africa and around 
the world. 
Institutional Partners 
Hector Pieterson Museum & Memorial (HPMM), Jo-
hannesburg, South Africa Maryland Institute for Tech-
nology in the Humanities (MITH), College Park, MD, 
USA Africana Studies Program, Hamilton College, Clin-
ton, New York, USA 
Project Partners 
Dr. Angel David Nieves, Soweto ’76 Project Director 
Mr. Ali Khangela Hlongwane, Chief Curator & Mu-
seum Director HPMM  
Dr. Doug Reside, MITH Assistant Director  
Mr. Greg P. Lord, MITH Web Designer & Software 
Engineer  
Mr. Arik Lubkin, Graduate Student, UM-CP 
DIGITAL HUMANITIES 2009
Page 380
Close Only Counts in 
Horseshoes and... Authorship 
Attribution? 
John Noecker Jr. 
Duquesne University 
jnoecker@gmail.com
 
Mike Ryan 
Duquesne University 
michaelryan@acm.org 
Patrick Juola 
Duquesne University 
juola@mathcs.duq.edu	
Amanda Sgroi 
Duquesne University 
sgroia@duq.edu
 
Stacey Levine 
Duquesne University 
sel@mathcs.duq.edu	
Benjamin Wells 
Duquesne University 
wellsb1930@duq.edu	
How much of an effect do transcription errors in a text document have on the ability to do useful sta-
tistical analysis on that document? In order to perform 
authorship	attribution,	it	is	often	necessary	to	first	have	
a digital copy of the documents in question. The task 
of authorship attribution is to assign an author ship tag 
to a document of unknown origin based on statistical 
analysis of the text and comparison with doc uments of 
known authorship. This is often automated by means of 
a computer, which necessitates the exis tence of digital 
copies of all the works to be analyzed. The success rates 
of optical character recognition (OCR) systems make 
them an attractive choice for the creation of these digi-
tal copies. The various docu ments can be scanned into a 
computer and converted automatically to text. Rice et al. 
documented OCR per-character accuracy rates of great-
er than 90% for nearly all commercial OCR systems 
tested, most of which scored in a range of 95-98% ac-
curacy (1996). More recent commercial claims by OCR 
companies suggest that accuracy rates are above 98%. 
However, is a 5% or even 2% error rate acceptable when 
creat ing a statistical authorship model? Is it necessary to 
proofread each scanned document by hand before per-
forming authorship attribution, or is the error rate small 
enough that it is unlikely to affect the overall result? 
We intend to present new results showing that it is not 
necessary to proofread scanned documents before using 
them to perform statistical authorship attribution. In fact, 
these	 results	 suggest	 that	 no	 signifi	cant	 performance	
degradation occurs when analyzing documents with per-
character error rates of less than 15%. As this is well be-
low the published averages for OCR systems, there is 
little need to worry about the few errors which will be 
introduced during the automated image-to-text conver-
sion. Accuracy of study materials is of course crucial for 
an ideal authorship attribution study. As Rudman (1997) 
put it, “most non-traditional authorship attribution re-
searchers do not understand what constitutes a valid 
study.” In 2002, he wrote, “do not include any dubitanda 
—a certain and stylistically pure Defoe sample must be 
established—all decisions must err on the side of exclu-
sion. If there can be no certain Defoe touchstone, there 
can be no ... authorship attribution studies on his canon, 
and no wide ranging stylistic studies.” Ide ally, we would 
have access to the original manuscripts to make sure that 
what we have is the pure work -- but in the real world, 
we may not have such access. We argue, in contradiction 
to Rudman, that by assess ing the likely contribution of 
types of error—such as errors introduced by bad OCR 
technology—we can determine whether such errors are 
likely	to	shake	our	confidence	in	our	results.	If	we	can	
give 10:1 odds that a given paper was likely written by 
Defoe,	we	will	still	be	confident	if	we	learn	that	our	odds	
might be as low as 9.8 or 9.9:1. 
For this experiment, we made use of the Java Graphi-
cal Authorship Attribution Program (JGAAP  www.
jgaap.com), a freely available Java program for per-
forming authorship attribution created by Patrick Juola 
of Duquesne University. This modular program breaks 
the task of authorship attribution into three subtasks, 
described	 as	 ’Canonicization’,	 ’Event	 Generation’	 and	
’Statistical	Analysis’.	 During	 the	 Canoni	cization	 step,	
documents are standardized and various preprocessing 
steps can occur. For this experiment, we created a Can-
onicization method which randomly changed a percent-
age of the characters in each docu ment, simulating the 
per-character error of an OCR system. We also convert-
ed all characters to lower case during this step. We gen-
erated	a	feature	set	of	’Words’,	which	JGAAP	defines	as	
any string of characters separated by whitespace. Finally, 
we used a normalized dot product as a nearest neighbor 
algorithm to assign authorship tags to unknown docu-
ments. Noecker and Juola (personal correspondence) 
have sug gested that this normalized dot product scoring, 
DIGITAL HUMANITIES 2009
Page  381
which	 they	 refer	 to	 as	 the	 ’Cosine	 Distance’,	 outper-
forms many more complicated techniques and is espe-
cially well suited to the feature set we chose. 
In order to test this experiment on real world data, we 
have used the Ad-hoc Authorship Attribution Compe-
tition (AAAC) corpus. The AAAC was an experiment 
in authorship attribution held as part of the 2004 Joint 
International Conference of the Association for Liter-
ary and Linguistic Computing and the Association for 
Computers and the Humanities. The AAAC corpus 
provides texts from a wide variety of different genres, 
languages and document lengths, assuring that the re-
sults would be useful for a wide vari ety of applications. 
The AAAC corpus consists of 98 unknown documents, 
distributed across 13 different problems (labeled A-M). 
An	analysis	method’s	AAAC	score	is	calculated	as	 the	
sum of the percent accu racy for each problem. Hence, a 
AAAC score of 1300% represents 100% accuracy on all 
problems. This score was designed to weight both small 
problems (those with only one or two unknown docu-
ments) and large problems equally. Because this score is 
not	always	sufficiently	descriptive	on	its	own,	we	have	
also included an overall accuracy rate in our experiment. 
That is, we calculate both the AAAC scoring and the total 
percentage of unknown documents which were assigned 
the correct authorship labels. These two scores provide a 
fair assessment of how the technique performed both on 
a per-problem and per-document basis. 
The	 experiment	 itself	 consisted	 of	 fifty-two	 iterations	
of the authorship attribution process over the entire 
AAAC corpus. The presented results will be an average 
of the effect of random transcription errors from 1% to 
100% of the characters in each document. As previously 
noted, we calculated both the AAAC score and overall 
percentage correct over some 5,096 experiments (98 ex-
periments per iteration). These overall results suggested 
that there was essentially no decrease in performance on 
either the AAAC score or the overall percentage for er-
ror rates of 1% to 2%, which many commercial OCR 
systems claim to achieve. Even for more skeptical er-
ror rates of 5%, the overall percentage correct decreased 
by about 1%, and the AAAC score by only about 20. 
This trend continues until roughly a 15% error rate, af-
ter which the perfor mance drops off rather considerably. 
Still, as Rice et al. have reported, all major OCR sys-
tems are capable of error rates considerably lower than 
15%, which strongly suggests that there is little reason to 
spend addi tional time proofreading scanned documents 
before performing authorship attribution. 
Graceful Degradation: 
Managing Digital Projects in 
Times of Transition and Decline
Bethany Nowviskie 
University of Virginia 
bethany@virginia.edu
Dot Porter 
Digital Humanities Observatory 
dot.porter@gmail.com
Past panel discussions at Digital Humanities confer-ences	 (including	 one	 on	 the	 question	 of	 “finished”	
work and another on innovative management tech-
niques) have offered anecdotal evidence about factors 
contributing to the success of digital projects.1  Our 
journals and conference programs brim with accounts of 
work in progress, generally presented at the height of its 
success.  The exigencies of grant funding and conference 
or publication submissions tend to make the record more 
sanguine about our projects than, perhaps, their full life-
cycle would merit.  This poster takes a deliberative look 
at that darker side of project management with which we 
are all too familiar – the experience of projects that have 
entered states of transition or even decline.  We will open 
and describe a research methodology for a broad survey 
of the digital humanities community related to project 
management	–	specifically	on	how	we	 think	about	our	
projects and behave toward them when they face times 
of transition and decline, and what we see as the causes 
and outcomes of those times.
Decline is an especially pressing issue for the digital 
humanities because of the tendency of our projects to 
be open ended.  Traditional, long-term scholarly proj-
ects (even the small subset which, like the majority of 
digital projects, are collaborative in some way) are gen-
erally projected to end with the publication of a mono-
graph or scholarly edition – something solid to sit on the 
shelves, regardless of whether the project itself endures 
past a publication point.  Digital humanities projects are 
more likely to have output such as databases or websites, 
objects that beg a sustainability plan and require long-
term curation even if they are not continually updated.2 
They are also more collaborative than most scholarly 
projects and therefore more dependent on the continuing 
energy and goodwill of various institutional and intellec-
tual partners.  One could argue that digital projects are, 
by nature, in a continual state of transition or decline.3 
What happens with the funding runs out, and the original 
project staff has moved on or been replaced?  What hap-
DIGITAL HUMANITIES 2009
Page 382
pens when intellectual property rests with a collaborator 
or an institution that does not wish to continue the work? 
If projects are particularly vulnerable in transitional 
phases, how can we anticipate and ameliorate the effects 
of these times?   Does the valuation of projects against 
conventional measures of scholarly success (teach-
ing, research, service) or within traditional disciplinary 
boundaries impact their continuity or conclusion?  Is 
there actually something qualitatively different about 
digital projects versus “traditional” scholarly undertak-
ings? Are there new models for scholarly output that 
match more satisfactorily to the real-world outcomes and 
trajectories of digital projects?  Do certain kinds of early 
planning make projects more likely to weather changes? 
What brands of institutional support are most helpful to 
projects that are meeting their natural or unnatural ends?
Survey questions address the following issues:
•	 basic questions of project funding (its duration and 
dependability) and the role of local, institutional 
support;
•	 the	early	definition	of	 the	project	 and	 its	potential	
for “mission creep;”
•	 the	definition	of	short-term	vs.	long-term	goals	for	
the project and to what extent the project met them;
•	 the relation of the project to inquiry in traditional 
disciplines, to pedagogy, to published research, and 
to tenure and professional advancement;
•	 staff retainment and continuity;
•	 matters of intellectual property and open source;
•	 the use of tools or techniques for project manage-
ment;
•	 explicit risk management in project design and the 
“sustainability” both of the product of the work and 
of its production process;
•	 and whether the views of survey participants regard-
ing digital scholarship have evolved over time in 
response to transitional experiences with their own 
projects or to larger changes in academic culture.
The “Graceful Degradation” survey has been constructed 
under advisement of experts in qualitative data analysis 
at the University of Virginia.  It will be conducted online 
and in paper format over the course of several months 
beginning in the summer of 2009.  The authors of the 
survey are well positioned to solicit responses from both 
North American and European projects and will endeav-
or to reach out more broadly – both geographically and 
to communities of practice (including digital history, 
computer music, and electronic publishing) that have 
been somewhat underrepresented at past digital humani-
ties gatherings.  
At	Digital	Humanities	’09	in	Maryland,	we	will	solicit	
responses from conference attendees and present our 
methodology in poster format.  We anticipate that this 
survey will help us to determine fruitful lines for future 
inquiry, including projects deserving of careful case 
study presentation.  We hope to identify and share some 
best practices in the design and management of projects 
that weather transitional periods well.  We would also 
deem this poster presentation a success if it broke the 
taboo on conversation about those digital humanities ef-
forts that – for reasons we will be prepared to describe 
– have failed to degrade with a measure of grace. 
Notes
1“Innovations in Interdisciplinary Research Project 
Management,” Ramsay et.al., Digital Humanities 2008 
(Oulu,	 Finland)	 and	 “Done:	 ‘Finished’	 Projects	 in	 the	
Digital Humanities,” Kirschenbaum et.al., Digital Hu-
manities 2007 (Urbana-Champaign, Illinois).
2Daniel Pitti, “Designing Sustainable Projects and Pub-
lications,” in A Companion to Digital Humanities, ed. 
Susan Schreibman, Ray Siemens, John Unsworth. Ox-
ford: Blackwell, 2004. http://digitalhumanities.org/com-
panion/ (accessed 13 November 2008).
3Diane Zorich, “A Survey of Digital Humanities Centers 
in the United States.” CLIR Reports, November 2008. 
http://www.clir.org/pubs/reports/pub143/contents.html 
(accessed 13 November 2008).
DIGITAL HUMANITIES 2009
Page  383
Markup Schemes for Manga and 
Digital Reformatting Systems 
Kazushi Ohya 
Tsurumi University 
ohyakazushi@gmail.com	
1 Introduction 
In this paper, we will present two schemes, one for encoding Manga and one for the collation of graphic 
resources, both of which are used in reformatting sys-
tems. The schemes and reformatting systems will pro-
vide	multiple	merits	for	carving	out	a	new	research	field	
for digital humanities. 
2 New Field Opened Up by Manga 
Manga, or a graphic novel originating in Japan is gradu-
ally becoming known as new reading art, or a kind of lit-
erature even outside of Japan. Manga contains multiple 
graphic units on a surface unit(like a page) and some-
times texts are written in the background, which we call 
depicted letters. Each frame, which is a unit of an image, 
is sometimes related to another frame using a caption. 
And, each balloon can be dominated by multiple frames 
and possibly other balloons. It means that we have to 
prepare a scheme to handle graphic units with structures 
which are not the same as the text structure. To digitize 
Manga	in	markup	languages	requires	more	fine	encoding	
schemes for the relationship between image and text data 
rather than those for simply illustrated texts, like refer-
ring	 directly	 a	 corresponding	 image	file.	Manga	 could	
be a new target for encoding in markup languages, and 
a good test bed for checking adaptability of reference 
schemes like TEI[1]. 
Manga has graphic expressions which help us envisage 
situations the story represents, which means that transla-
tions of Manga will not be so different from language 
to language, compared to translations of other types 
of literature like novels. Then, we can expect that the 
multi-lingual corpora based on Manga will be semanti-
cally more stable. The corpora would be good resources 
of analysis for natural language processing especially for 
machine translations. 
Manga has the potential also for good corpora of ono-
matopoeia, or ways to imitate phenomena with letters, 
which can be regarded as a type of mimicry using pho-
netic values of letters/characters. Onomatopoeia have 
been neglected as academic research targets for a long 
time since its presence in linguistics or semiotics has 
been denied. However, in our view, onomatopoeia is 
a	 full-fledged	 domain	 of	 linguistics	 as	 well	 as	 nouns,	
sentences, and discourse units, or at least should be a 
domain for lexical items in multi-lingual dictionaries. 
Manga will provide a good resource for making corpora 
of onomatopoeia. 
List 1
List 2
3 Schemes for Text and Images of Manga 
As a preliminary experiment, we had encoded two titles 
of Manga, Nodame Cantabile and Crayon Shinchan, fol-
lowing the scheme of a drama module for performance 
text	in	TEI	P5,	and	confirmed	that	the	scheme	can	also	be	
used as a base scheme for Manga. This could mean that 
Manga can be treated as a kind of “continuities” which 
are	used	as	scripts	in	film	productions,	sometimes	called	
storyboards.	However,	we	also	confirmed	that	we	should	
revise the scheme to describe the connections between 
frames, balloons, and texts. For example, it is impossible 
to encode all of the correlation of abstract units and im-
ages, like a section and multiple page images, in a text 
data structure, 
4 Reformatting Systems 
The Library of Congress in the US started a reformatting 
program, whose aim is mainly the preser vation of the ap-
pearance of physical books or documents[11]. We plan to 
DIGITAL HUMANITIES 2009
Page 384
make reformatting systems, but the objective is different 
from	L.C.’s.	Our	system	will	be	designed	for	converting	
physical forms into digital forms, and then back to physi-
cal formats like: books to scrolls, scrolls to books, books 
to books, and so on. The idea comes from discovery and 
observation as follows. The extant Ouma Jirushis, the 
early color-printed publications in Japan, are all in scroll 
formats, except for one in Tsurumi University, which is 
in a set of folios in which holes for binding remain. All 
the materials contain the number of leaves and volumes, 
printed in the center of each folio. It would mean that 
the Ouma Jirushi was originally binded in or printed for 
a book form, but later was re-formed into a scroll for-
mat. This kind of shift in formats can now be observed in 
printed music. Players sometimes reform a sheet or book 
of music to a long sheet or scroll of music for making 
easy to follow notes without turning over any pages or 
sheets. For narratives which contain time-sensitive con-
tent, scrolls would be more suitable than books. A format 
style would not have developed from being a primitive 
one to being an advanced one in history. Digitally refor-
matting systems will provide opportunities to verify the 
propositions and re-examine which formats are the most 
suitable for the content that existing materials have.
Figure 2 
5 Reformatting Manga from Recto First to 
Verso First Read ing Style 
According to a news story in the Yomiuri Shimbun[13], 
French translation version of Pink, which is a title of 
Japanese Manga, was published with two formats; one 
for reading from left to right pages, which is an ordinary 
way to Western culture, and one for reading from right to 
left pages, which is an authentic Manga style. The latter 
is easy to make a translated version, however the sales 
have not been good as compared to the former. This kind 
of trials has also been carried out in US[12]. Making a 
verso	first	 reading	 style	version	 costs	 publishers	much	
time	 and	 money	 for	 re-fabrication.	 For	 example,	 let’s	
compare	an	original	format	in	Fig.1	to	a	recto-first	for-
mat in Fig.2. 
Since there is no change in images, the directions of 
frames and balloons are the same on both formats. How-
ever, the directions of language reading is different from 
them.	Then,	editors	have	tried	to	make	recto-first	formats	
into	verso-first	 formats	 for	English	versions.	Fig.3	 is	a	
verso-first	 format	with	 two	ways	 of	 changing:	 transla-
tional	 and	 reflectional	 symmetry	movements.	The	 first	
row in Fig.3 is a mirror image of originals. Each frame 
on the second row in Fig.3 is results of translational sym-
metry	movement.	Then,	on	the	first	row	a	balloon	direc-
tion is the same as a frame direction, but is not on the 
second row. On the other hand, information pictured in 
frames is kept on the second row instead. Which way is 
adopted in editing translation versions depends on edi-
tors’	expertise.	The	aim	of	our	system	is	to	support	the	
re-formatting processes. 
Fig.1 and Fig.2
6 Collation of Image Units 
In	 order	 to	make	 the	 reformatting	 system,	 first	 of	 all,	
we have to lay down a scheme for collation of graphic 
resources. The results from the scheme will be used as 
data	for	analysis	to	find	out	heuristic	rules	for	automatic	
frame relocation, then for instructions in converting sys-
tems. Making a scheme for collation of graphic units will 
be a new challenging target. Now we are experimenting 
DIGITAL HUMANITIES 2009
Page  385
with	descriptions	to	find	ideal	schemes	for	collation	of	
graphic units. 
Fig.1 and Fig.3
Sample Code
For example, encapsulating balloon in formation into 
frames in image structures looks to make easy to specify 
types of move ments in reformatting. However, it is still a 
problem in order to make adequate results to what extent 
we should encode instructions about image movements 
in collation data on the assumption that image processors 
can	 adjust	 fluctuating	 location	 information	 encoded	 in	
XML data.
7 Conclusion
Now we are making markup texts of several titles of 
Manga following the TEI scheme  as much as we can, 
and seeking a reference scheme for collation of image 
unites in comparing a Japanese version with the two dif-
ferent formats of English translation versions. We are 
planning to alter the TEI scheme to match our require-
ments especially making useful collation data for refor-
matting systems.
8 References
[1]L.Burnard and S.Bauman eds. (2007) The TEI 
Guidelines P5, TEI
[2]K.Ohya and S.Tutiya (1999) “Links between link 
elements in compound data units” in Japanese, IPSJ SIG 
Technical Report Vol.99. No.48., IPSJ
[3]K.Ohya (2006) “Markup problems: Syntactical anal-
ysis and steps to their resolution” in Japanese, TEI Day 
in Kyoto 2006 Report, Kyoto University
[4]K.Ohya (2008) “Management of links between link 
elements to represent correlation on link structures” in 
Japanese, IPSJ SIG Technical Report Vol.2008. No.100., 
IPSJ
[5]K.Yasuko et al. (2006) Witchblade Takeru Vol.1 in 
Japanese, Akita Shoten
[6]K.Yasuko et al. (2007) Witchblade Takeru Manga 
Vol.1, Bandai Entertainment.
[7]K.Yasuko et al. (2008) Witchblade Takeru Manga 
Vol.1, Top Cow Productions
[8]Y.Usui (1992) Kureyon Shinchan Vol.1 in Japanese, 
Futabashya
[9]Y.Usui (2002) Crayon Shinchan Vol.1, Comicsone
[10]Y.Usui (2008) Crayon Shinchan Vol.1, DC Comics
[11]The Library of Congress (2006) “Preserva-
tion Digital Reformatting Program”, The Library of 
Congress(USA)
[12](2006-11-21)	 “’Witchblade’	 Manga	 in	 Two	 For-
DIGITAL HUMANITIES 2009
Page 386
mats”, ICv2.com
[13](2007-04-03), ”Page order makes an impact on 
sales” in Japanese, Yomiuri Shimbun
Text and Pictures in Japanese 
Historical Documents
Takaaki Okamoto
Ritsumeikan University
04c0004@sch.otani.ac.jp
Majoring in Japanese History, I have been conduct-ing research with its focus on handwriting in his-
torical documents. Through searching for documents 
with the same handwriting from a great number of his-
torical documents, I collate documents written by the 
same person, analyzing them with questions of not only 
what	is	written,	but	also	why	this	specific	person	wrote	
this	 specific	 document,	 and	why	 this	 particular	 text	 is	
included	in	this	specific	textual	body.
Since comparisons and analyses of handwritings reveal 
what text-only analyses cannot, history and palaeogra-
phy have been arguing importance of the study of this 
kind. Despite this argument, however, we have only seen 
insufficient	progress	in	the	study	of	handwriting.	A	major	
reason for this derives from the fact that researchers have 
not been facilitated with a suitable environment to help 
them conduct ever repeating tasks of not only searching 
the same letters and characters in enormous amounts of 
texts of enormous amounts of documents, but also com-
paring them. This paper proposes a computer system to 
facilitate such an environment for historians and its pos-
sible applications for pictorial materials. Please note that 
what	I	am	aiming	for	here	is	not	automatic	identification	
of handwriting by computer. Rather, the computer is to 
help	researchers’	identification	by	organizing,	searching,	
and displaying data.
In	 order	 to	 identify	 somebody’s	 handwriting,	 first	 I	
have to search letters and characters that appear both 
in	 a	 ‘standard’	 document	written	 by	 that	 person	 and	 a	
document to be compared with. Since this is a fairly ex-
haustive research to look for all comparable letters and 
characters, I use computers to sort out and organize in-
formation about what kind of letters are located in what 
place in which document, and based on this, I have been 
doing research on methods to search and display charac-
ters or strings of characters.
There are two kinds of information about where a char-
acter is located in a document. One is logical informa-
tion, expressing information about the location by using 
terms	such	as	page,	line,	and	column.	You	can	find	ex-
amples	of	this	kind	in	book	indexes	and	the	computer’s	
full text search. The other is to point out the location 
DIGITAL HUMANITIES 2009
Page  387
visually. This is as if someone brings a book with him, 
opening the page, and pointing out to you exactly the 
place you are searching.
In the system I propose here, I separate characters in the 
text from each other, putting them into relational data-
base in which each character is treated as one record. 
For each character, I assign these two kinds of locus 
information. One is its location in the logical structure 
of the text—what number in terms of the page, the line, 
and the column; and the other is the physical location as 
expressed in the coordinates on the digital image. This 
kind of assignment, manually conducted, results in not 
only reconstructing text by assembling such characters, 
but also specifying the location of a character in a digital 
image of the document. When character data and im-
age data are linked through coordinates, we can create 
a character catalogue by cutting characters out of docu-
ment images. We can also search for a character or com-
bination of those and highlight them in the digital image, 
as if someone brings a book with him, opening the page, 
and pointing out to you the exact character you want as 
Fig. 1 shows.
I belong to the Japanese Culture Research Group a part 
of Global COE (Center of Excellence) program Digital 
Humanities Center for Japanese Arts and Cultures, Rit-
sumeikan University, and the Group puts focus more on 
ukiyo-e and other visual material than on textual ones 
such as archival documents. For this reason, I am now 
working on to systematize information on ‘what image 
is	where	in	what	material’	in	digitalized	images	that	the	
same	university’s	Art	Research	Center	has	been	ever	ac-
cumulating. This system works like putting tags with 
some notes on pictures, but by using computerized tags 
rather than paper ones. So, what is the point of using the 
computer here?
First of all, among the many merits of this procedure, 
we can create other contents, based on every piece of in-
formation about what is where. For instance, if we place 
a	mark	 on	 the	 publisher’s	 seal	 in	 an	ukiyo-e print and 
input its data, we can not only search for and display it 
in the database, but also make a program that creates a 
list	of	the	publisher’s	seals	by	cutting	out	the	image	parts	
with	the	publisher’s	seals	and	generate	it	in	PDF	or	other	
formats upon creating a layout that links the data inputs. 
Secondly, using the computerized tags means that we 
can show data on what is where in what material by us-
ing URLs. Imagine the situation, in which one research-
er may want to inform another researcher of a part of a 
picture in the collection of the Art Research Center ar-
chives. He might attach the whole image or only a part of 
it to his mail with detailed explanation. Instead of such 
toils, when utilizing the data on the web, he would only 
need to create the data on ‘what is where in what mate-
rial’	and	send	the	URL.	The	receiver	would	then	access	
the URL from his browser and inspect the picture with a 
tag attached to it and read the notes.
Untill now computers have mainly been used in the hu-
manities as a means to create databases. Starting with 
catalogues, now we can examine both full texts and im-
ages of textual materials. While the catalogues and full 
texts, and the catalogues and images, are respectively 
linked, we have seen little progress in the linkage be-
tween the full texts (or, in the case of pictorial materials, 
data on various elements of the picture) and the images. 
Since this system makes this possible and people can use 
it personally, researchers can organize their research ma-
terials of full texts and images which they have collected 
by linking them. Besides such personal use, this system 
can be developed to be a system for multiple users. I 
believe that by systematizing data on ‘what is where in 
what	material,’	 we	 can	 suggest	 further	 possibilities	 of	
applying computers for the humanities, and that makes 
significant	 contribution	 to	 not	 only	 study	 of	 handwrit-
ing, history and paleography but also the humanities in 
general.
Fig.1
DIGITAL HUMANITIES 2009
Page 388
Text-Image Linking 
Environment (TILE)
Dorothy Carr Porter 
Digital Humanities Observatory 
dot.porter@gmail.com
Doug Reside 
Uniersity of Maryland, College Park 
dresied@umd.edu
John Walsh 
Indiana University 
jawalsh@indiana.edu	 
Introduction 
To create the next generation of the technical infra-structure supporting image-based editions and elec-
tronic archives of humanities content, we are developing 
a new web-based image markup tool, the Text-Image 
Linking Environment (TILE), through a collaboration of 
the Maryland Institute for Technology in the Humani-
ties, Indiana University Bloomington, the Royal Irish 
Academy,	 the	 University	 of	 Oregon,	 and	 Harvard’s	
Center for Hellenic Studies. Despite the proliferation of 
image-based editions and archives, the linking of images 
and textual information remains a slow and frustrating 
process for editors and curators.  TILE, built on the ex-
isting code of the AXE image tagger, will dramatically 
increase	the	ease	and	efficiency	of	this	work.		TILE	will	
be interoperable with other popular tools (including both 
the Image Markup Tool and the Edition Production and 
Presentation Technology suite) and capable of produc-
ing TEI-compliant XML for linking image to text.  We 
will also put the image linking features of the newest 
version of the Text Encoding Standard (TEI P5) through 
its	first	rigorous,	“real	world”	test,	and,	at	the	close	of	the	
project, expect to provide the TEI with a list of sugges-
tions for improving the standard to make it more robust 
and effective. TILE will be developed and thoroughly 
tested with the assistance of our project partners, who 
represent	 some	 of	 today’s	 most	 exciting	 image-based	
editions projects, in order to create a tool generated by 
the community, for the community, with the expectation 
that, unlike so many other tools, it will be used by the 
community.
History of Images in the Digital Environment
Texts, from the earliest classical inscriptions to most 
twentieth-century correspondence, were originally in-
scribed on such physical objects as stones, papyrus 
scrolls, codex manuscripts, printed books, and hand-
written and typewritten letters.  As editors transfer a 
text from its original inscription, some of this context 
is necessarily obscured.  Further, editors must often 
make potentially questionable decisions as they inter-
pret the unclear or damaged text on the original arti-
fact.  A good editor will, of course, highlight such in-
terventions in textual notes, but such notes, usually in 
small type and inconveniently separated from the main 
text, often go unread.  The inclusion of page facsimi-
les can make the editorial process more transparent, 
but in print editions the reproduction of multiple, high 
quality images is often prohibitively expensive.  Digi-
tal facsimile editions, on the other hand, may be dis-
tributed far less expensively, and so many editors are 
now choosing to publish their facsimile editions online. 
 
The growth of the Internet as a public space in the 
early	 1990s	 led	 to	 the	 first	 generation	 of	 widely-ac-
cessible scholarly electronic archives, and even at this 
early stage many projects integrated images into their 
work	 in	 significant	 ways.	 	 The	Valley	 of	 the	 Shadow	
(1993), provided images for some of letters in the col-
lection (in relatively low resolution), and the Rossetti, 
Dickinson, and William Blake Archives brought to-
gether encoded texts and images or parallel viewing 
and study.1  The relationship between image and texts 
in these archives is quite simple: for example, the page 
image of the source of the edited text in the Valley of 
the Shadow or the Rossetti Archive may be opened in 
a separate window, but the links go no deeper than the 
page level.  One cannot, for instance, link from a word 
in the edited text to its location in the image or click 
an interesting area in the image to read an annotation. 
 
At the same time that these relatively open-ended online 
archives were under development, other scholars were 
taking advantage of digital technologies to build self-
contained scholarly editions.  Some of the earliest efforts 
include the Wife of Bath’s Prologue on CDROM (Chau-
cer 1996), the Electronic Beowulf (Kiernan 1999), and 
the Piers Plowman Electronic Archive, Vol. 1, (Langland 
2000). As with the online archives, these early editions 
were limited in how closely they linked image and text. 
The Electronic Beowulf did provide some annotations 
linked to areas on the manuscript folio image, but there 
are few of these as the coordinates for each had to be 
added to the HTML “by hand.”
As the community of scholars developing image-based 
projects has grown in the past decade, tools have been 
created that are actively used for project development. 
As of November 14, 2008, the project investigators know 
of no fewer than ten tools or collections of tools that al-
DIGITAL HUMANITIES 2009
Page  389
low users to edit or display images within the context of 
textual projects or editions. These range from those that 
simply display an image alongside a text, to very robust 
software suites which support the development of com-
plete image-based projects with substantial functionality 
beyond simple text-to-image mapping. 
The simplest tools enable the viewing of images along-
side text transcription, either for editing or for display. 
Juxta, developed through Networked Infrastructure for 
Nineteenth-century Electronic Scholarship (NINES) 
<http://www.nines.org/tools/juxta.html> provides a 
window	for	viewing	image	files	(if	provided)	alongside	
transcriptions, which could be very useful for an editor 
checking readings or adding annotations, but does not 
provide any method for connecting the image with the 
text beyond the page level. Similar is the Versioning Ma-
chine, developed by Susan Schreibman at the University 
of Maryland Libraries (http://v-machine.org/):  a display 
tool for comparing encoded texts that also enables page 
images to be linked to the text at the page level. These 
tools are both useful, but for those scholars who seek to 
include	more	fine-grained	linking	in	their	projects	they	
are not suitable.
There are also tools that support the linking of image to 
transcription or annotation. The Edition Production and 
Presentation Technology (EPPT), developed by Kevin 
Kiernan at the University of Kentucky under the aegis of 
the Electronic Boethius and ARCHway projects (http://
www.eppt.org/eppt/) is a set of tools that have been de-
veloped in and run through the Eclipse software devel-
opment platform. One of the main functions of the tool is 
to link transcription to an image of text, although it pro-
vides much more robust functionality. The Image Mark-
up Tool (IMT), under development by Martin Holmes at 
the	University	of	Victoria,	BC,	is	the	first	tool	to	output	
complete and valid TEI P5 XML.  The IMT enables a 
user to place a series of annotations on an image, result-
ing	in	a	file	that	validates	against	 the	regular	(unmodi-
fied)	TEI	P5	schema,	and	then	enables	the	user	to	create	
HTML for the display of those annotations online. The 
IMT is very simple and easy to use, and is in many ways 
a model of the type of tool that we will be developing in 
this project - it does one thing, and it does it very well. 
Unfortunately, the IMT runs only on Windows machines 
and cannot be easily ported into new web-based proj-
ects.  TILE will interoperate with the constrained IMT 
TEI format.
There have also been some efforts to build tools to auto-
mate the creation of links between transcribed text and 
image of that text.  Hugh Cayless at UNC-Chapel Hill 
has recently developed a system for automating im-text 
linking, a process he presented at the Text Encoding Ini-
tiative	Member’s	Meeting,	November	2008,2 and Reside 
has also developed the Word Linking tool, originally de-
veloped for the Shakespeare Quartos project.
The Ajax XML Encoder (AXE), also developed by Re-
side, allows users with limited technical knowledge to 
add	metadata	to	text,	image,	video,	and	audio	files.		Us-
ers can collaboratively tag a text in TEI, associate XML 
with	 time	stamps	 in	video	or	audio	files,	and	mark	off	
regions of an image to be linked to external metadata. 
At present the web-based image tagger allows users to 
select regions in an image and store the coordinates of 
this region in a database, but it does not provide tools to 
make use of this data once it is stored.  The text tagger 
allows a user to specify a relaxNG schema and then tag 
a text using this schema, but it requires users to enter 
coordinates for image links by hand (it does not, at pres-
ent, interface easily with the image tagger).  The tools 
in AXE were always intended to be interoperable and 
to have the functionality described in this narrative, and 
this current collaboration allows us to move the suite to 
the next stage of its development.
The Tool
TILE will be based primarily on the Ajax XML Encoder 
(AXE).  Through TILE, we will extend the functionality 
of AXE to allow the following:
•	 Semi-automated creation of links between tran-
scriptions and images of the materials from which 
the transcriptions were made.  Using a form of opti-
cal character recognition, our software will recog-
nize words in a page image and link them to a pre-
existing textual transcription.  These links can then 
be checked, and if need be adjusted, by a human.
•	 Annotation of any area of an image selected by the 
user with a controlled vocabulary (for example, the 
tool can be adjusted to allow only the annotations 
“damaged” or “illegible”).
•	 Application of editorial annotations to any area of 
an image.
•	 Support linking for non-horizontal, non-rectangular 
areas of source images.
•	 Creation of links between different, non-contiguous 
areas of primary source images. For example: 
•	 captions and illustrations; 
•	 illustrations and textual descriptions; 
DIGITAL HUMANITIES 2009
Page 390
•	 analogous texts across different manuscripts
We are especially concerned with making our tool avail-
able for integration into many different types of project 
environments, and we will therefore work to make the 
system requirements for TILE as minimal and as generic 
as possible.  
Notes
1Valley of the Shadow: Two Communities in the Ameri-
can Civil War, Virginia Center for Digital History, Uni-
versity of Virginia (http://valley.vcdh.virginia.edu/); The 
Complete Writings and Pictures of Dante Gabriel Rosset-
ti, A Hypermedia Archive, edited by Jerome J. McGann, 
University of Virginia (http://www.rossettiarchive.org/); 
Dickinson Electronic Archives, edited by Martha Nell 
Smith, Online. Institute for Advanced Technology in the 
Humanities (IATH), University of Virginia (http://www.
emilydickinson.org/); The William Blake Archive. Ed. 
Morris Eaves, Robert N. Essick, and Joseph Viscomi. 
(http://www.blakearchive.org/).
2Hugh Cayless, “Experiments in Automated Linking of 
TEI Transcripts to Manuscript Images,” presented at the 
Text	Encoding	Initiative	Member’s	Meeting,	November	
2008. http://www.cch.kcl.ac.uk/cocoon/tei2008/pro-
gramme/abstracts/abstract-166.html
References
Carlquist, J. 2004. “Medieval Manuscripts, Hypertext 
and Reading. Visions of Digital Editions. Literary & 
Linguistic Computing 19.1,105-118. 
Chaucer, G. 1996. Wife of Bath’s Prologue on CDROM. 
Edited by P. Robinson. Cambridge University Press.
Holmes, M. 2007. Image Markup Tool v. 1.7. [http://
www.tapor.uvic.ca/~mholmes/image_markup/] Ac-
cessed 2008-11-13. 
Kiernan, K.. 2005. “Digital Facsimiles in Editing: Some 
Guidelines for Editors of Image-based Scholarly Edi-
tions.” Electronic Textual Editing. Ed. Lou Burnard, , 
Katherine	O’Brien	O’Keeffe	and	John	Unsworth.	New	
York: Modern Language Association, 2005. [preprint at 
http://www.tei-c.org/About/Archive_new/ETE/Preview/
kiernan.xml]
Kiernan, K. 1999. The Electronic Beowulf. University of 
Michigan Press.
Kirschenbaum,	M.	G.	2002.	Editor’s	 Introduction:	 Im-
age-based Humanities Computing. Computers and the 
Humanities 36.1, 3-6. 
Langland, W. 2000. Piers Plowman Electronic Archive, 
Vol. 1. Edited by R. Adams. University of Michigan Press. 
 
TEI Consortium, eds. 2007. “Digital Facsimiles.” Guide-
lines for Electronic Text Encoding and Interchange. [Last 
modified	date:	2008-07-04].	[http://www.tei-c.org/release/
doc/tei-p5- doc/en/html/PH.html] Accessed 2008-07-25. 
 
 
 
 
 
 
 
 
DIGITAL HUMANITIES 2009
Page  391
Authorship Attribution, The 
Large and Small Effect Sizes of 
Divergence	as	Classification
Mike Ryan 
Duquesne University 
michaelryan@acm.org
Patrick Juola
Duquesne University 
juola@mathcs.duq.edu
A common method of performing authorship attri-bution	 (or	 text	 classification	 in	 general)	 involves	
embedding the documents in a high-dimensional fea-
ture space and calculating similarity judgments in the 
form of numeric “distances” between them. Using (for 
example) a k-nearest neighbor algorithm, an unknown 
document can be assigned to the “closest” (in similarity 
or distance) group of reference documents. However, the 
word	“distance”	 is	 ill-defined	and	can	be	 implemented	
conceptually in many different ways. We examine the 
implications of one broad category of “distances”.
This notion of distance can be generalized to dissimilar-
ity judgements without previous embedding in a space. 
An example of this is the Kullback-Leibler Divergence 
which calculates an information-theoretic dissimilarity 
measure, effect size, between two event streams where 
the events are not necessarily independent and thus can-
not be directly tabulated as simple histograms. This kind 
of “distance” can easily be incorporated into a text clas-
sification	system.
To a topologist, a “distance” is a numeric function D(x,y) 
between two points or objects, such that
•	 D(x,y) is always nonnegative, and always positive 
if x != y
•	 D(x,y) = D(y,x)
•	 D(x,y) + D(y,z) >= D(x,z)
However, there are many useful distance-like measures 
(technically known as “divergences”) that do not have 
all these properties. In particular, divergences such as 
the Kullback-Leibler divergence and vocabulary overlap 
are not the same when measured from different basis. 
So, assume that you have two documents A and B and 
want	to	find	the	divergence	between	them.	The	methods	
would	be	to	find	the	divergence	of	B	from	A,	D(A,	B)	or	
to	find	the	divergence	of	A	from	B	D(B,A).	These	two	
ways of applying the same divergence will give you dif-
ferent results.
The two results being different is important because it 
implies that there is different information captured by 
each one. This has been shown to be true for some di-
vergence measures (Juola Ryan, 2008) but upon review-
ing those previous results, it seems there is rarely a case 
where information spread across both divergences i.e. 
the information gained from one is better then an aver-
age of the two. The problem then becomes that we do not 
know which one will contain more information. Because 
of this we have come up with criteria for selecting one 
over the other. The criteria devised are simple; we will 
either use the max of the two methods, or the min of the 
two methods.
To test this, we are in the process of applying several di-
vergence functions to a standardized corpus [the AAAC 
corpus (Juola, 2004)] of authorship attribution problems 
using the JGAAP framework (Juola et al, this confer-
ence). We will compare each run normally, backwards, 
then both taking the max and min.
Preliminary results using the Kullback-Leibler Diver-
gence and the LZW Distance indicate that using the max 
of the two divergences will on some problems increase 
accuracy by up to four-fold. We plan to continue this 
work	using	other	divergences;	 if	 this	finding	continues	
to hold, we consider this to be an important step to elim-
inating some of the “ad-hoc-ness” of the current state 
of authorship attribution, as we will be able to provide 
some steps to analyzing not merely what methods per-
form best, but what extensions of these methods can be 
used to improve their performance.
DIGITAL HUMANITIES 2009
Page 392
ArchInSite: Augmented 
(Reality) Architecture
Eric Sauda 
UNC Charlotte 
ejsauda@uncc.edu
Nick Ault  
UNC Charlotte 
nwault@uncc.edu
Zac Porter 
UNC Charlotte 
ztporter@uncc.edu
Fig. 1 Diagram of ArchInSite operation.
ArchInSite is an augmented reality application that combines three-dimensional modelling, video com-
positing and a global positioning system into a handheld 
device. This system allows a user to move through the 
landscape, while using a mobile device to view virtual 
models accurately placed in space. ArchInSite is unique 
in its combination of a hand held device with GPS and 
video compositing.  This system can be useful for vi-
sualizing architectural designs, sculpture and augmented 
environments directly on real site, while simultaneously 
interacting with and making design decisions within 
the virtual environment. Further, ArchInSite offers the 
possibility of creating a virtual world woven into real 
space, both as a means of displaying architectural in-
formation and exploring design alternatives within the 
existing conditions. At the conceptual level, privileging 
the perspectival view challenges the conventional place 
of orthographic projections as the media of architectural 
design. 
Architectural Background
While the vast majority of the architectural communi-
ty is concerned with scaled, planometric drawings that 
represented	spaces	from	a	God’s	eye	view,	we	are	inter-
ested in exploring a technology that would allow us to 
visualize	buildings	in	a	real-time,	interactive	first-person	
perspective. Our reasoning is simple enough—if one 
experiences	the	world	in	a	first-person	perspective,	why	
not	design	buildings	in	a	first-person	perspective?		It	ap-
pears to us that too often architects are referencing non-
experiential spatial concepts, such as a geometrical rela-
tionship between adjacent rooms that is only evident in 
plan, instead of the direct experiential nature of inhabit-
ing	a	building.		We	consider	first-person	perspectives	of	
a building to be indubitable knowledge for architectural 
experience. Likewise, we consider an overall, spatial 
plan of a building to be comparable to the philosopher 
Edmund	 Husserl’s	 description	 of	 a	 belief.	 	 Therefore,	
a progression through a building, in which one is able 
to	 take	 in	several	first-person	perspectives	 (indubitable	
knowledge), allows one to begin making some basic 
assumptions (beliefs) about the overall spatial plan of 
the building.  So, while the experience of a building 
progresses	from	first-person	perspectives	 to	an	overall,	
spatial plan, the standard practice for designing a build-
ing progresses in reverse, from planometric drawing to 
perspective.   We see this problem as an opportunity to 
create a device that would provide the kinds of real-time, 
perspectival	visualizations	necessary	to	design	in	a	first-
person perspective.            
It	is	our	contention	that	the	analysis	of	these	first-person	
perspectives, not as our own perspectives, but as an ide-
al, archetypal perspective, should be the driving force of 
an architectural design. Like Husserl, we believe that a 
further consideration of things in and of themselves can 
provide a foundation for more conceptual and abstract 
thoughts.   
Background/ Related Work
It is possible to trace the concept of computer generated 
reality	 to	Ivan	Sutherland’s	pioneering	work	with	head	
mounted displays and his idea of a window into a virtual 
world (Sutherland). Much of this early work focused on 
the	creation	of	a	world	separate	from	our	‘normal’	world,	
which became known as virtual reality.  The term ‘aug-
mented	reality’	was	first	introduced	by	two	Boeing	engi-
DIGITAL HUMANITIES 2009
Page  393
neers (Caudell), who created the Wire Bundle Assembly 
Project, which combined a heads-up, see-through, head-
mounted display that registered computer produced di-
agrams superimposed on real world objects using half 
silvered	mirrors	 and	position	 tracking.	Steven	Feiner’s	
work	on	‘A	Touring	Machine’	(Feiner)	combined	bulky	
GPS equipment with optical overlay heads-up display 
to geo-spatially register and display text information 
about campus building interactively. Ongoing work by 
Behzadam and Kamat (Behzadam) at the University of 
Michigan is using video compositing and GPS data to 
feed a head mounted video display for possible use in 
construction.
Research Issues and Implementation
Augmenting the sensible world with additional informa-
tion	 is,	on	 it’s	 face,	 a	 simple	and	 reasonable	goal.	Be-
cause we never just look without intention, the ability to 
enrich the visual context with information makes pos-
sible new forms of understanding and interaction. 
Augmented reality has raised important ideas about 
both hardware and software. The hardware issues have 
largely revolved around issues of display and registra-
tion (by contrast, generating 3d models and placing them 
on still photographic backgrounds is well understood). 
Display choices are largely focused on heads-up display 
technologies, with choices between see-through displays 
and video compositing. Registration of the model image 
with	the	real	world	is	a	difficult	and	on-going	problem,	
requiring specialized equipment (head tracking acceler-
ometers) and limited environments (position tracking). 
While these efforts continue to show promise, their cur-
rent state is still largely fragile and more suitable to re-
search labs than to wide spread use.
We have developed a prototype for a system that incor-
porates three-dimensional modelling, GPS tracking, an 
internal camera and video compositing to produce a de-
vice that creates a hybrid condition between the virtual 
and real worlds. The current implementation consists of 
a Sony UX micro pc running Maya modelling software, 
a GPS data parser and video capture software. The sys-
tem utilizes a MEL script that enables the built in camera 
and the data streamed from the GPS to reorient the mod-
el in virtual space based upon the position of the user. A 
system such as this is feasible for virtually all camera/
GPS enabled phones (currently estimated at 400 million 
units/year worldwide); this makes their use potentially 
ubiquitous.
Results
We have successfully tested the use of ArchInSite for 
viewing of proposed architectural models in the land-
scape. The system is readily understood and embraced 
by users, and while the registration problems endemic 
with head mounted systems do not completely disap-
pear, separating the view port from direct attachment 
to users (by substituting hand held display for heads-up 
display)	provides	much	more	flexibility	for	the	users	to	
comprehend the registration of the virtual and the real. 
We began with preselected models that were placed at 
specific	 points	 in	 the	 landscape;	we	 then	 implemented	
(using	Maya’s	default	modelling	tool	set)	the	ability	to	
make changes to the model while concurrently viewing 
virtual architecture integrated into its site.
Fig. 3 Screenshots showing change of user position 
and corresponding change in model and composited 
background.
We	have	three	major	findings.
1. The system that we have implemented utilizes eas-
Fig. 2 ArchInSite user changing position on an outdoor site.
DIGITAL HUMANITIES 2009
Page 394
ily obtainable handheld devices rather than heads-
up	displays,	and	side	steps	the	difficulty	of	solving	
registration problems. Our users have found this 
handheld device approach to be a convincing win-
dow that they can connect directly and viscerally to 
the context. 
2. We believe that ArchInSite can prove valuable as a 
design tool for architects and designers. The ability 
to see work with a model on site is useful because it 
gives you a richer and more fulsome understanding 
of the context than is possible when pursued through 
a distance by sketches, renderings and/or drawings. 
Coupling this visual understanding with the ability 
to alter the computerized model allows the designer 
to sensitively recalibrate and redesign the model in 
the actual setting.
3. We	think	that	significant	opportunity	exists	to	con-
strue a virtual world that is threaded inside the real 
world and supplements it in interesting ways.  Ar-
chitecture was often used as a classical rhetorical 
device for memorizing facts and telling stories and 
we see the potential for its re-emergence threaded 
through the real world.
Further Work
We are currently developing a new modelling and com-
positing implementation that will use custom software 
rather than off-the-shelf applications that often have high 
levels of system overhead. Our further goal is to move 
this application, or a variation of it, over to a more com-
pact and ubiquitous device, such as the iPhone, a device 
that currently contains the hardware necessary to drive 
the software.
We will also be teaching an architectural studio course 
that will use ArchInSite as the primary means of design, 
seeking to challenge the hegemony of the orthographic.
References
Behzadan A.H., and Kamat V.R. (2005), Visualization 
of Construction Graphics in Outdoor Augmented 
Reality, Proceedings of the 2005 Winter Simulation 
Conference, Orlando, FL 
Caudell, Thomas P., and Mizell, David W. (1992). Aug-
mented Reality: An Application of heads-Up Display 
Technology to Manual Manufacturing Processes, 1968 
Fall Joint Computer Conference, AFIPS Conference 
Proceeding 33, 757-764 (1992).
S. Feiner et al., “A Touring Machine: Prototyping 3D 
Mobile Augmented Reality Systems for Exploring the 
Urban Environment,” Proc. 1st Int”l Symp. Wear-
able Computers  (ISWC “97), IEEE CS Press, Los 
Alamitos, Calif., 1997, pp. 74-81.
Sutherland, Ivan E. (1968). A Head Mounted Three Di-
mensional Display, 1968 Fall Joint Computer Confer-
ence, AFIPS Conference Proceeding 33, 757-764 (1968).
DIGITAL HUMANITIES 2009
Page  395
An Image-Based Document 
Reader with Editing Functions 
for Education and Research on 
Digital Humanities.
 
Hiroyuki Sekiguchi 
Ritsumeikan University 
h-seki@fc.ritsumei.ac.jp
This paper focuses on an electronic document reader we have developed, a new method of lecture using 
this reader, and some possible examples of its utility for 
research on digital humanities. 
Introduction 
Recently, more and more documents have been stored 
in digital archives. Electronic documents have many ad-
vantages, e.g., high performance when searching text, 
instantaneous document transmission, and negating the 
need for archive space of the corresponding physical 
documents. Because of this, we can read thousands of 
digitized books anywhere with our notebook PCs. 
Despite	these	benefits,	however,	it	is	often	said	that	most	
people	find	it	difficult	to	remember	the	contents	of	digi-
tal documents when reading them on a computer display, 
compared with reading the physical paper document. We 
can propose several reasons for this. Firstly, any physical 
movements which indirectly assist our recollection, such 
as eye-movement or page turning, are not experienced 
when reading digital documents. Secondly, making 
notes or memos or marking on the electronic document 
is usually not possible, and even if it is, we cannot do it 
as easily as on paper. 
Some reports say that taking memos or highlighting 
parts of documents improves our understanding and re-
membering. Since such activity requires proactive deci-
sions on selecting contexts, these actions activate our 
brain much more than silent reading. Of course, these 
markings and memos help us recollect our thoughts dur-
ing a later reading of the document. 
To overcome these shortcomings, we have developed a 
document reader that enables us to freely mark digitized 
documents. The reader we have developed has several 
marking functions, such as, highlighting areas of text, in-
serting bookmark stamps, and adding memos to the text. 
One of the most unique features of our reader is to real-
ize these mark-up functions with usability almost equal 
to that of paper media. This point is important because 
such an easy-to-use text reader has not prevailed yet. 
Our	reader	stores	mark-up	data	in	a	file	separate	to	the	
original document. As the mark-up data is written in a 
simple text-numeric format, it can be easily used by a lot 
of software applications, such as spreadsheets, statistics 
analysis or image processing packages. 
In this paper, we will start with an explanation of our 
reader’s	 functions	 and	 then	 introduce	 two	 applications	
using it: a new method of lecturing using the reader, and 
examples of using mark-up data for the study of digital 
humanities. 
Functions Of Our Reader 
We now introduce editing functions equipped in our 
document-reader. An example text containing mark-up 
highlights, stamps, and memos written on the text is 
shown in Figure 1. 
Figure 1: A sample document with markings 
Highlights: 
We can highlight text just by click-and-dragging the 
mouse along the text. Any other operations like open-
ing menus or clicking icons are not required. Dragging 
from left-to-right highlights the text in red, while, drag-
ging from right-to-left highlights in blue. You may think 
that	highlighting	in	blue	is	a	little	bit	difficult	due	to	the	
right-to-left movement, but in exchange for this incon-
venience, you do not have to choose a separate color to 
use. That is, blue is naturally used for short key words or 
important contexts. 
Stamps: 
If you push the space key, a stamp appears on the text. 
The stamp has two faces: “?” and “!”. The intended pur-
pose of the stamp mark “?” is to mark hard-to-understand 
concepts, or points that need to be followed up later. The 
“!” mark is used for “interesting” or essential points of 
concepts. 
Stamps also act as bookmarks. If you push the “<” or 
“>” key, the reader jumps to the previous or next stamp, 
respectively. If there is no following stamp in the current 
document, the reader jumps to the next bookmark in the 
DIGITAL HUMANITIES 2009
Page 396
following document. 
Text memos: 
You can add notes or memos anywhere in the document. 
Push the “Tab” key to enter text-input mode. Characters 
you type on your keyboard will appear at the position 
of the mouse cursor. To exit input mode, push the “Tab” 
key again. Unlike the other editing applications, our 
reader does not show any text-input-box which spoils 
the look and feel of writing on the paper. 
User’s	mark-up	data	file	format:	 	
A	user’s	mark-up	data	is	stored	in	an	associated	file.	An	
example	of	a	user’s	mark-up	data	is	shown	in	Figure	2.	
You can see that it is quite a simple form of text-numeric 
data. The number in the top line indicates the position 
of the cursor when closing the document on the previ-
ous reading. Each of the following lines describes the 
marker type and the coordinates of the surrounding rect-
angle.	In	the	case	of	a	text-memo,	the	user’s	text	is	added	
as a character string after the coordinate values. These 
data are used not only for displaying mark-ups, but for 
other various purposes, such as taking mark-up statistics, 
extracting highlighted-regions from the document, and 
comparing comments written by different reviewers. We 
will now show some examples below. 
Figure 2: Mark-up data of Figure 1 
Use During Lectures 
Thanks to the improvement of our computer literacy and 
network technology, computer-assisted education, (so 
called e-learning), has become more and more popular. 
Our university has several computer rooms, but they are 
mainly used for learning computer software or program-
ming. So, we have been developing a new style of lectur-
ing using our reader to make the most use of computer 
rooms for the classes of humanities. The arrangement of 
a typical class is as follows. 
First, the students read the lecture notes for the next class 
with our reader, highlighting or putting stamps on phras-
es they cannot understand clearly. 
Next, the teacher receives student mark-up data via the 
network prior to the lecture. The lecturer checks the stu-
dent reports with a summarized view (shown in Figure 
3), and then, he examines which areas of the lecture stu-
dents are having a hard time understanding. As a result, 
the teacher can modify his lecture so that it is more suit-
able and understandable for the students. 
There	 are	 several	 other	 benefits	 for	 the	 students,	 too;	
1) They can understand lessons more deeply by paying 
attention to areas they could not clearly understand. 2) 
Their sense of participation is promoted because their 
mark-up results directly affect the lecture content or ex-
planation. 
These	major	benefits	are	achieved	by	relatively	simple	
editing actions by the students. According to a class 
evaluation taken at the end of the semester, most of our 
students approved of this style of lecture. By investigat-
ing the mark-up data gathered from them, we can ex-
tract	 difficult-to-understand	 keywords	 and	 sections	 of	
the	lecture.	In	the	near	future,	it	may	be	possible	to	find	
relations between their marking style and their learning 
results. 
Figure 3: Summarized views of student marks. 
Using For The Study Of Digital Humanities 
A particular feature that paper media have is that we can 
easily write on them, however, the reality is that, it is 
usually prohibited to write on research materials. By us-
ing our reader, we are completely free from such restric-
tions.	This	is	one	of	the	benefits	of	our	writable	reader	
used as a researching tool. 
In addition, by displaying marks or comments written 
by more than one researcher on the same document, we 
can clearly recognize which points should be focused on, 
and what the differences are between their ideas. This 
function is also useful for referee reading or proofread-
ing performed by several reviewers. 
Finally, since each piece of mark-up data indicates its po-
sition in the material, it is easy to extract marked regions 
from the original material. After gathering these regions, 
they can be shown as listings or KWIC (keyword in con-
text) forms. We can also use the marked region as a tem-
plate for searching over the document. In this way, this 
reader can be used as a front-end to researching tools. 
Conclusions 
In	this	paper,	we	have	confirmed	the	utility	values	of	our	
digital document reader for understanding computerized 
documents and its usefulness in education. As mentioned 
DIGITAL HUMANITIES 2009
Page  397
above, our reader can be used in several ways for the 
research of digital humanities. Especially, by handling 
materials such as image documents, our reader is suit-
able for handwritten documents and those with a large 
number	of	figures.	
You can download our text reader from our website. We 
would like you to try it and give us feedback. Please feel 
free to stop by our poster session, as we are going to 
demonstrate it and introduce new methods for research 
on digital humanities. 
URL: http://www.img.is.ritsumei.ac.jp/~h-seki/beeR-
eader/ 
New Digital Tools at the William 
Blake Archive
William Shaw 
UNC-Chapel Hill 
wsshaw@email.unc.edu	
In this talk, I plan to discuss and demonstrate my re-cent work at the William Blake Archive—namely, my 
development of a virtual lightbox application.  I will dis-
cuss this work in the context of our editorial rationale, 
software roadmap (including our user interface redesign, 
currently scheduled for completion in early 2009), and 
our past approaches to digital tool development.
The user interface of the Blake Archive currently relies 
on Java-based tools developed at the Institute for Ad-
vanced Technology in the Humanities (IATH), Univer-
sity of Virginia, in the mid-1990s.  These applets, Ima-
geSizer	and	Inote,	allow	users	to	view	some	of	Blake’s	
work	at	true	size	and	to	read	the	Archive	editors’	image	
descriptions.  They have proven useful and durable, but 
we have long sought to incorporate their functionality 
into a more powerful tool: a virtual lightbox application 
that provides users with image manipulation capabili-
ties, features for object annotation, and the ability to col-
lect	and	compare	Blake’s	work	across	genres,	media,	and	
time periods.
As technical editor of the Blake Archive, I have spent 
the past several months developing this software, and the 
Archive has successfully deployed it on our testing site. 
My poster will discuss the ways in which this software 
greatly expands the functionality of the Blake Archive 
in particular, and, in general, the ways in which it can 
be adapted to any digital project concerned with imag-
ing in various contexts, with a special emphasis on art-
historical and manuscript studies.
In terms of the Blake Archive, The primary goal of the 
lightbox is to give serious Blake scholars an indispen-
sible tool for comparative analysis.  By allowing users 
to collect different plates from different copies of an il-
luminated book (for example) and view them all at true 
size, the lightbox provides a fundamental tool of art-
historical analysis that has hitherto been absent from the 
Blake Archive (as well as from similar projects, such as 
the Rossetti Archive).  In addition, its annotation features 
allow users to explore image descriptions and editorial 
commentary; they also enable users to search currently 
loaded images by keyword, motif, or other markup char-
acteristics. 
DIGITAL HUMANITIES 2009
Page 398
Furthermore, its backward-compatibility with SGML 
Inote annotations permits a smooth transition between 
previous annotation/description schemes and the new, 
XML-enabled Lightbox environment.
In more general terms, I will discuss the architecture of 
the Blake Archive lightbox, emphasizing its open-source 
(MIT/X11) licensing and simple API.  Its design, which 
is	 intended	 to	be	both	 straightforward	 and	flexible,	 al-
lows it to be incorporated into any web-based digital 
project with ease.  Its simple, XML-based image annota-
tion format is adaptable to a wide range of image markup 
needs.  Finally, its installation procedure—which relies 
on simple JavaScript functions—will ensure that it is 
easy to integrate into other projects.
In addition to discussing the features and development 
of	the	lightbox,	I	will	explain	how	it	fits	into	the	editorial	
rationale of the Blake Archive.  As editors Morris Eaves, 
Robert N. Essick, and Joseph Viscomi have pointed out, 
the priority that we grant to the media, methods, and 
histories of artistic production has dictated a feature of 
the	Archive	 that	 influences	virtually	every	aspect	of	 it.	
It is utterly fundamental: we emphasize the physical ob-
ject—the plate, page, or canvas—over the logical textual 
unit—the poem or other work abstracted from its physi-
cal medium. This emphasis coincides with our archival 
as well as with our editorial objectives.  (Eaves, Essick, 
and Viscomi, “Principles”).
This art-historical emphasis on the physical object, rath-
er than exclusively on the literary text, is continued in 
the lightbox.  It not only allows users to focus on the 
physical objects themselves, rather than editorial appa-
ratus, but also complements both our editorial principles 
of diplomatic transcription and our archival principles of 
size	fidelity	and	comparative	analysis.
In conclusion, I plan to discuss my development of the 
lightbox	application;	to	explain	how	the	application	fits	
into both the Blake Archive site redesign and, in theory, 
answers the needs of other projects; and to argue that, as 
a tool, it is both an important part of our user experience 
and a manifestation of the editorial principles that have 
guided the entire history of the William Blake Archive.
Works Cited
Eaves, Morris, Robert Essick, and Joseph Viscomi. 
“Editorial Principles: Methodology and Standards in the 
Blake Archive.” The William Blake Archive.  April 15, 
2005 <http://www.blakearchive.org/blake/public/about/
principles/index.html>
-----. “Technical Summary of the William Blake Ar-
chive.” The William Blake Archive. 16 June 2008 <http://
www.blakearchive.org/blake/public/about/tech/index.
html>
DIGITAL HUMANITIES 2009
Page  399
TADA Research Evaluation 
Exchange: Winning 2008 
Submissions
Stéfan Sinclair 
McMaster University 
sgsinclair@gmail.com	
Winners:  Dave Beavan, Susan Brown, J. 
Stephen Downie, Carlos Fiorentino, Patrick 
Juola, Shelly Lukon, Peter Organisciak, 
Geoffrey Rockwell, Susan Schreibman, 
Kirsten Uszkalo
 
In	 the	 spring	 of	 2008	 the	Text	Analysis	Developers’	Alliance organized a digital humanities tools compe-
tition called T-REX, modelled on similar competitions 
such as MIREX (music information retrieval) and TREC 
(text retrieval competition), (cf. Downie 2006). The 
community response to T-REX was very positive and 
among the many submissions received, judges selected 
winners from the following categories: 
•	 Best New Web-based Tool
•	 Best New Idea for a Web-based Tool
•	 Best New Idea for Improving a Current Web-Based 
Tool
•	 Best New Idea for Improving the Interface of the 
TAPoR Portal
•	 Best Experiment of Text Analysis Using High Per-
formance Computing
The categories above deliberately cover not only work-
ing tools, but also ideas, designs and preliminary experi-
ments; a primary objective of T-REX is to encourage 
the involvement and collaboration of programmers, de-
signers, and users. More information on the categories, 
the judges, and other aspects of T-REX are available at 
http://tada.mcmaster.ca/trex/. 
The organizers and winning participants of T-REX would 
like to propose a cluster of posters that showcase vari-
ous aspects of the research done. In particular, we will 
prepare seven “half” posters presenting relevant aspects 
from each of the winning T-REX submissions. In addi-
tion, an eighth “half poster” will provide an overview of 
the inaugural TREX competition, lessons learned, and 
new initiatives for the second round. Below are brief de-
scriptions of each of the seven project posters. 
Susan Brown et al.,  
Degrees of Connection Tool (New Tool) 
This linkage tracing tool allows users working on a large 
collection of documents to explore the linkages within 
the collection based on the semantic tags it contains. Our 
prototype based on the Orlando textbase traces links be-
tween people mentioned in different XML documents 
based on co-occurrences of a small set of key tags that 
occur across many documents: personal names, organi-
zation names, places, and titles. This exploits the tagging 
to get at connections between people that may not be 
made by direct linkages between documents, but rather 
by the co-occurrence of tags within two documents, or 
a pathway from document x to document y by way of 
document z in which different tags common to x and y 
occur in z. It is a way of getting at implicit but neverthe-
less potentially important linkages, and while it emerges 
in this case from an interest in literary history, the tool 
could	be	useful	to	other	fields	ranging	from	journalism	
to creative writing, sociology or psychology. It provides 
a new way of exploring the large digital collections re-
searchers are increasingly using. The poster will 1) out-
line the principles on which the prototype is based, 2) list 
key features for a fully-developed, generalized version, 
3) explain our application of graph theory to the tagged 
text, and 4) outline the design challenges that emerged in 
the development of the prototype. A live demo will allow 
attendees to test the prototype. 
David Beavan,  
Collocate Cloud (Idea for Existing Tool 
Improvement) 
Clouds of information e.g. keywords, tags or words, are 
a very useful way to aggregate and present vast quanti-
ties of data. These clouds have gone on to be used in 
many web 2.0 sites. As such they are becoming a well 
known and understood visualisation by many users. 
TAPoRware currently provides a Word Cloud visualisa-
tion, which shows the frequency of words in a document. 
Scholars often wish to go further, to see how a particular 
word is used, by examining which words co-occur near 
their search word. TAPoRware already has this Colloca-
tion tool, showing the results in tabular form. 
The Collocate Cloud would merge the collocation output 
and the cloud visualisation technology. It will show the 
collocates of a particular search word in cloud format. 
The alphabetical ordering of the Collocate Cloud would 
allow	the	user	 to	find	or	discount	a	word	quickly.	Fre-
DIGITAL HUMANITIES 2009
Page 400
quencies and collocational strength are shown by size 
and brightness, letting these terms stand out visually. 
Carlos Fiorentino,  
The Magic Circle (Idea for New Tool) 
The Magic Circle is an information glyph that allows 
scholars to visually summarize combinations of the lexi-
cal information included in text collections (typically fre-
quency data about words, lemmas, and parts of speech) 
and the bibliographic information attached to these texts. 
The glyph consists of a set of rings organized outwards 
from the centre and divided in wedges or sections. The 
lexical data determine the size of the centre, which also 
shows a word, a lemma, or a part of speech, and the total 
number of search results for that word, lemma, or part 
of	speech	within	a	specified	work	or	set	of	works.	The	
bibliographic	data	is	related	to	authors	specified	by	the	
user, and the rings allow the user to analyze how the 
search results are distributed in different collections as 
well as in different periods of time. The color sets of the 
rings follow patterns of associations with variations in 
hue, tone and saturation. A comparative scale helps the 
user to understand the volume of information found in 
context with the whole volume of information present in 
the collections. 
Alejandro Giacometti et al.,  
Ripper Browser (New Tool) 
The Ripper Browser is a prototype for rich-prospect 
browsing of text collections. Rich-prospect browsing 
interfaces are designed to aid research tasks such as ex-
ploration and synthesis by providing both a meaningful 
representation of each item in a collection and tools to 
manipulate their visual organization (Ruecker 2003). The 
Ripper browser offers an environment for exploration 
and interaction with digital text documents. The system 
creates tiles that contain faceted information about each 
document. The tiles can be manipulated with a series of 
controls to reveal or hide details, organize them accord-
ing	to	a	particular	hierarchy,	or	select	a	specific	group.	
By adapting the size of the tiles, the Ripper browser al-
lows researchers to visualize the complete collection and 
the precise information they need about each document 
in view at all times. The Ripper browser was developed 
in web-native technologies: HTML, JavaScript, and uses 
the	jQuery	library.	It	is	configured	to	use	text	collections	
provided by the MONK Project. The Ripper browser is 
part of an ongoing effort to understand the potential of 
rich-prospect browsing and improve on our strategies for 
designing rich-prospect tools. It has allowed us to exper-
iment further with meaningful representation, increased 
our understanding of the importance of sequences, and 
provided insight into new possibilities for organization 
in visualizations. 
Patrick Juola & Shelly Lukon,  
Back-of-the-Book Index Generation 
(Experiment in HPC) 
This is actually a work-in-progress; as we have detailed 
elsewhere (Juola, 2005, ACH/ALLC; Lukon and Juola, 
2006, DH2006), we are working on a program to apply 
standard ML techniques, including latent semantic anal-
ysis, to the problem of back-of the-book index genera-
tion.  LSA implicitly uses huge document-by-term matri-
ces to determine which words appear in similar contexts 
and are therefore good candidates for grouping under a 
single index term. 
The	sheer	size	of	this	matrix	makes	it	difficult	to	work	
without HPC; one of the tools we are using is the 200+ 
node Beowulf cluster available at Duquesne University 
Computer Science Department. We analyze the docu-
ment to be indexed (which can in theory be arbitrarily 
large but in practice will be about novel-sized) to select 
candidate words (mostly nouns, via POS tagging) for in-
dexing, then use LSA to identify potential relationships 
among those words. 
Peter Organisciak,  
Bookmarklet for Immediate Text Analysis 
(Improving TAPoR) 
This idea is of an online interface for the generation of 
TAPoR bookmarklets on demand. 
Bookmarklets are browser bookmarks that run javascript 
code. They provide value to text analysis tools in two 
way: ease and ubiquity. They allow one-click connection 
of content to tool and, more importantly, allow it to be 
run on whatever content the user is at. 
One problem of bookmarklets is that they are static, 
which means that customization of the query is limited. 
One solution would be to call up an interface every time 
the bookmarklet is called. Doing so, however, is an im-
pediment to the core concept of ease. Rather, through an 
interface for creating customized bookmarklets, a user 
can create single-purpose bookmarklet buttons that do 
the same command every time, immediately and directly. 
Kirsten C. Uszkalo,  
Throwing Bones (Idea for New Tool) 
The Throwing Bones interface operates as a means to 
discover meaningful relationships within a corpus of 
texts. These relationships will appear as a series of piles, 
which	the	user	can	zoom	into	and	out	of,	shuffle	through,	
DIGITAL HUMANITIES 2009
Page  401
and examine closer for more comprehensive, annotated 
information. For example, in the case of a corpus of early 
modern witchcraft trials, a user might want to see the re-
lationship between animal familiars and accusers. After 
shuffling,	the	top	item	in	a	pile	would	show	the	number	
of familiars, while the cards beneath show the number of 
accusers, illustrating a connection between the imagina-
tion of accusers and the presence of familiars in trials 
and texts.  The piles could also be based on geographic, 
temporal, textual, or relationship proximity. The concept 
behind Throwing Bones is that the interface will not only 
offer the pleasure of play, but also erudite and serendipi-
tous textual analysis. 
Bringing Southern Oral Stories 
Online
Natasha Smith 
University of North Carolina, Chapel Hill
nsmith@email.unc.edu
Joshua Berkov 
School of Communication Arts
jberkov@gmail.com
Cliff Dyer 
University of North Carolina, Chapel Hill 
jcd@unc.edu
Hugh Cayless
New York University 
hugh.cayless@nyu.edu
In recent years, oral histories have become an alterna-tive medium for interrogating the past and have as-
sumed a prominent place in historical inquiry. They 
offer unique perspectives from individuals who have 
witnessed history in the making and often yield unpar-
alleled insight into the lives and times that they record. 
Long constrained by the media used to record them, 
oral histories are increasingly the target of digitization 
initiatives that seek to preserve these voices and make 
them heard by disparate audiences. “Oral histories of the 
American South” (http://docsouth.unc.edu/sohp/) is just 
such an initiative.
From its beginnings as a pilot project in 2004, this en-
deavor, funded by the Institute of Museum and Li-
brary Services, grew rapidly and attracted attention. 
Documenting the American South, a digital publishing 
program at the Carolina Digital Library and Archives 
(CDLA), worked in close cooperation with a number 
of departments at the University of North Carolina at 
Chapel Hill – the UNC Library, Southern Oral Histories 
program (SOHP), and School of Education – and applied 
new technologies, open standards and some of the tested 
practices highlighted in other DocSouth collections. Far 
from being simply a collection of digitized documents, 
these oral histories undergo rigorous analysis by subject 
specialists. The practice of applying such scholarship 
has added considerable value to other recently published 
collections and has brought together the perspectives of 
historians	and	the	first-hand	experiences	of	witnesses	to	
history. Finally, it is an experimental project to build an 
DIGITAL HUMANITIES 2009
Page 402
interface to simultaneously display audio and transcripts 
from interviews.
The project will be complete by the time of the confer-
ence in June 2009 and we are proposing to present a 
poster on the work we have done, highlighting the chal-
lenges and joy experienced by the Project team of librar-
ians, humanists, technologists, and – not to forget – users 
who participated in several usability testings and studies.
The process of creating digital documents of these oral 
histories is indeed a challenge. From the beginning—the 
process of selection is a daunting prospect unto itself. It 
is the task of historians from the SOHP to select 500+ 
representative interviews from a collection that now 
numbers well over 4,000. This includes careful attention 
to privacy and copyright issues. Only interviews free of 
restriction are considered for inclusion in the project col-
lection.
Cassette tapes and typescripts are the raw materials from 
which these interviews are remade into digital objects. 
Audio engineers at the Southern Folklife Collection, 
using the best available hardware, software, and their 
own	considerable	 expertise,	produce	digital	 audio	files	
in both WAV (preservation) and MP3 (access) formats. 
These audio data are of the highest possible quality, and 
comply with international standards and best practices 
for the creation and preservation of digital audio content. 
The typescripts are all encoded in TEI P4 (with the plan 
of conversion to P5), conforming to level 4. But the story 
doesn’t	end	here…
Once created, these newly digitized interviews are sub-
jected to intense historical analysis by subject specialists 
in the Southern Oral History Program (SOHP). With the 
guidance of scholarly advisors, specialists, mainly PhD 
History students, read through the transcript, write ab-
stracts, create descriptive titles, and select particularly 
powerful segments. Their decisions are based on a num-
ber of criteria, from intrigue to major historical relevance 
and from uniqueness to conformation of commonality. 
Once these segments are chosen, the PhD students then 
assign keywords (category/subcategory combinations) 
to each of these segments, and a given segment will usu-
ally have multiple keywords. The keywords are then giv-
en a rank, depending on their relevance to the segment. 
A given segment might have 3 keywords, all of varying 
degree if importance or relevance to the segment itself. 
Assigning these keywords and then prioritizing them 
is what makes our soon-to-be released new advanced 
search so effective. A user can type in a keyword and im-
mediately retrieve the interviews that are most relevant 
due to our efforts to assign and then prioritize these key-
words. Finally, PhD students write short descriptions for 
the selected segments and provide the historical context 
to the interviews.
Once complete, these interviews return to DocSouth, 
where they are prepared for publication. DocSouth staff 
collect various existing metadata from a number of 
sources to create rich records for each interview, enhanc-
ing the XML transcripts and adding the interviews to the 
MySQL database. These metadata are subsequently used 
by library catalogers to generate MARC records to fur-
ther enhance retrieval. A trained specialist listens to the 
interviews	and	inserts	timestamps	into	the	files	based	on	
the selections made by the SOHP. These text timestamps 
become points of entry into the audio—clicking on them 
plays back the audio for that segment.
The	interviews	are	displayed	as	flat	HTML	files,	gener-
ated	in	advance	from	the	TEI	source	files	using	XSLT.	
We have implemented a cutting-edge advanced search 
interface that is constructed with Python/Django forms 
and templates. The form framework provides hooks for 
robust input validation, while the templates separate the 
content from the display for us, so non-tech-savvy de-
signers can help craft an elegant search interface. The 
search index is built on Solr, a lucene-based search en-
gine	which	allows	for	fast,	flexible	searching	on	full-text	
or	fielded	queries.	Database	access	is	in	many	places	fa-
cilitated by a robust Object Relational Mapper written 
in Python called SQLAlchemy. This allows queries to 
the DB to occur seamlessly, without having to rely on 
cumbersome, fragile SQL queries.
The success of bringing these valuable documents out 
of archives and into the eyes and ears of the public 
come from the efforts of its many participants, includ-
ing librarians, historians, and education specialists. The 
project team members and the various institutions and 
interests they represent work together to produce digi-
tized oral history interviews that are the result of the 
application of technology solutions, adherence to stan-
dards, scholarship, and exceptional technical expertise. 
All of these efforts bring additional value to oral history 
interviews that are moving and insightful stories of an 
American South in the process of profound and irrevo-
cable transformation.
References
http://www.tei-c.org/wiki/index.php/TEI_in_Libraries:_
Guidelines_for_Best_Practices
http://www.djangoproject.com/
http://www.sqlalchemy.org/
DIGITAL HUMANITIES 2009
Page  403
http://lucene.apache.org/solr/ A Historical GIS Analysis of 
the Landscape Compositions: A 
Case Study of Folding Screens 
“Rakuchu-Rakugai-zu”
Akihiro Tsukamoto   
Ritsumeikan University 
atv28073@fc.ritsumei.ac.jp
Introduction
Recently, the use of Geographic Information Systems (GIS) to analyze historical space has attracted inter-
ests; this approach is known as Historical GIS (HGIS) 
a subdiscipline in Digital Humanities (William and 
Thomas, 2004). In this paper, I propose a new methodol-
ogy to analyze compositions of landscape in paintings 
with GIS, which I apply for analyzing Japanese screen 
paintings known as rakuchu-rakugai-zu (洛中洛外図), 
created between the 16th and 18th centuries. Rakuchu-
rakugai-zu	 provides	 contemporary	 bird’s-eye	 views	 of	
the	 city	 and	 environs	 of	 Kyoto,	 Japan’s	 capital	 at	 the	
time. The paintings capture man-made structures such 
as residences and palaces of prominent samurai and 
court nobles, temples and shrines; natural features such 
as hills and rivers; and festivals and other human activi-
ties. Nearly all of these works consist of a pair of folding 
screens of six panels each. At present, over 100 such sets 
are known to have survived. 
Because the scenes of Kyoto painted on these screens 
provide historical information not found in written 
texts, they have attracted interests of scholars in various 
fields.	Particularly	noteworthy	among	these	studies	are	
the following three results of detailed analyses of space 
on rakuchu-rakugai-zu screens. Takeda (1966) clas-
sifies	 these	 screens	 into	 three	 types,	 according	 to	 their	
era	of	creation	and	drawing	method.	 (i)	The	‘standard’	
type, commonly known as the ‘first-generation,’	depicts	
scenes from the late 16th century, mainly views of ur-
ban districts of both Shimogyo, i.e., the southern half of 
the town,  and Kamigyo, i.e., the northern half of the 
town, with the surrounding hills as a backdrop. (ii) The 
‘variant’	type	offers	close-up	depictions	of	specific	sub-
jects	or	districts.	 (iii)	The	‘developed’	 type,	commonly	
known as the ‘second-generation,’	depicts	scenes	from	
the 17th century, particularly of Nijo Castle and down-
town Kyoto, with hills in the background. Takeda argues 
that	the	changes	in	drawing	method	reflect	a	shift	from	
seasonal nature paintings employing elements of the ‘fa-
mous	views’	painting	tradition	to	genre	paintings	reflect-
ing trends of the times.
DIGITAL HUMANITIES 2009
Page 404
The knowledge obtained from the previous research 
concerning changes in the subject matter and geographic 
range of rakuchu-rakugai-zu is essential to our under-
standing of the screen compositions. However, such pre-
vious research has not shed light on how actual views 
of Kyoto were transferred to the restricted dimensions 
of the screen surface, nor on how real geographic data 
were manipulated in the process of making that transfer. 
This is because the past analyses have relied solely on 
the simple mapping of landmarks. To analyze the com-
positional methods employed to paint these scenes, not 
enough is simply mapping landmarks; we must also de-
termine which landmarks and districts were distorted, 
and how. In this paper, I propose a new approach, HGIS, 
which offers an analytic methodology utilizing GIS to 
measure distortions in the landscape depicted on the 
screen. This methodology visualizes distortions in the 
drawn space by linking the positions of landmarks as 
they appear on the screen painting and on survey maps, 
and	transforming	the	configuration	of	the	screen	accord-
ingly. Combining the results obtained by this method 
with those of previous research should provide us with 
more detailed and precise understanding of the drawn 
space on these screens.
Methodology
The methodology I introduce here uses GIS spatial anal-
ysis functions to scan the screen surface onto a survey 
coordinate grid based on the relative positions of land-
marks in the screen painting. The analytical procedures 
go as follows (Fig. 1): 
1. Derive coordinate values for landmarks, both on the 
screen and on a survey coordinate grid;
2. Generate a link table from these two point-data sets;
3. Use projective transformation and rubber sheeting 
techniques to project the screen surface onto the sur-
vey coordinate grid; and
4. Project the areas of the rubber sheet-derived poly-
gons onto the screen. 
This process gives visual representation to differences 
between actual space and the space drawn on the screen. 
Results show that screens painted in the 17th century and 
later distorted actual space more than screens painted in 
the 16th century, indicating a decrease in adherence to 
perspective-like conventions. This trend toward greater 
distortion suggests a shift in landscape drawing meth-
ods away from realism. Increasing deformation may be 
attributed to changes in political regime as well as an 
expansion	 of	 the	 public’s	 geographic	 awareness.	 I	 see	
significant	 advantages	 of	 using	 GIS	 in	 understanding	
Drawn Area and Regularity in Drawing, which I will ex-
plain in detail here.
Drawn Area
Using GIS visualizes a drawn area exactly, which means 
we could know which landmarks are included or not 
in the painting. Depending upon the area included, we 
could know how much of forced inclusion, meaning 
spatial	distortion,	occurs.	In	the	first-generation	screens,	
for example, the projective transformation-derived poly-
gons and rubber sheeting-derived polygons are similar in 
shape. The second-generation screens, however, reveal a 
pronounced tendency to forcibly incorporate landmarks 
to the north and south of the city, even though this results 
in various expedient changes in the drawn area of the left 
screen and a progressive narrowing of the background 
scenery in the right screen. Finally, the variant-type 
screens may be explainable as the ones with their focus 
on	 specific	 districts,	 for	which	 the	 forced	 inclusion	 of	
landmarks as employed in the second-generation screens 
proved	 insufficient.	 This	 proves	 distortion	 progresses	
over time.
Regularity in Drawing
Using GIS makes it possible for me to analyze regularity 
in drawing, i.e., the locations of spatial abbreviations and 
exaggerations on the screens. First-generation screens 
exhibit orderly increases in area value, suggesting that 
specific	conventions	similar	 to	 the	 rules	of	perspective	
were followed to achieve the precise geographic posi-
tioning of landmarks. The result is a realistic rendering 
of geographic space. In contrast, second-generation and 
variant-type screens exhibit numerous instances of dis-
torted space, and area value increases variously from 
locale to locale. This suggests that valued less was the 
actual positional relationships of landmarks in the draw-
ing of these scenes. This means that GIS clearly shows 
that in terms of their rendering of real-space positional 
relationships	the	first-generation	screens	are	superior	to	
these later ones.
Conclusions
Until now, there has been little research applying GIS 
to analyze landscape drawing methods. This case study 
shows, however, such an analysis is feasible when ap-
plied to paintings covering a wide area of the city scape, 
such as rakuchu-rakugai-zu screens. The analytical 
methodology presented in this study involves not merely 
mapping landmarks that appear on the screens, but also 
projecting a “virtual screen” onto a survey map. I was 
able	to	implement	this	process	for	the	first	time	by	using	
the	‘adjust,’	‘area	calculation,’	and	‘table	join’	functions	
DIGITAL HUMANITIES 2009
Page  405
of GIS analysis. The results obtained with this methodol-
ogy offer insights from a geographic point of view into 
the approaches taken to space drawing and landmark po-
sitioning in these works.
Analyzing landscape drawing methods, conventional 
art history mentions that from the 17th century on, the 
depiction	of	politically	significant	 landmarks	became	a	
norm and took priority over the accurate rendering of 
geographic locations. That is, landmarks took on the as-
pect of symbols, which the creators of the screens in-
creasingly incorporated, paying less attention to their 
positional relationships. 
In short, it turned out that my case study of rakuchu-
rakugai-zu screens of the 16th and 17th centuries sup-
ports this point of conventional art history not from a 
subjective but an objective point of view as using GIS 
can visualize drawn area and regularity in drawing ex-
actly, measured in quantity (Fig. 2). While this overview 
has provided a sense of general trends over time, I hope 
to follow up this study with more detailed analyses of 
individual screens in the context of the sociopolitical en-
vironments in which they were created. 
Acknowledgements
This research was conducted with assistance from the 
Japan Society for the Promotion of Science and the Min-
istry of Education, Culture, Sports, Science and Technol-
ogy, Global COE Program, “Digital Humanities Center 
for Japanese Arts and Cultures”, Ritsumeikan University.
References
Takeda, T. 武田恒夫 (1966). Rakuchu-rakugaizu 
to sono tenkai 洛中洛外図とその展開. In Kyoto 
kokuritsu hakubutsukan ed: Rakuchu-rakugai-zu 洛中
洛外図, kadokawa shoten 角川書店, 1966, pp.17-32. 
William, G. and Thomas, III (2004). Computing and the 
Historical Imagination. In Schreibman, A., Siemens, R., 
Siemens, R. G. and Unsworth, J. (eds), A Companion 
to Digital Humanities, Oxford: Blackwell Publishing, 
pp.56-68.
Figure 1. Procedures
DIGITAL HUMANITIES 2009
Page 406
Figure 2. From Realism to Deformation
Towards an Online Edition 
of the Slovenian Biographical 
Lexicon
Petra Vide Ogrin 
Slovenian Academy of Sciences and Arts, Library 
petra.vide@zrc-sazu.si
Tomaž	Erjavec 
Jožef	Stefan	Institute 
tomaz.erjavec@ijs.si	
1. Introduction
The paper presents the project of digitization of the Slovenian	 Biographical	 Lexicon	 (SBL).	 We	 first	
describe the up-conversion of the source OCR text to a 
richly structured XML, encoded according to the Text 
Encoding Initiative Guidelines TEI P5 (TEI, 2008), us-
ing the module on biographical and prosopographical 
data. Next, some more challenging aspects of the con-
version process are discussed, in particular the extraction 
of meta-data, the expansion of abbreviations into their 
fully	 inflected	 forms,	 the	diachronic	nature	of	 the	 text,	
and the effects of some language technology tools devel-
oped	for	the	Slovenian	language	on	the	efficiency	of	the	
information retrieval for Slovenian texts.
2. The SBL
The Slovenian Biographical Lexicon summarizes the 
lives	 and	work	of	notable	figures	 from	Slovenia’s	 cul-
tural	history.	It	gives	a	picture	of	Slovenia’s	cultural	life,	
from its beginnings up to the contemporary time by in-
cluding those who participated in its cultural develop-
ment, were of Slovenian nationality or born in Slovenia, 
and were active in the homeland or abroad, as well as 
persons of foreign origin who with their work among the 
Slovenians	influenced	the	Slovenian	cultural	life.	It	com-
prises 15 volumes plus index, with over 3,000 pages or 
16 mio characters, and covers 5,031 biographical entries 
or, as some of the entries are family names, over 5,100 
persons. Its publication spanned almost 70 years (1925-
1991).	SBL	aims	to	cover	not	only	a	person’s	biography	
but also to give information about the important litera-
ture depicting their life and work or to direct a user to the 
whereabouts of their unpublished work, photographs, 
letters etc. The data in the SBL articles were always 
checked against the primary material source. As such, 
the	SBL	is	a	reliable	reference	for	any	relevant	scientific	
research	in	the	fields	of	humanities,	social	sciences	and	
the history of natural sciences.
DIGITAL HUMANITIES 2009
Page  407
In order to widen the availability of SBL, the Slovenian 
Academy of Sciences and Arts (SASA)1 and the Scien-
tific	Research	Centre	of	the	SASA	(SRC	SASA)2 is un-
dertaking the digitization of the SBL in order to make it 
freely available on-line, also enabling the kind of infor-
mation retrieval not allowed by the nature of printed text.
3. Up-conversion and TEI P5 encoding
The	 SBL	 was	 first	 scanned	 and	 the	 text	 OCRed.	 The	
OCR was then semi-manually corrected for errors and 
then via a series of automatic steps converted into a rich 
TEI	 encoding.	The	 first	 step,	 via	 the	OpenOffice	 text-
editor and associated XSLT TEI stylesheet,3 converted 
the source text into a basic TEI structure (Erjavec and 
Ogrin, 2005). Since some metadata were already avail-
able in the form of an Access database, this was exported 
into the XML format, following the TEI P5 module on 
biographical and prosopographical data. Then additional 
metadata, such as the information for the <floruit> el-
ement	defining	the	exact	period	for	more	than	one	occu-
pation for a particular person in the entry, are added into 
the metadata structure manually. From here, metadata 
are added to the entries, and, in a related process, the text 
of the entries is further annotated. 
Figure 1: TEI-XML document excerpt with the 
<listPerson> structure
3. 1. The TEI structure
The encoding scheme of the SBL follows the TEI P5 
Guidelines, in particular the module on biographical 
and prosopographical data. P5 introduces elements for 
a structured biographic entry, for which the information 
contained in the text is extracted and encoded separately 
using the <person> element, which contains informa-
tion on the name(s), sex, nationality, faith, dates of birth 
and death, facts about residence, occupation, important 
life events, etc. In addition to the text, each entry of the 
lexicon thus also contains metadata, useful for searching 
and organising the SBL.
3. 2. Extracting metadata
Since some metadata not strictly connected with the per-
son, who is the topic of the entry, such as other named 
persons in the entry articles, have been manually ex-
tracted from the SBL text, we are exploring the possi-
bilities of automatic encoding to speed up the encoding 
process. Partially, this can be done on the basis of exist-
ing indexes of the SBL and via external resources, such 
as gazetteers. These, coupled with the power of regular 
expressions in Perl, can extract the relevant terms, such 
as dates, with a fairly high accuracy. A more principled 
solution would be to implement a general Named Entity 
Recognition (NER) system for Slovenian, which would 
recognize and categorise persons and places names, 
dates and other numeric expressions. Such a system does 
not yet exist for Slovenian, and we are exploring the pos-
sibility of writing NER rules for one of the widely used 
human language technology toolsets, such as GATE4 or 
NooJ5 (Bekavac, 2002).
3. 3. Abbreviations
SBL is written in encyclopaedic style, which means 
dense language and many abbreviations. There are 
several types of abbreviations: a) bibliographic abbre-
viations (e.g. RDHV for Razprave Znanstvenega društva 
za humanistične vede); b) abbreviations to denote the 
authors of SBL articles (e.g. Rš for Fran Ramovš); c) 
abbreviations to refer to a biographical entry within an 
article (e.g. A. for Abraham); d) abbreviations for cer-
tain geographic names (e.g. Lj. for Ljubljana or Clvc 
for Celovec); e) general abbreviations (e.g. the names of 
months).
While the use of abbreviations was appropriate for the 
printed books, it will only impair readability and make 
searching	more	difficult	in	the	digital	edition.	The	abbre-
viations are therefore expanded into their full forms and 
encoded with the TEI <choice>, <abbr> and <ex-
pan> elements.  The basic resource for the expansion 
are lists of bibliographic abbreviations and abbreviations 
denoting authors of particular articles from the printed 
edition, and an additional abbreviation lexicon, semi-
automatically compiled from the SBL.
A problem that occurs in the expansion of abbreviations 
stems from the fact that the Slovenian language, like all 
Slavic	 languages,	 is	 a	 highly	 inflective	 language.	This	
DIGITAL HUMANITIES 2009
Page 408
means abbreviations have to be expanded into their ap-
propriate	full	forms	in	the	correct	inflection,	which	de-
pends on the context of the abbreviation. So, for example, 
“Rojen v Lj.” (Born in Ljubljana), should be expanded 
to “Rojen v Ljubljani”, with “Ljubljana” in the locative 
case. This problem is dealt with by automatic morpho-
syntactic tagging of the text, and then, on the basis of the 
tag assigned to an abbreviation, generating the appropri-
ate	inflected	form	of	the	lemma.	This	work	uses	tagging	
and lemmatisation models automatically obtained from 
annotated corpora and morphological lexicons, which 
have the advantage of generalising to out-of-vocabulary 
words	(Erjavec	and	Džeroski,	2004).				
3. 4. Language change
Another challenge regarding the language of SBL is due 
to its long publication period of almost 70 years. In this 
period the language, particularly its lexical aspects, has 
changed	 significantly.	Changes	 affect	 particularly	 geo-
graphic names (e.g. Curih instead of Zürich) and terms 
denoting occupations and activities, from spelling vari-
ants to complete substitution of words. These changes 
will have to be taken into account to ensure adequate 
information retrieval. The plan is to add normalised 
(contemporary) forms to the forms in the text, by encod-
ing them with the appropriate <choice>, <reg> and 
<orig> elements. The annotation will be automatic, but 
based on a lexicon of variants, semi-automatically com-
piled from the SBL. This lexicon, in itself an interesting 
diachronic language resource, is being built on the basis 
of a textual analysis of SBL, taking into account external 
language resources, such as dictionaries and gazetteers, 
e.g. of old and new names of places.
4. Access 
The SBL is to be made freely accessible for full-text and 
structured searching. We are exploring the possibility 
of using the open source Apache Solr6 search platform 
based on the Lucene Java search library, which enables 
such kinds of queries, accepts TEI / XML documents and 
enables different plugins and extra features, such as the 
lemmatization tool for Slovenian. No research has been 
done yet, however, on its effects upon the efficiency of 
the information retrieval for the Slovenian language.
References
Bekavac,	 Božo:	 Strojno	 obilježavanje	 hrvatskih	 tek-
stova – stanje i perspektive. (Computer annotation of 
Croatian texts – current state and perspectives) Suvre-
mena lingvistika. 53-54 (2002), p. 173-182.
Cankar, Izidor et al. (eds) (1925-1991): Slovenski bi-
ografski leksikon. Ljubljana: Slovenska akademija zna-
nosti in umetnosti.
Erjavec,	 Tomaž	 and	 Džeroski,	 Sašo	 (2004):	 Machine	
Learning of Morphosyntactic Structure: Lemmatising 
Unknown	Slovene	Words.	Applied	Artificial	Intelligence	
18 (1), p. 17-40.
Erjavec,	 Tomaž	 and	 Ogrin,	 Matija	 (2005):	 Digitalisa-
tion of Literary Heritage Using Open Standards. In: Paul 
Cunningham, Miriam Cunningham (eds.). Innovation 
and knowledge economy: issues, applications, case stud-
ies, (Information and communication technologies and 
the knowledge economy). Amsterdam [etc.]: IOS Press, 
2005, p. 999-1006.
TEI Consortium, eds. (2008) TEI P5: Guidelines for 
Electronic Text Encoding and Interchange. TEI Consor-
tium. http://www.tei-c.org/Guidelines/P5/.
Notes
1http://www.sazu.si/ 
2http://www.zrc-sazu.si/ 
3http://www.tei-c.org/wiki/index.php/TEI_OpenOffice_
Package
4http://gate.ac.uk/ 
5http://www.nooj4nlp.net/ 
6http://lucene.apache.org/solr/
DIGITAL HUMANITIES 2009
Page  409
From the Local to the Global 
Sphere: Prospects of Digital 
Humanities for Japanese Arts 
and Cultures
Keiji Yano 
Ritsumeikan University 
yano@lt.ritsumei.ac.jp
Ryo Akama
Ritsumeikan University
rat03102@lt.ritsumei.ac.jp
Kozaburo Hachimura
Ritsumeikan University 
hachimura@media.ritsumei.ac.jp
Hiromi Tanaka 
Ritsumeikan University 
hiromi@cv.ci.ritsumei.ac.jp
Mitsuyuki Inaba
Ritsumeikan University 
inabam@sps.ritsumei.ac.jp
This poster discusses each of the research activities, issues, and prospects on Digital Humanities for 
Japanese arts and cultures at “the Digital Humanities 
Center for Japanese Arts and Cultures” of Ritsumeikan 
University. By doing so, we would like to suggest new 
directions in Digital Humanities, still fairly a new inter-
disciplinary	research	field,	as	well	as	contributions	Digi-
tal Humanities could make to the humanities in general, 
and	more	specifically	for	the	studies	of	Japanese	arts	and	
cultures. As we do conduct these research activities as 
a	five-year	Global	COE	(Center	of	Excellent)	Program	
that the Art Research Center (ARC) at Ritsumeikan Uni-
versity has launched with the support from the Ministry 
of Education, Culture, Sports, Science and Technology 
(MEXT) since 2007.
Our program of “the Digital Humanities Center for Japa-
nese Arts and Cultures” is to further a study of the hu-
manities based on Digital Humanities, taking Japanese 
art and culture as its subject, as centered on the historic 
city of Kyoto. For this purpose, we intend to make full 
use of the most advanced information technologies, such 
as digital archives, visualization, media technologies, 
Geographical Information Systems (GIS) and Web 2.0. 
By using such information technologies, we at the Cen-
ter systematically organize information on Japanese art 
and culture, which will be open to the public and useful 
in a wide variety of contexts. The Center also serves as 
a global portal for the study of Japanese art and culture, 
aiming not only to create a new system of cultural ex-
changes among talented researchers within and out of 
Japan, but also to make the results of research available 
throughout the world. That means that we see the his-
toric city of Kyoto as both a base for promoting scholar-
ship on Japanese art and culture around the world, and a 
global hub for education and research.
Our digital archives are meant to bring together informa-
tion on Japanese art and culture, both tangible and intan-
gible, which are left unorganized all over the world. The 
archives of such cultural properties use information man-
agement technology to combine different types of data: 
text, pictures, sound, moving images and the motion of 
the human body. This combination of different types of 
date, covering both tangible and intangible cultural prop-
erties, gives us a great advantage over the conventional 
text-centered research. Furthermore, by systematically 
linking to databases on other systems, we can revolu-
tionize the amount and quality of material available for 
research, leading to great improvements in the quality of 
research and to a new perspective on humanities studies. 
For example, GIS technology will lead to further ad-
vances in the visibility and amount of data relating to 
the time and space content of various aspects of arts, 
culture and urban landscapes. Using the bi-directional 
format network environment of Web 2.0, we will create 
a portal that makes information more open, cooperative 
and usable. The resulting research will be compiled and 
distributed on-line. We will also promote research into 
analytical methods and archiving technology related to 
the exquisite cultural properties that characterize the his-
toric cultural city of Kyoto.
We like to start this poster with a brief introduction of 
the program and its missions. After that, each of the fol-
lowing	five	 research	 topics	will	 be	 presented,	 discuss-
ing where their research interests lie in Japanese arts and 
cultures, and what their methodologies and challenges 
are in terms of digital humanities.
1. “Digital Archives for Japanese Woodblock Prints: 
Their Global Linkage”
2. “Virtual Kyoto: Integration of Digital Contents of 
Japanese Arts and Cultures on 4D-GIS with a Time 
Dimension”
DIGITAL HUMANITIES 2009
Page 410
3. “Digitization and Analysis of Traditional Dance 
Body Movement by using Motion Capture Technol-
ogy” 
4. “3D Modeling and Visualization of Japanese Tradi-
tional Arts and Cultural Assets”
5. “Collaborative Web Technologies for Japanese Arts 
and Cultures”
Also we will address issues and prospects we share at 
the Center, including the current situation and tendencies 
of the Humanities in Japan—urgent issues we have to 
tackle with our activities at the Center. 
Some of the presumable issues to be addressed may in-
clude the following:
1. Humanities scholars in Japan, who tend to work 
domestically within their academic societies, shall 
partake in a global academic environment, includ-
ing a Web-based one.
2. While humanities scholars tend to see the Web and 
information technology as mere tools, they also 
have to realize that using these tools may give rise 
to new perspectives and paradigms.
3. Japanese Humanities researchers should realize that 
the Internet is now advancing into the age of Web 
2.0, meaning that the environment of the Web is be-
coming ever more crucial for humanities research. 
This pattern of research is common in Europe and 
the United States, and as a high-level institute for 
research and education we need to work in a similar 
way, in order to ensure the future development of 
research.
4. Not only Humanities scholars who are handling Jap-
anese language but also other East Asian languages 
have	been	experiencing	some	difficulties	in	encod-
ing texts or sharing textual resources on the Web. 
This is because an international standard of charac-
ter encoding, Unicode, has limitations to represent 
characters in those languages.
We hope to contribute to the further discussion on Digi-
tal Humanities from the perspective of a non-Western 
country.
Websites:
http://www.arc.ritsumei.ac.jp/lib/GCOE/guideline_e.
html 
http://www.geo.lt.ritsumei.ac.jp/uv4w/frame_e.jsp
Restoring 3D Digital Woodcut 
Shape for Reproducing Ancient 
Book 
Xin Yin 
Ritsumeikan University 
yin@cv.ci.ritsumei.ac.jp
Ryo Akama 
Ritsumeikan University 
rat03102@lt.ritsumei.ac.jp
Hiromi T. Tanaka 
Ritsumeikan University 
hiromi@cv.ci.ritsumei.ac.jp
Kazuaki Nagai 
Nara University
1. Introduction 
Cultural heritage is important for studying history, and some studies have been carried out to preserve it as 
digital data. Hanpon are ancient woodcut-printed books, 
all of which are regarded as important cultural heritage 
in Japan. (see Figure 1 for woodcut and hanpon). The 
woodcut looks black as its surface is covered with ink. 
A lot of studies about hanpon and woodcut have been 
done. However, there is little study to connect these two 
studies using digital techniques. The main contribution 
of our study is developing digital techniques for studying 
hanpon history using Woodcut information. 
Fig. 1 Woodcut and hanpon. 
There are some versions of hanpon whose true pub-
lished time is not clear. As the paper or woodcut have 
shrunken over time, it is possible to guess which ver-
DIGITAL HUMANITIES 2009
Page  411
sion is published early and which is late. Recently, some 
woodcuts were found, and we hope to know the original 
size of hanpon printed by these woodcuts. If we could 
do so, we can know which version of hanpon is printed 
by the founded woodcuts. Warped in shape, however, 
they cannot be utilized to print han pon directly. To solve 
this problem, we propose techniques to restore woodcut 
shapes in a digital way so that we can reproduce ancient 
hanpon. As traditional method, a printer put some wa-
ter on the back of Woodcut and the Woodcut shape can 
be restored near a plane. But the size of the Woodcut 
restored using the traditional method is different to the 
original one. To solve this problem, a digital technique 
for restoring the shape of digital Woodcut is proposed. 
Then we can produce original ancient hanpon. 
Basically, our system is a virtual printing system. Okada1
 
proposes a system for sculpturing and printing in a vir-
tual world. Focusing on virtually sculpturing 3D objects, 
his system can print Japanese drawings if one paints 
some colors on the virtual surface of a carved object. 
Yet, the system uses a carved plane for virtual printing 
and cannot be utilized in the case of measured woodcut 
digital data, as real woodcuts are distorted a little. For 
virtual printing on the distorted surface of woodcut,2
 
uses 
a	 small	plane	 to	fit	 to	a	distorted	woodcut	 surface	and	
make a virtual printing. However, this method ignores 
changes in woodcut size. To acquire a hanpon the same 
as the ancient one, it is necessary to restore woodcut 
shapes. 
Fig. 2 Aligned 3D digital Woodcut model
For restoring woodcut shapes, we can learn a lot from 
some research concerned with wood drying.3 Finite ele-
ment method (FEM), for example, is utilized to simulate 
moist translation and shape variation. This study in the 
wood	research	field	mainly	focuses	on	how	to	decrease	
shape distortion that occurs during the drying process. 
As wood is an orthotropic material, we propose an al-
gorithm for determining orthotropic direction to restore 
woodcut shapes. 
The following is the procedure to restore the woodcut 
shape for reproducing the ancient book of hanpon. First, 
using commercial software developed from the algo-
rithm4, we measure 3D of the woodcut, whose data are 
to be aligned to make the 3D digital woodcut. After the 
woodcut shape is restored, we can print out virtual han-
pon. 
2. Measuring and aligning 3D Woodcut 
point cloud data 
The non-contact 3D-Measurement machine VIVID910 
is utilized to measure woodcut point clouds. When mea-
suring the woodcut, it is necessary to take bump patterns 
on the woodcut surface as precisely as possible. Because 
the measurement machine cannot measure all the surface 
of the object at once, we need to generate object shape 
models by using measured point clouds from multiple 
viewpoints. Commercial software is utilized to align 
cloud	 data.	At	 first,	 the	 rough	 corresponding	 point	 is	
defined	on	different	cloud	data.	Then,	the	software	can	
value the globe errors of the total aligned point clouds 
and adjust the position and direction of these point 
clouds. At the end, the point clouds are aligned in one 
3D digital woodcut model. Figure 2 show the aligned re-
sult. Different colors show different parts of the woodcut 
cloud point data.
3. Constructing wood board model 
As the woodcut surface is very delicate, the computing 
cost will be consid erable if one uses this delicate model. 
A rough mesh model is constructed to restore the wood-
cut shape. As the woodcut is made from a piece of wood 
board, if the wood board shape is restored, so will be 
the woodcut shape. Hence, the rough model is the wood 
board. This wood board model will is a box after its 
shape is restored. 
To construct the wood board model, we utilize the wood-
cut section. In the Figure 3, the bottom line is the wood-
cut model section, and the top line is the constructed 
wood board model section. The top line is the connec-
tion line between local highest points of woodcut model 
sections. Some woodcut sections are utilized to construct 
the total wood board model. The distance between these 
sections is the same as the woodcut's thickness. After all 
DIGITAL HUMANITIES 2009
Page 412
sections are processed with this method, we can obtain 
the wood board for restoring. 
Fig. 3 Constructing the wood board model from Woodcut 
section 
4. Restoring the wood board model 
As mentioned above, wood is an orthotropic material. 
The wood study needs to consider the three main axes 
of the wood. Figure 4 shows a tree trunk form in a co-
ordinate system with the three standard directions: the 
trunk	axis	or	fiber	direction	L, the radial direction R that 
passes	through	the	tree’s	core,	and	the	tangent	direction	
T along the annual ring. When wood dries and shrinks, 
shrinkage	 in	 the	 tangential,	 radial,	 and	fiber	 directions	
occurs in a ratio of 10:5:0.5. With this reason, the wood 
board distorts. 
While the surface of woodcut is black, on the woodcut 
end,	we	can	find	some	exposed	parts,	including	the	annu-
al	ring	pattern.	From	this	annual	ring,	the	tree’s	core	can	
be understood as the annual ring is nearly a concentric 
cylinder.	From	the	position	of	the	tree’s	core,	the	wood	
three standard directions can be determined all over the 
wood board. 
Fig. 4 Wood three axis. 
The woodcut is made of wild cherry tree or boxwood. 
Shrinkage of this type of the wood is about 0.31% on 
the tangent direction T , and 0.17% on the radial direc-
tion R.	Since	the	shrinkage	on	fiber	direction	L is very 
small, its change L is ignorable. If the moisture content 
is lower than 30%, the wood starts distorting, in Japan, 
after the moisture content is 15%, the moisture content 
stops changing, and the shape stops distorting. As Figure 
5 shows, point A is one point in the wood board. ε is the 
vector to show plastic strain on the point A. T is tangent 
direction and R is radial direction. The plastic strain can 
be shown as the following equation: 
ε = ε
T
 − ε
R
 =(S
T
 × T − S
R
 × R) × ΔW (1) 
where, ST is shrinkage on direction T and S
R
 is shrinkage 
on direction R.	ΔW is moisture content variation and is 
1%.	Using	this	equation,	the	plastic	strain	ε can be com-
puted. The restoring process is repeated until the wood 
board surface gets closer to a plane or the total moisture 
content reaches 30%. This is the restored wood board. 
As the connection between the woodcut point cloud and 
wood board mesh is known, it is easy to obtain a restored 
woodcut point cloud from the restored wood board. As 
the local highest point is on a plane after shape restored, 
the virtual hanpon can be printed easily by using the 
height information of the restored woodcut. If the height 
is higher than 0.3mm under the highest point, it is the 
part for printing. The printed result is shown in Figure 6. 
Fig. 5 Plastic Strain.
5. Results 
The	final	 rendering	result	 is	carried	out	on	a	computer	
with the GPU (Graph ics Processing Unit) and can ren-
der the hanpon on real time. The graph card is NVIDIA 
GeForce 6800 GS. 
Since the Japanese paper became old and was not as 
white as it was printed out, brown color captured from 
old Japanese paper is added into the virtual printing re-
sult.	 The	 virtual	 printing	 result	 and	 the	 Japanese	 fiber	
DIGITAL HUMANITIES 2009
Page  413
model	are	utilized	to	render	based	on	the	fiber	reflection	
model.5	Figure	6	shows	the	final	rendering	results.	The	
left image shows the hanpon appearance with white 
color after it was printed out about 200 years ago. The 
right one shows the result after hanpon color variation. 
Using the technique proposed, the ancient hanpon is rep-
resented in the virtual world. 
As the woodcut shape is restored, the size of printed 
hanpon is the same as the original hanpon when it was 
printed hundreds years ago. Comparing the size of han-
pon and virtual printed result using proposed method, 
the published history of hanpon can be understood well. 
This information is very important for history research. 
Fig. 6 Rendering result of hanpon. 
6. Conclusion and discussion 
This paper proposes the techniques to restore woodcut 
shapes and reproduce ancient hanpon. The bump pat-
tern of woodcut surface is very minute, and the color of 
woodcut is black. To restore the woodcut shape, there-
fore, the FEM method is utilized. This method can ob-
tain	fine	printing	 results	 even	woodcut	 is	distorted.	To	
our	knowledge,	it	is	first	time	to	try	to	study	hanpon his-
tory using virtual Woodcut printing method. 
Some work, however, needs to be done in the future. 
Right now, using 3D scanner cannot capture the wood-
cutfs details. Thus, we need to develop tech niques to im-
prove original 3D data precision. One idea is to use the 
high resolution camera to take high resolution photos in 
a different lighting environment. Using image process-
ing technique makes the printed areas possible to be 
extracted. Then, using the proposed techniques in this 
paper to calibrate the hanpon size, we can obtain delicate 
printed results. We also hope to develop a Virtual Re ality 
system in which the user can watch the cultural heritage 
and touch its surface at the same time. This Virtual Real-
ity system is not only a new type digital museum, but 
also is entrainment system such as game as well. 
References 
1) M.Okada, S.Mizuno and J.Toriwaki: Virtual Sculpt-
ing and Virtual Wood block Printing by Model-Driven 
Scheme, the Journal of the Society for Art and Science, 
Vol.1, No.2, pp.74–84 (2002). 
2) Yin, X., Eto, T., Akama, R., Nagai, K. and Tanaka, 
H.T.: Digital Woodcut Measurement and Ancient Han-
pon Rendering, Proceedings of 2008 ASIA GRAPH, 
Vol.2, No.1, pp.31–36 (2008). 
3) Carlsson, P. and Esping, B.: Optimization of the wood 
drying process, Structural and Multidisciplinary Optimi-
zation, Vol.14, No.4, pp.232–241 (1997). 
4) Soucy, M. and Laurendeau, D.: A General Surface Ap-
proach to the Inte gration of a Set of Range Views, IEEE 
Trans. Pattern Anal, Vol.17, No.4, pp.344–358 (1995). 
5) Yin, X., Cai, K., Takeda, Y., Akama, R. and T.Tanaka, 
H.:	 Measurement	 of	 Reflection	 Properties	 in	 Ancient	
Japanese Drawing Ukiyo-e, Proceedings of 8th Asian 
Conference on Computer Vision (ACCV 2007), Part I, 
LNCS 4843, pp.779–788 (2007). 
DIGITAL HUMANITIES 2009
Page 414
Author Index
Ács, Bernie ............................................................... 306 
Adamo-Villani, Nicoletta ........................................... 49 
Adamson-Williams, Stephanie ................................. 335 
Abels, Eileen ............................................................ 184
Akama, Ryo ...................................................... 409, 410 
Alhoori, Hamed ........................................................ 119 
Allen, Robert C. ....................................................... 255 
Álvarez, Omar .......................................................... 119 
Ammerman, Jack ..................................................... 299 
Anderson, Sheila ........................................................ 47 
Andreev, Vadim Sergeevich ....................................... 52 
Archer, Dawn ........................................................... 309
Armstrong, Karin ..................................................... 250 
Arneil, Stewart ........................................................... 54 
Athenikos,	Sofia	J.	.....................................................	56 
Audenaert, Neal ......................................................... 63 
August, Stephanie ...................................................... 21 
Ault, Nick ................................................................. 392 
Auvil, Loretta ........................................................... 306 
Baayen, R. Harald ...................................................... 90 
Bachta Ed ................................................................. 181 
Barner-Rasmussen, Michael ...................................... 65 
Baron, Alistair .......................................................... 309 
Bauman, Syd .............................................................. 68 
Beavan, Dave ........................................................... 399 
Bederson, Benjamin B. ............................................ 313 
Bennett, Alex ............................................................ 315 
Berkov, Joshua ......................................................... 401
Bernola, Ashley ........................................................ 160 
Bieling, Tom ............................................................. 157 
Bilisoly, Roger ......................................................... 318 
Bjork, Olin ............................................................... 320 
Blackwell, C. W. .......................................................... 6 
Blanke, Tobias ..................................................... 2, 136 
Blier, Suzanne .......................................................... 367 
Bobenhausen, Klemens .............................................. 69 
Bodard, Gabriel ............................................................ 2 
Boggs, Jeremy .......................................................... 333 
Bol, Peter .................................................................. 367 
Bowman, A. K. ......................................................... 237
Bradley, John ...................................................... 72, 322 
Brady, J. M. .............................................................. 237 
Brandhorst, Hans ...................................................... 324 
Brey, Gerhard ........................................................... 136 
Brom, Timothy H. ...................................................... 75 
Brown, Susan ................................................... 209, 399 
Buchmüller, Sandra .................................................. 157
Buzzetti Dino ......................................................... 8, 12 
Byrne, Thomas ......................................................... 167 
Canfield,	Kip	..............................................................	77 
Capitanu, Boris ......................................................... 306 
Carvalho, Andreia .................................................... 327 
Castro, Filipe ............................................................ 344 
Caton, Paul ................................................................. 80 
Cayless, Hugh .................................................... 83, 401 
Chen, Sherol ............................................................. 328 
Chen, Szu-Pei ............................................................. 85 
Chesley, Paula Horwath ............................................. 90 
Chow, Kenny K. N. .................................................. 350 
Chun, Susan ............................................................. 181 
Ciula, Arianna .................................................. 211, 331 
Clement, Tanya .................................................. 37, 154 
Coartney, Jama S. ..................................................... 294
Cohen, Dan ............................................................... 333 
Connor, Holly L. ...................................................... 195 
Connors, Louisa ......................................................... 92 
Crandell, Caitlin ....................................................... 363
Crist, Laurie ............................................................. 263 
Davies, Mark .............................................................. 97 
Day, Shawn ........................................................ 40, 226 
Demonet, Marie Luce ................................................ 99 
Dickson, Maggie ...................................................... 335 
Dik, Helma ............................................................... 338 
Dobson, Teresa ......................................................... 287 
Donahue, Rachel ........................................................ 27 
Douglass Jeremy ........................................................ 20 
Downie, J. Stephen ........................................... 230, 399 
Dyer, Cliff ................................................................ 401 
Eberle-Sinatra, Michael ........................................... 340 
Eckhardt, Kevin ....................................................... 255 
Eder, Maciej ............................................................. 242 
Edmond, Jennifer ....................................................... 40 
Edwards, Richard L. ................................................. 341 
Eide, Øyvind ............................................................ 101 
Elliott, Tom .................................................................. 4 
Emery, Doug ............................................................ 281 
Erjavec,	Tomaž	.........................................................	406
Finn, Edward ............................................................ 103 
Fiorentino, Carlos ..................................................... 399 
Fitzgerald, Sharon Quinn ......................................... 369 
Fitzpatrick, Kathleen ................................................ 106 
Flanders, Julia .......................................................... 108 
Fleischmann, Kenneth R. ......................................... 110 
Fraistat, Neil ......................................................... 22, 47 
France, Fenella G. .................................................... 113 
French, Amanda L. ................................................... 116 
Fretwell, Erica .......................................................... 117 
Furuta, Richard ........................................... 63, 119, 344 
Galey, Alan ............................................................... 240 
Gannod, Gerald C. ................................................... 195 
Genovesi, Chiara .......................................................... 9 
Gerber, Anna ............................................................ 124 
Gilbert, Joseph F. ....................................................... 29 
Gillies, Sean ................................................................. 4 
Gist, Christopher ........................................................ 29 
Goldbeck, Jennifer ........................................... 181, 348 
DIGITAL HUMANITIES 2009
Page  415
Goldfield.	Joel	..........................................................	127 
Gong, Emily ............................................................. 363 
Gourley, Don .............................................................. 40 
Green, Garth ............................................................. 299 
Griffioen,	James	.........................................................	75 
Guertin, Carolyn ......................................................... 43 
Hachimura, Kozaburo .............................................. 409 
Hansen, Eric F. ......................................................... 113 
Harrell, D. Fox ......................................... 130, 301, 350 
Hatley, Leshell ......................................................... 353 
Hedges, Mark ....................................................... 2, 136 
Herrick, Scott ........................................................... 320 
Holdzkom, Lynn ....................................................... 335 
Honkapohja, Alpo .................................................... 355
Hoover, David L. ............................................. 141, 145 
Hsiang, Jieh ................................................................ 85 
Huitfeldt, Claus ........................................................ 257 
Hunter, Jane .............................................................. 124 
Inaba, Mitsuyuki ...................................... 148, 248, 409 
Ippolito, John ........................................................... 369 
Isolani, Alida ................................................................ 9 
Jambou, Louis .......................................................... 365 
Jannidis, Fotis ............................................................ 17
Jerz, Dennis ................................................................ 22 
Jockers, Matthew ..................................................... 151 
John, Jeremy ............................................................. 297 
Johnson, Amy ........................................................... 335 
Johnston, Kelly ........................................................... 29 
Jones, Steven Edward .............................................. 153 
Jong, Chang-Han ..................................................... 154 
Joost, Gesche ........................................................... 157
Juola, Patrick ............ 160, 162, 208, 357, 380, 391, 399 
Kainz, Chad ................................................................ 47 
Keating, John G. ............................... 163, 167, 170, 220 
Kelber, Nathan ......................................................... 359 
Kermanidis, Katia Lida ............................................ 173 
Khosmood, Foaad .................................................... 177 
Kimura, Fuminori ............................................. 278, 361 
King, Katie ................................................................. 43 
Kirschenbaum, Matthew ............................................ 22 
Klavans, Judith ................................................. 181, 184 
Kleint, John .............................................................. 181 
Kramer-Smyth, Jeanne ............................................. 348 
Kraus, Kari ................................................................. 27 
Kraus, Rachel ........................................................... 363 
Krauwer, Steven ......................................................... 47 
Kretzschmar, Jr., William A. .................................... 186 
Lagressa,	Elizabeth	Sofia	.........................................	188 
Laiacona, Nick ......................................................... 198 
LaPlante, Rebecca .................................................... 181 
Lavagnino, John ................................................... 14, 15 
Le Priol, Florence ..................................................... 365
Leitch, Cara .............................................................. 250
Lester, Dave ............................................................. 333 
Levine, Stacey .......................................................... 380 
Levinson, Robert ...................................................... 177 
Lewis, Benjamin G. ................................................. 367 
Lieu, Tiffany ............................................................. 363 
Lin, Jimmy ............................................................... 184 
Lin, Xia ...................................................................... 56 
Lindemann, Marilee ................................................... 43 
Lippincott, Thomas .................................................. 190 
Llorà, Xavier ............................................................ 306 
Lopez, Tamara .......................................................... 331 
Lorito, Claudia ............................................................. 9 
Losh, Elizabeth ........................................................... 20 
Lowood, Henry .......................................................... 24 
Lukon, Shelly ........................................................... 399 
Lutz, Marilyn R. ....................................................... 369 
Lynch, Paul ............................................................... 192 
Maeda, Akira .................................................... 278, 361 
Mandell, Laura ................................................. 195, 198 
Marcoux, Yves .......................................................... 257 
Marino, Marc ....................................................... 19, 21
Marotta, Daniele ........................................................... 9 
Mason-Marshall, Jacob ............................................ 363 
Matteoli, Marco ............................................................ 9 
Mayo, Kate ............................................................... 181 
McCarl, Clayton ....................................................... 371 
McDonough, Jerome ............................................ 22, 24 
McGee, Micki .......................................................... 341 
Medler, Ben .............................................................. 130 
Melson, John ............................................................ 201 
Meschini, Federico ........................................... 203, 204 
Miyake, Maki ........................................................... 373 
Monroy, Carlos ........................................................ 344 
Moody, Ellen .............................................................. 43 
Moore, Elise ............................................................. 255 
Moulthrop, Stuart ..................................................... 206 
Muller, A. Charles .................................................... 375 
Muñiz, Miguel .......................................................... 119
Nagai, Kazuaki ......................................................... 410 
Nagasaki, Kiyonori .................................................. 375 
Newton, Greg ............................................................. 54 
Nieves, Angel ........................................................... 378 
Noecker Jr., John ...................................... 208, 357, 380 
Nowviskie, Bethany ........................................... 29, 381 
Ohno, Shin ............................................................... 248 
Ohya, Kazushi .......................................................... 383
Okamoto, Takaaki .................................................... 386 
Organisciak, Piotr ..................................... 209, 233, 399 
Pacheco, Cristina Diego ........................................... 365 
Pasin, Michele .......................................................... 211 
Palmer, Richard ........................................................ 136 
Passonneau, Rebecca ............................................... 184 
Pierazzo, Elena ......................................................... 215 
Plaisant, Catherine ....................................... 32, 37, 154
Porter, Dorothy Carr ..................... 40, 68, 250, 381, 388
DIGITAL HUMANITIES 2009
Page 416
Porter, Zac ................................................................ 392 
Posthumus, Etienne .................................................. 324 
Potter, William G.  ................................................... 186 
Prickman, Gregory J. ............................................... 217 
Pytlik Zillig, Brian L. ............................................... 219 
Quinn, Alex .............................................................. 313
Rajkumar, Prahalad .................................................. 154
Ranaweera, Kamal ................................................... 209 
Rayson, Paul ............................................................. 309 
Rehbein, Malte ................................................... 11, 220 
Reside, Doug ...................................... 22, 223, 224, 388 
Rinehart, Richard ..................................................... 369 
Rockwell, Geoffrey .......... 209, 226, 230, 233, 252, 399 
Rosatelli, Meghan Elizabeth .................................... 236
Roued Olsen, Henriette ............................................ 237 
Rowlands, Ian ........................................................... 297 
Ruecker, Stan ................................... 209, 233, 240, 287 
Rutledge, Patrick ...................................................... 313 
Ryan, Mike ............................................... 357, 380, 391 
Rybicki, Jan ...................................................... 242, 245
Saito, Shinya ............................................................ 248
Sauda, Eric ............................................................... 392
Scheinfeldt, Tom ...................................................... 333 
Schreibman, Susan ....................................... 18, 40, 399 
Seales, W. Brent ......................................................... 75
Sekiguchi, Hiroyuki ................................................. 395 
Sgroi, Amanda .......................................................... 380
Shaw, William .......................................................... 397 
Sheffield,	Carolyn	............................................	181,	184
Sherriff, Liam ........................................................... 250 
Shiel, Patrick ............................................................ 220 
Shimoda, Masahiro .................................................. 375 
Shneiderman, Ben .................................................... 154 
Siddiquie, Behjat ...................................................... 154
Siemens, Lynne ........................................................ 250
Siemens, Ray ............................................................ 250 
Sinclair, Stéfan ................................. 230, 233, 252, 399 
Smith, D. N. ................................................................. 6 
Smith, Martha Nell ..................................................... 43 
Smith, Natasha ......................................... 255, 335, 401 
Soroka, Adam ............................................................. 29 
Soergel, Dagobert ............................................. 181, 184 
Speer, Sandy ............................................................. 357 
Sperberg-McQueen, Michael ................................... 257 
Spiro, Lisa ................................................................ 260 
Srihari, Rohini K. ..................................................... 263 
Srinivasan, Harish .................................................... 263 
Stalnaker, Rommie L. .............................................. 294 
Steger, Sara ................................................................ 32 
Stein, Robert ............................................................. 181 
Stokes, Peter A. ........................................................ 266 
Swanstrom, Lisa ....................................................... 269 
Tabata, Tomoji .......................................................... 270
Tanaka, Hiromi T. .................................................... 410 
Tarte, S. M. ............................................................... 237 
Teehan, Aja ....................................................... 163, 167 
Templeton, Thomas Clay ......................................... 110 
Terras, Melissa ......................................................... 275 
Thawonmas, Ruck .................................................... 278 
Thompson, Faye ....................................................... 331 
Toledo, Alejandro ..................................................... 278 
Toth, Michael B. ............................................... 113, 281 
Toton, Sarah ............................................................. 283 
Tozzini, Cinzia ............................................................. 9 
Travis, Charles ......................................................... 285
Tsukamoto, Akihiro .................................................. 403 
Tu, Hsieh Chang ........................................................ 85 
Upton, Daniel ........................................................... 130 
Urbina, Eduardo ....................................................... 119 
Uszkalo, Kirsten C. .................................... 35, 287, 399
van Dalen-Oskam, Karina ........................................ 290
van den Heuvel, Charles .......................................... 138
Vide Ogrin, Petra ...................................................... 406 
Vogel, Carl ............................................................... 192 
Vuillemot, Romain ..................................................... 37
Walsh, John .............................................................. 388 
Wang, Xiaoguang ..................................................... 148 
Wardrip-Fruin, Noah .................................................. 19
Webb, Sharon ........................................................... 170 
Welge, Michael ........................................................ 306 
Wells, Benjamin ....................................................... 380
Wen, Hefeng (Eddie) ................................................ 250 
Whaling, Richard ..................................................... 338 
Wheeles, Dana ......................................................... 198 
Wiesner, Susan L. ..................................................... 294 
Wilkens, Matthew .................................................... 296 
Williams, Peter ......................................................... 297 
Winget, Megan ........................................................... 27 
Wosh, Peter J. ........................................................... 116 
Wynne, Martin ........................................................... 47
Yano, Keiji ............................................................... 409
Yin, Xin .................................................................... 410
Zafrin, Vika .............................................................. 299 
Zhu, Jichen ....................................................... 130, 301